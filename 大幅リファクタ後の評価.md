# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè©•ä¾¡ã®è¦³ç‚¹ã«ã¤ã„ã¦
_Exported on 2025/11/22 at 23:01:57 JST from Cursor (2.1.19)_

---

**User**

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è©•ä¾¡ã‚’ã—ã¦

ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¦³ç‚¹ã‚„ã€ä¾å­˜æ€§æ³¨å…¥ã®è¦³ç‚¹ã€æ‹¡å¼µã®ã—ã‚„ã™ã•ã®è¦³ç‚¹ãªã©ã®è¦³ç‚¹ã‹ã‚‰ãŠé¡˜ã„ã—ã¾ã™ã€‚

---

**Cursor**

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®åŒ…æ‹¬çš„ãªè©•ä¾¡ã‚’è¡Œã†ãŸã‚ã«ã€ã¾ãšä¸»è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚

ãã‚Œã§ã¯ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®åŒ…æ‹¬çš„ãªè©•ä¾¡ã‚’è¡Œã„ã¾ã™ã€‚

# ğŸ“Š ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç·åˆè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

## 1. ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¦³ç‚¹

### âœ… **å„ªã‚Œã¦ã„ã‚‹ç‚¹ï¼ˆã‚¹ã‚³ã‚¢: 9/10ï¼‰**

#### 1.1 ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å®Ÿè£…ãŒå„ªç§€

```
Application Layer (API)
    â†“
Workflow Layer (Atomic â†’ Composite â†’ Orchestrations)
    â†“
Node Layer (Primitives â†’ Composites)
    â†“
Provider/Service Layer
    â†“
Infrastructure Layer
```

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ **æ˜ç¢ºãªè²¬ä»»åˆ†é›¢** ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚å„å±¤ãŒç‹¬ç«‹ã—ãŸå½¹å‰²ã‚’æŒã¡ã€ä¾å­˜é–¢ä¿‚ãŒä¸€æ–¹å‘ï¼ˆä¸Šã‹ã‚‰ä¸‹ï¼‰ã«ãªã£ã¦ã„ã¾ã™ã€‚

#### 1.2 æ®µéšçš„ãªåˆæˆï¼ˆProgressive Compositionï¼‰ãƒ‘ã‚¿ãƒ¼ãƒ³

```18:33:src/workflows/README.md
| å±¤                 | å½¹å‰²                       | ä¾‹                            | çµ„ã¿åˆã‚ã›å…ƒ              |
| ------------------ | -------------------------- | ----------------------------- | ------------------------- |
| **Atomic**         | å˜ä¸€æ©Ÿèƒ½ã®æœ€å°å®Ÿè¡Œå˜ä½     | chat, rag_query               | Nodes å±¤ã‚’ä½¿ã†            |
| **Composite**      | è¤‡æ•°ã® Atomic ã‚’çµ„ã¿åˆã‚ã› | ppt_summary, chain_of_thought | Atomic ã‚’çµ„ã¿åˆã‚ã›ã‚‹     |
| **Orchestrations** | è¤‡æ•°ã® Composite ã‚’çµ±åˆ    | morning_routine (ä»Šå¾Œ)        | Composite + Atomic ã‚’çµ±åˆ |
```

ã“ã®3å±¤æ§‹é€ ã¯ **å†åˆ©ç”¨æ€§ã¨æ‹¡å¼µæ€§ã‚’æœ€å¤§åŒ–** ã—ã¦ã„ã¾ã™ã€‚LEGO ãƒ–ãƒ­ãƒƒã‚¯ã®ã‚ˆã†ã«ã€å°ã•ãªéƒ¨å“ã‚’çµ„ã¿åˆã‚ã›ã¦å¤§ããªæ©Ÿèƒ½ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚

#### 1.3 ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹åˆ†é›¢ã®åŸå‰‡ï¼ˆISPï¼‰ã®é©ç”¨

```8:109:src/core/providers/llm.py
class LLMProvider(ABC):
    """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    
    ã“ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’å®Ÿè£…ã™ã‚‹ã“ã¨ã§ã€ç•°ãªã‚‹LLMã‚µãƒ¼ãƒ“ã‚¹
    ï¼ˆGemini, OpenAI, Anthropicãªã©ï¼‰ã‚’çµ±ä¸€çš„ã«æ‰±ãˆã¾ã™ã€‚
    
    Example:
        >>> provider = GeminiProvider(api_key="...")
        >>> response = await provider.generate("Hello!")
        >>> print(response)
    """
    
    @abstractmethod
    async def generate(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            temperature: ç”Ÿæˆã®å¤šæ§˜æ€§ï¼ˆ0.0-1.0ã€é«˜ã„ã»ã©å¤šæ§˜ï¼‰
            max_tokens: æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°
            **kwargs: ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        
        Raises:
            Exception: ç”Ÿæˆã«å¤±æ•—ã—ãŸå ´åˆ
        """
        pass
    
    @abstractmethod
    async def generate_json(
        self,
        prompt: str,
        schema: Type[BaseModel],
        temperature: float = 0.7,
        **kwargs
    ) -> BaseModel:
        """æ§‹é€ åŒ–ã•ã‚ŒãŸJSONå‡ºåŠ›ã‚’ç”Ÿæˆ
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            schema: Pydanticã‚¹ã‚­ãƒ¼ãƒï¼ˆå‡ºåŠ›ã®å‹å®šç¾©ï¼‰
            temperature: ç”Ÿæˆã®å¤šæ§˜æ€§
            **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
        Returns:
            Pydanticãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        
        Raises:
            ValueError: JSONã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆ
            Exception: ç”Ÿæˆã«å¤±æ•—ã—ãŸå ´åˆ
        
        Example:
            >>> class TodoItem(BaseModel):
            ...     title: str
            ...     priority: str
            >>> 
            >>> result = await provider.generate_json(
            ...     "TODOã‚’ä½œæˆã—ã¦ãã ã•ã„",
            ...     schema=TodoItem
            ... )
            >>> print(result.title)
        """
        pass
    
    @abstractmethod
    async def generate_with_context(
        self,
        user_query: str,
        context: str,
        system_instruction: Optional[str] = None,
        temperature: float = 0.7,
        **kwargs
    ) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ããƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆRAGç”¨ï¼‰
        
        Args:
            user_query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            context: å‚è€ƒæƒ…å ±ï¼ˆæ¤œç´¢çµæœã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãªã©ï¼‰
            system_instruction: ã‚·ã‚¹ãƒ†ãƒ å‘½ä»¤ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            temperature: ç”Ÿæˆã®å¤šæ§˜æ€§
            **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        
        Example:
            >>> answer = await provider.generate_with_context(
            ...     user_query="æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ï¼Ÿ",
            ...     context="æ©Ÿæ¢°å­¦ç¿’ã¯AIã®ä¸€åˆ†é‡ã§...",
            ...     system_instruction="å°‚é–€å®¶ã¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„"
            ... )
        """
        pass
```

æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãŒæ˜ç¢ºã§ã€è¤‡æ•°ã®å®Ÿè£…ï¼ˆGeminiã€Mock ãªã©ï¼‰ã‚’ç°¡å˜ã«åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã¾ã™ã€‚

### âš ï¸ **æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚‹ç‚¹**

#### 1.4 Service Layer ã®å½¹å‰²ãŒæ›–æ˜§

```1:53:src/services/llm/base.py
"""LLM Services Base Classes"""

from abc import ABC, abstractmethod
from typing import Optional, Type, Dict, Any
from pydantic import BaseModel


class BaseLLMService(ABC):
    """LLMã‚µãƒ¼ãƒ“ã‚¹ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    async def generate(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            temperature: ç”Ÿæˆã®å¤šæ§˜æ€§ï¼ˆ0.0-1.0ï¼‰
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            **kwargs: ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        pass
    
    @abstractmethod
    async def generate_json(
        self,
        prompt: str,
        schema: Type[BaseModel],
        **kwargs
    ) -> BaseModel:
        """
        æ§‹é€ åŒ–ã•ã‚ŒãŸJSONå‡ºåŠ›ã‚’ç”Ÿæˆ
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            schema: Pydanticã‚¹ã‚­ãƒ¼ãƒ
            **kwargs: ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
        Returns:
            Pydanticãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        pass
```

`BaseLLMService` ã¨ `LLMProvider` ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãŒã»ã¼åŒã˜ã§ã™ã€‚ã“ã‚Œã¯ **é‡è¤‡** ã§ã‚ã‚Šã€å½¹å‰²åˆ†æ‹…ãŒä¸æ˜ç¢ºã§ã™ã€‚ç¾åœ¨ã€Service Layer ã¯å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã¨ã—ã¦æ©Ÿèƒ½ã—ã¦ã„ã¾ã™ãŒã€é•·æœŸçš„ã«ã¯æ•´ç†ãŒå¿…è¦ã§ã™ã€‚

#### 1.5 MCPï¼ˆModel Context Protocolï¼‰å±¤ã®çµ±åˆãŒä¸å®Œå…¨

MCP å±¤ã¯ç‹¬ç«‹ã—ã¦ã„ã¾ã™ãŒã€ä»–ã®å±¤ã¨ã®çµ±åˆãŒé™å®šçš„ã§ã™ã€‚ã‚ˆã‚Šä¸€è²«ã—ãŸæ–¹æ³•ã§ Provider ãƒ‘ã‚¿ãƒ¼ãƒ³ã«çµ±åˆã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

---

## 2. ğŸ’‰ ä¾å­˜æ€§æ³¨å…¥ã®è¦³ç‚¹

### âœ… **å„ªã‚Œã¦ã„ã‚‹ç‚¹ï¼ˆã‚¹ã‚³ã‚¢: 9.5/10ï¼‰**

#### 2.1 ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆConstructor Injectionï¼‰ã®å¾¹åº•

```50:65:src/workflows/atomic/chat.py
    def __init__(self, llm_provider: Optional[LLMProvider] = None):
        """
        Args:
            llm_provider: LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆçœç•¥æ™‚ã¯GeminiProviderï¼‰
        """
        # âœ… ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒæŒ‡å®šã•ã‚Œãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨
        if llm_provider is None:
            llm_provider = GeminiProvider(
                api_key=settings.gemini_api_key,
                model="gemini-2.0-flash-exp"
            )
        
        self.llm_node = LLMNode(provider=llm_provider)
        self.graph = self._build_graph()
        logger.info(f"ChatWorkflow initialized with {llm_provider.__class__.__name__}")
```

DI ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒ **å®Œç’§ã«å®Ÿè£…** ã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’æä¾›ã—ã¤ã¤ã€ãƒ†ã‚¹ãƒˆæ™‚ã«ã¯ãƒ¢ãƒƒã‚¯ã‚’æ³¨å…¥ã§ãã‚‹è¨­è¨ˆã§ã™ã€‚

#### 2.2 ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®Ÿè£…

```16:93:src/core/factory.py
class ProviderFactory:
    """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ï¼ˆè»½é‡ç‰ˆï¼‰
    
    å„ç¨®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ç”Ÿæˆã‚’ä¸€å…ƒç®¡ç†ã—ã¾ã™ã€‚
    è¨­å®šãƒ™ãƒ¼ã‚¹ã§ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç”Ÿæˆã§ãã€æ–°ã—ã„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ç™»éŒ²ã‚‚å¯èƒ½ã§ã™ã€‚
    
    Example:
        >>> # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§ç”Ÿæˆ
        >>> provider = ProviderFactory.create_llm_provider()
        >>> 
        >>> # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ç”Ÿæˆ
        >>> provider = ProviderFactory.create_llm_provider(
        ...     provider_type="gemini",
        ...     config={"model": "gemini-pro"}
        ... )
        >>> 
        >>> # æ–°ã—ã„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç™»éŒ²
        >>> ProviderFactory.register_llm_provider("openai", OpenAIProvider)
    """

    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ¬ã‚¸ã‚¹ãƒˆãƒª
    _llm_providers: Dict[str, Type[LLMProvider]] = {
        "gemini": GeminiProvider,
        "mock": MockLLMProvider,
    }
    
    _rag_providers: Dict[str, Type[RAGProvider]] = {
        "simple": SimpleRAGProvider,
    }

    @classmethod
    def create_llm_provider(
        cls,
        provider_type: str = "gemini",
        config: Optional[Dict[str, Any]] = None
    ) -> LLMProvider:
        """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç”Ÿæˆ
        
        Args:
            provider_type: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚¿ã‚¤ãƒ—ï¼ˆ"gemini", "mock"ãªã©ï¼‰
            config: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å›ºæœ‰ã®è¨­å®š
        
        Returns:
            LLMProvider: ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
        
        Raises:
            ValueError: æœªçŸ¥ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚¿ã‚¤ãƒ—ã®å ´åˆ
        
        Example:
            >>> # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆGeminiï¼‰
            >>> provider = ProviderFactory.create_llm_provider()
            >>> 
            >>> # ãƒ¢ãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
            >>> provider = ProviderFactory.create_llm_provider(
            ...     provider_type="mock",
            ...     config={"default_response": "Test response"}
            ... )
        """
        if provider_type not in cls._llm_providers:
            available = ", ".join(cls._llm_providers.keys())
            raise ValueError(
                f"Unknown LLM provider type: {provider_type}. "
                f"Available: {available}"
            )

        provider_class = cls._llm_providers[provider_type]
        config = config or {}

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’é©ç”¨
        if provider_type == "gemini":
            if "api_key" not in config:
                config["api_key"] = settings.gemini_api_key
            if "model" not in config:
                config["model"] = "gemini-2.0-flash-exp"

        logger.info(f"Creating LLM provider: {provider_type}")
        return provider_class(**config)
```

ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚Šã€**ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ç”ŸæˆãŒä¸€å…ƒç®¡ç†** ã•ã‚Œã¦ã„ã¾ã™ã€‚æ–°ã—ã„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ç™»éŒ²ã‚‚ç°¡å˜ã§ã™ã€‚

#### 2.3 ãƒ†ã‚¹ã‚¿ãƒ“ãƒªãƒ†ã‚£ã®å‘ä¸Š

ãƒ¢ãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€å¤–éƒ¨ API ã«ä¾å­˜ã›ãšã«ãƒ†ã‚¹ãƒˆã§ãã¾ã™ã€‚ã“ã‚Œã¯ **æœ¬ç•ªãƒ¬ãƒ™ãƒ«ã®ãƒ†ã‚¹ãƒˆæˆ¦ç•¥** ã§ã™ã€‚

### âš ï¸ **æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚‹ç‚¹**

#### 2.4 DI ã‚³ãƒ³ãƒ†ãƒŠã®ä¸åœ¨

ç¾åœ¨ã€ä¾å­˜æ€§ã®è§£æ±ºã¯å„ã‚¯ãƒ©ã‚¹ã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã§è¡Œã‚ã‚Œã¦ã„ã¾ã™ã€‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒå¤§ãããªã‚‹ã¨ã€**DI ã‚³ãƒ³ãƒ†ãƒŠ**ï¼ˆä¾‹: `dependency-injector` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸç®¡ç†ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

```python
# å°†æ¥çš„ãªæ”¹å–„æ¡ˆ
from dependency_injector import containers, providers

class Container(containers.DeclarativeContainer):
    config = providers.Configuration()
    
    # Providers
    llm_provider = providers.Singleton(
        GeminiProvider,
        api_key=config.gemini_api_key,
        model=config.model
    )
    
    # Workflows
    chat_workflow = providers.Factory(
        ChatWorkflow,
        llm_provider=llm_provider
    )
```

---

## 3. ğŸš€ æ‹¡å¼µã®ã—ã‚„ã™ã•ã®è¦³ç‚¹

### âœ… **å„ªã‚Œã¦ã„ã‚‹ç‚¹ï¼ˆã‚¹ã‚³ã‚¢: 9/10ï¼‰**

#### 3.1 ãƒ—ãƒ©ã‚°ã‚¤ãƒ³çš„ãªæ‹¡å¼µãŒå¯èƒ½

```128:163:src/core/factory.py
    @classmethod
    def register_llm_provider(
        cls,
        name: str,
        provider_class: Type[LLMProvider]
    ):
        """æ–°ã—ã„LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç™»éŒ²ï¼ˆæ‹¡å¼µç”¨ï¼‰
        
        Args:
            name: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å
            provider_class: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚¯ãƒ©ã‚¹
        
        Example:
            >>> class CustomProvider(LLMProvider):
            ...     pass
            >>> 
            >>> ProviderFactory.register_llm_provider("custom", CustomProvider)
            >>> provider = ProviderFactory.create_llm_provider("custom")
        """
        logger.info(f"Registering LLM provider: {name}")
        cls._llm_providers[name] = provider_class
    
    @classmethod
    def register_rag_provider(
        cls,
        name: str,
        provider_class: Type[RAGProvider]
    ):
        """æ–°ã—ã„RAGãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç™»éŒ²ï¼ˆæ‹¡å¼µç”¨ï¼‰
        
        Args:
            name: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å
            provider_class: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚¯ãƒ©ã‚¹
        """
        logger.info(f"Registering RAG provider: {name}")
        cls._rag_providers[name] = provider_class
```

æ–°ã—ã„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®è¿½åŠ ãŒ **ç™»éŒ²ã™ã‚‹ã ã‘** ã§å®Œäº†ã—ã¾ã™ã€‚ã“ã‚Œã¯ **Open/Closed åŸå‰‡** ã®å„ªã‚ŒãŸå®Ÿè£…ã§ã™ã€‚

#### 3.2 æ®µéšçš„ãªæ©Ÿèƒ½è¿½åŠ ãŒå®¹æ˜“

```python
# Step 1: æ–°ã—ã„Providerã‚’ä½œæˆ
class OpenAIProvider(LLMProvider):
    async def generate(self, prompt: str, **kwargs) -> str:
        # OpenAI APIå®Ÿè£…
        ...

# Step 2: ç™»éŒ²
ProviderFactory.register_llm_provider("openai", OpenAIProvider)

# Step 3: ä½¿ç”¨
workflow = ChatWorkflow(
    llm_provider=ProviderFactory.create_llm_provider("openai")
)
```

#### 3.3 è¨­å®šãƒ™ãƒ¼ã‚¹ã®æŸ”è»Ÿæ€§

```9:42:src/core/config.py
class Settings:
    """Simple settings class for environment configuration"""

    def __init__(self):
        self.app_name: str = "LangGraph Training"
        self.debug: bool = True

        # API Keys
        self.gemini_api_key: Optional[str] = os.getenv("GEMINI_API_KEY")
        self.slack_token: Optional[str] = os.getenv("SLACK_TOKEN")

        # Jira Configuration
        self.jira_token: Optional[str] = os.getenv("JIRA_TOKEN")
        self.jira_server: Optional[str] = os.getenv("JIRA_SERVER")
        self.jira_email: Optional[str] = os.getenv("JIRA_EMAIL")

        # RAG Configuration
        self.embedding_model: str = os.getenv("EMBEDDING_MODEL", "gemini")
        self.embedding_dimension: int = int(os.getenv("EMBEDDING_DIMENSION", "768"))
        self.gemini_embedding_model: str = os.getenv("GEMINI_EMBEDDING_MODEL", "models/embedding-001")

        # Supabase Configuration
        self.supabase_url: Optional[str] = os.getenv("SUPABASE_URL")
        self.supabase_key: Optional[str] = os.getenv("SUPABASE_KEY")
        self.supabase_service_key: Optional[str] = os.getenv("SUPABASE_SERVICE_KEY")

        # Document Processing
        self.max_chunk_size: int = int(os.getenv("MAX_CHUNK_SIZE", "1000"))
        self.chunk_overlap: int = int(os.getenv("CHUNK_OVERLAP", "200"))

        # Search & Retrieval
        self.similarity_threshold: float = float(os.getenv("SIMILARITY_THRESHOLD", "0.7"))
        self.max_retrieval_results: int = int(os.getenv("MAX_RETRIEVAL_RESULTS", "5"))
```

ç’°å¢ƒå¤‰æ•°ãƒ™ãƒ¼ã‚¹ã®è¨­å®šã«ã‚ˆã‚Šã€ç’°å¢ƒã”ã¨ã«ç•°ãªã‚‹è¨­å®šã‚’ç°¡å˜ã«é©ç”¨ã§ãã¾ã™ã€‚

### âš ï¸ **æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚‹ç‚¹**

#### 3.4 è¨­å®šç®¡ç†ã®æ”¹å–„

ç¾åœ¨ã® `Settings` ã‚¯ãƒ©ã‚¹ã¯æ©Ÿèƒ½çš„ã§ã™ãŒã€ä»¥ä¸‹ã®æ”¹å–„ãŒå¯èƒ½ã§ã™ï¼š

```python
# æ”¹å–„æ¡ˆ: Pydanticãƒ™ãƒ¼ã‚¹ã®è¨­å®šç®¡ç†
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    app_name: str = "LangGraph Training"
    debug: bool = True
    
    # API Keys
    gemini_api_key: str
    slack_token: Optional[str] = None
    
    # è¨­å®šã®æ¤œè¨¼
    @validator('gemini_api_key')
    def validate_api_key(cls, v):
        if not v:
            raise ValueError('GEMINI_API_KEY is required')
        return v
    
    class Config:
        env_file = '.env'
        case_sensitive = False
```

#### 3.5 ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®è‡ªå‹•èª­ã¿è¾¼ã¿

ç¾åœ¨ã¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’æ‰‹å‹•ã§ç™»éŒ²ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å°†æ¥çš„ã«ã¯ã€ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰è‡ªå‹•èª­ã¿è¾¼ã¿ã™ã‚‹ä»•çµ„ã¿ãŒã‚ã‚‹ã¨ä¾¿åˆ©ã§ã™ã€‚

```python
# æ”¹å–„æ¡ˆ: ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®è‡ªå‹•èª­ã¿è¾¼ã¿
import importlib
import pkgutil

def auto_register_providers(package_name: str):
    """æŒ‡å®šãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‹ã‚‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è‡ªå‹•ç™»éŒ²"""
    package = importlib.import_module(package_name)
    for _, module_name, _ in pkgutil.iter_modules(package.__path__):
        module = importlib.import_module(f"{package_name}.{module_name}")
        # LLMProviderã®ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦ç™»éŒ²
        ...
```

---

## 4. ğŸ¯ ãã®ä»–ã®è©•ä¾¡è¦³ç‚¹

### 4.1 ã‚³ãƒ¼ãƒ‰å“è³ªï¼ˆã‚¹ã‚³ã‚¢: 8.5/10ï¼‰

**âœ… è‰¯ã„ç‚¹ï¼š**
- å‹ãƒ’ãƒ³ãƒˆå®Œå‚™
- Docstring å……å®Ÿ
- Async/Await ã®é©åˆ‡ãªä½¿ç”¨
- Pydantic ã«ã‚ˆã‚‹å‹å®‰å…¨æ€§

**âš ï¸ æ”¹å–„ç‚¹ï¼š**
- ã„ãã¤ã‹ã®ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒä¸€èˆ¬çš„ã™ãã‚‹ï¼ˆ`Exception` ã® catchï¼‰
- ãƒ­ã‚®ãƒ³ã‚°ãƒ¬ãƒ™ãƒ«ã®çµ±ä¸€æ€§ãŒä¸ååˆ†

### 4.2 ãƒ†ã‚¹ã‚¿ãƒ“ãƒªãƒ†ã‚£ï¼ˆã‚¹ã‚³ã‚¢: 9/10ï¼‰

**âœ… è‰¯ã„ç‚¹ï¼š**
- ãƒ¢ãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å®Ÿè£…
- 33+ ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
- DI ã«ã‚ˆã‚Šå˜ä½“ãƒ†ã‚¹ãƒˆãŒå®¹æ˜“

**âš ï¸ æ”¹å–„ç‚¹ï¼š**
- çµ±åˆãƒ†ã‚¹ãƒˆã®ä¸è¶³
- ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆã®ä¸åœ¨

### 4.3 ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆã‚¹ã‚³ã‚¢: 9.5/10ï¼‰

**âœ… è‰¯ã„ç‚¹ï¼š**
- å„ãƒ•ã‚§ãƒ¼ã‚ºã®å®Œäº†ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆPHASE1-4_COMPLETE.mdï¼‰
- ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆæ›¸ï¼ˆWORKFLOWS_NEW_ARCHITECTURE.mdï¼‰
- README ã®å……å®Ÿ
- ã‚³ãƒ¼ãƒ‰å†…ã®ã‚³ãƒ¡ãƒ³ãƒˆã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³

**âš ï¸ æ”¹å–„ç‚¹ï¼š**
- API ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆOpenAPI/Swaggerï¼‰ã®è‡ªå‹•ç”Ÿæˆã‚’æ´»ç”¨ï¼ˆFastAPI ã®æ¨™æº–æ©Ÿèƒ½ï¼‰
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¬ã‚¤ãƒ‰ã‚„ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã®è¿½åŠ 

### 4.4 å¾Œæ–¹äº’æ›æ€§ï¼ˆã‚¹ã‚³ã‚¢: 10/10ï¼‰

```176:210:src/api/routes_workflows.py
# ============================================================================
# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
# ============================================================================

@router.post("/simple-chat")
async def run_simple_chat_legacy(input_data: ChatInput):
    """å¾Œæ–¹äº’æ›æ€§: simple-chat â†’ atomic/chat"""
    return await run_chat(input_data)


@router.post("/ppt-summary")
async def run_ppt_summary_legacy(
    file: UploadFile = File(...),
    summary_style: str = "bullet_points"
):
    """å¾Œæ–¹äº’æ›æ€§: ppt-summary â†’ composite/ppt-summary"""
    return await run_ppt_summary(file, summary_style)


@router.post("/rag")
async def run_rag_legacy(input_data: RAGQueryInput):
    """å¾Œæ–¹äº’æ›æ€§: rag â†’ atomic/rag-query"""
    return await run_rag_query(input_data)


@router.post("/chain-of-thought")
async def run_chain_of_thought_legacy(input_data: ChainOfThoughtInput):
    """å¾Œæ–¹äº’æ›æ€§: chain-of-thought â†’ composite/chain-of-thought"""
    return await run_chain_of_thought(input_data)


@router.post("/reflection")
async def run_reflection_legacy(input_data: ReflectionInput):
    """å¾Œæ–¹äº’æ›æ€§: reflection â†’ composite/reflection"""
    return await run_reflection(input_data)
```

å¾Œæ–¹äº’æ›æ€§ã®ç¶­æŒãŒ **å®Œç’§** ã§ã™ã€‚æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã‚’å£Šã•ãšã«æ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ç§»è¡Œã§ãã¦ã„ã¾ã™ã€‚

---

## ğŸ“Š ç·åˆè©•ä¾¡

| è¦³ç‚¹ | ã‚¹ã‚³ã‚¢ | ã‚³ãƒ¡ãƒ³ãƒˆ |
|------|--------|----------|
| **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£** | 9.0/10 | ãƒ¬ã‚¤ãƒ¤ãƒ¼åˆ†é›¢ã€æ®µéšçš„åˆæˆãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå„ªç§€ã€‚Serviceå±¤ã®æ•´ç†ãŒèª²é¡Œã€‚ |
| **ä¾å­˜æ€§æ³¨å…¥** | 9.5/10 | DI ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®Ÿè£…ãŒå®Œç’§ã€‚DI ã‚³ãƒ³ãƒ†ãƒŠã®å°å…¥ã§æ›´ã«å‘ä¸Šå¯èƒ½ã€‚ |
| **æ‹¡å¼µæ€§** | 9.0/10 | ãƒ—ãƒ©ã‚°ã‚¤ãƒ³çš„æ‹¡å¼µãŒå®¹æ˜“ã€‚è‡ªå‹•èª­ã¿è¾¼ã¿æ©Ÿèƒ½ã§æ›´ã«æ”¹å–„å¯èƒ½ã€‚ |
| **ã‚³ãƒ¼ãƒ‰å“è³ª** | 8.5/10 | å‹å®‰å…¨æ€§ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå……å®Ÿã€‚ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®æ”¹å–„ä½™åœ°ã‚ã‚Šã€‚ |
| **ãƒ†ã‚¹ã‚¿ãƒ“ãƒªãƒ†ã‚£** | 9.0/10 | ãƒ¢ãƒƒã‚¯å¯¾å¿œã€å¤šæ•°ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã€‚çµ±åˆãƒ†ã‚¹ãƒˆã®è¿½åŠ ãŒæœ›ã¾ã—ã„ã€‚ |
| **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ** | 9.5/10 | éå¸¸ã«å……å®Ÿã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€‚ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã®è¿½åŠ ã§å®Œç’§ã«ã€‚ |
| **å¾Œæ–¹äº’æ›æ€§** | 10/10 | å®Œç’§ãªå¾Œæ–¹äº’æ›æ€§ã®ç¶­æŒã€‚ |

### **ç·åˆã‚¹ã‚³ã‚¢: 9.2/10 ğŸ‰**

---

## ğŸ¯ æ¨å¥¨ã•ã‚Œã‚‹æ”¹å–„äº‹é …ï¼ˆå„ªå…ˆåº¦é †ï¼‰

### é«˜å„ªå…ˆåº¦

1. **Service Layer ã®æ•´ç†**
   - `BaseLLMService` ã¨ `LLMProvider` ã®å½¹å‰²ã‚’æ˜ç¢ºåŒ–
   - é‡è¤‡ã‚’å‰Šé™¤ã—ã€Service Layer ã‚’å»ƒæ­¢ã™ã‚‹ã‹ã€æ˜ç¢ºãªä¾¡å€¤ã‚’æä¾›ã™ã‚‹è¨­è¨ˆã«å¤‰æ›´

2. **çµ±åˆãƒ†ã‚¹ãƒˆã®è¿½åŠ **
   - ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹è¿½åŠ 
   - ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆã¨ CI/CD ã¸ã®çµ±åˆ

3. **ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®æ”¹å–„**
   - ã‚«ã‚¹ã‚¿ãƒ ä¾‹å¤–ã‚¯ãƒ©ã‚¹ã®å®šç¾©
   - ã‚ˆã‚Šå…·ä½“çš„ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

### ä¸­å„ªå…ˆåº¦

4. **DI ã‚³ãƒ³ãƒ†ãƒŠã®å°å…¥**
   - `dependency-injector` ãªã©ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’æ¤œè¨
   - ä¾å­˜æ€§ã®ç®¡ç†ã‚’ã‚ˆã‚Šä½“ç³»çš„ã«

5. **è¨­å®šç®¡ç†ã®æ”¹å–„**
   - `pydantic-settings` ã¸ã®ç§»è¡Œ
   - è¨­å®šã®æ¤œè¨¼ã¨å‹å®‰å…¨æ€§ã®å¼·åŒ–

6. **ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®è‡ªå‹•èª­ã¿è¾¼ã¿**
   - ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®è‡ªå‹•æ¤œå‡º
   - å‹•çš„ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç™»éŒ²

### ä½å„ªå…ˆåº¦

7. **ç›£è¦–ã¨ãƒ­ã‚®ãƒ³ã‚°ã®å¼·åŒ–**
   - æ§‹é€ åŒ–ãƒ­ã‚®ãƒ³ã‚°ï¼ˆJSONå½¢å¼ï¼‰
   - ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ï¼ˆãƒ—ãƒ­ãƒ¡ãƒ†ã‚¦ã‚¹ãªã©ï¼‰

8. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–**
   - ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ—ãƒ¼ãƒ«ï¼ˆãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°ï¼‰
   - ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥

---

## ğŸ’¡ ã¾ã¨ã‚

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ **éå¸¸ã«é«˜å“è³ª** ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ä¸»ãªå¼·ã¿ã¯ï¼š

âœ… **æ˜ç¢ºãªãƒ¬ã‚¤ãƒ¤ãƒ¼åˆ†é›¢** - è²¬ä»»ãŒæ˜ç¢ºã§ä¿å®ˆã—ã‚„ã™ã„  
âœ… **å„ªã‚ŒãŸä¾å­˜æ€§æ³¨å…¥** - ãƒ†ã‚¹ã‚¿ãƒ“ãƒªãƒ†ã‚£ã¨æ‹¡å¼µæ€§ãŒé«˜ã„  
âœ… **æ®µéšçš„ãªåˆæˆãƒ‘ã‚¿ãƒ¼ãƒ³** - å†åˆ©ç”¨æ€§ãŒæœ€å¤§åŒ–ã•ã‚Œã¦ã„ã‚‹  
âœ… **å……å®Ÿã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ** - ç†è§£ã—ã‚„ã™ãã€å¼•ãç¶™ããŒå®¹æ˜“  
âœ… **å®Œç’§ãªå¾Œæ–¹äº’æ›æ€§** - æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã‚’å£Šã•ãªã„ç§»è¡Œ  

ã„ãã¤ã‹ã®æ”¹å–„ç‚¹ã¯ã‚ã‚Šã¾ã™ãŒã€ãã‚Œã‚‰ã¯ã€Œã‚ˆã‚Šè‰¯ãã™ã‚‹ãŸã‚ã€ã®ã‚‚ã®ã§ã‚ã‚Šã€ç¾çŠ¶ã§ã‚‚ **æœ¬ç•ªç’°å¢ƒã§ä½¿ç”¨ã§ãã‚‹ãƒ¬ãƒ™ãƒ«** ã«é”ã—ã¦ã„ã¾ã™ã€‚

ç‰¹ã«ã€**ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã®æ®µéšçš„ãªå®Ÿæ–½**ï¼ˆPhase 1-4ï¼‰ã«ã‚ˆã‚Šã€æŠ€è¡“çš„è² å‚µã‚’æ®‹ã•ãšã«æ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ç§»è¡Œã§ãã¦ã„ã‚‹ç‚¹ã¯ç§°è³›ã«å€¤ã—ã¾ã™ã€‚

ğŸ‰ **ç·åˆè©•ä¾¡: ã‚¨ã‚¯ã‚»ãƒ¬ãƒ³ãƒˆï¼ˆ9.2/10ï¼‰**

```plaintext
Application Layer (API)
    â†“
Workflow Layer (Atomic â†’ Composite â†’ Orchestrations)
    â†“
Node Layer (Primitives â†’ Composites)
    â†“
Provider/Service Layer
    â†“
Infrastructure Layer
```

```plaintext
| å±¤                 | å½¹å‰²                       | ä¾‹                            | çµ„ã¿åˆã‚ã›å…ƒ              |
| ------------------ | -------------------------- | ----------------------------- | ------------------------- |
| **Atomic**         | å˜ä¸€æ©Ÿèƒ½ã®æœ€å°å®Ÿè¡Œå˜ä½     | chat, rag_query               | Nodes å±¤ã‚’ä½¿ã†            |
| **Composite**      | è¤‡æ•°ã® Atomic ã‚’çµ„ã¿åˆã‚ã› | ppt_summary, chain_of_thought | Atomic ã‚’çµ„ã¿åˆã‚ã›ã‚‹     |
| **Orchestrations** | è¤‡æ•°ã® Composite ã‚’çµ±åˆ    | morning_routine (ä»Šå¾Œ)        | Composite + Atomic ã‚’çµ±åˆ |
```

```python
class LLMProvider(ABC):
    """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    
    ã“ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’å®Ÿè£…ã™ã‚‹ã“ã¨ã§ã€ç•°ãªã‚‹LLMã‚µãƒ¼ãƒ“ã‚¹
    ï¼ˆGemini, OpenAI, Anthropicãªã©ï¼‰ã‚’çµ±ä¸€çš„ã«æ‰±ãˆã¾ã™ã€‚
    
    Example:
        >>> provider = GeminiProvider(api_key="...")
        >>> response = await provider.generate("Hello!")
        >>> print(response)
    """
    
    @abstractmethod
    async def generate(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            temperature: ç”Ÿæˆã®å¤šæ§˜æ€§ï¼ˆ0.0-1.0ã€é«˜ã„ã»ã©å¤šæ§˜ï¼‰
            max_tokens: æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°
            **kwargs: ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        
        Raises:
            Exception: ç”Ÿæˆã«å¤±æ•—ã—ãŸå ´åˆ
        """
        pass
    
    @abstractmethod
    async def generate_json(
        self,
        prompt: str,
        schema: Type[BaseModel],
        temperature: float = 0.7,
        **kwargs
    ) -> BaseModel:
        """æ§‹é€ åŒ–ã•ã‚ŒãŸJSONå‡ºåŠ›ã‚’ç”Ÿæˆ
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            schema: Pydanticã‚¹ã‚­ãƒ¼ãƒï¼ˆå‡ºåŠ›ã®å‹å®šç¾©ï¼‰
            temperature: ç”Ÿæˆã®å¤šæ§˜æ€§
            **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
        Returns:
            Pydanticãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        
        Raises:
            ValueError: JSONã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆ
            Exception: ç”Ÿæˆã«å¤±æ•—ã—ãŸå ´åˆ
        
        Example:
            >>> class TodoItem(BaseModel):
            ...     title: str
            ...     priority: str
            >>> 
            >>> result = await provider.generate_json(
            ...     "TODOã‚’ä½œæˆã—ã¦ãã ã•ã„",
            ...     schema=TodoItem
            ... )
            >>> print(result.title)
        """
        pass
    
    @abstractmethod
    async def generate_with_context(
        self,
        user_query: str,
        context: str,
        system_instruction: Optional[str] = None,
        temperature: float = 0.7,
        **kwargs
    ) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ããƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆRAGç”¨ï¼‰
        
        Args:
            user_query: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
            context: å‚è€ƒæƒ…å ±ï¼ˆæ¤œç´¢çµæœã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãªã©ï¼‰
            system_instruction: ã‚·ã‚¹ãƒ†ãƒ å‘½ä»¤ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            temperature: ç”Ÿæˆã®å¤šæ§˜æ€§
            **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        
        Example:
            >>> answer = await provider.generate_with_context(
            ...     user_query="æ©Ÿæ¢°å­¦ç¿’ã¨ã¯ï¼Ÿ",
            ...     context="æ©Ÿæ¢°å­¦ç¿’ã¯AIã®ä¸€åˆ†é‡ã§...",
            ...     system_instruction="å°‚é–€å®¶ã¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„"
            ... )
        """
        pass
```

```python
"""LLM Services Base Classes"""

from abc import ABC, abstractmethod
from typing import Optional, Type, Dict, Any
from pydantic import BaseModel


class BaseLLMService(ABC):
    """LLMã‚µãƒ¼ãƒ“ã‚¹ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    async def generate(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            temperature: ç”Ÿæˆã®å¤šæ§˜æ€§ï¼ˆ0.0-1.0ï¼‰
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            **kwargs: ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        pass
    
    @abstractmethod
    async def generate_json(
        self,
        prompt: str,
        schema: Type[BaseModel],
        **kwargs
    ) -> BaseModel:
        """
        æ§‹é€ åŒ–ã•ã‚ŒãŸJSONå‡ºåŠ›ã‚’ç”Ÿæˆ
        
        Args:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            schema: Pydanticã‚¹ã‚­ãƒ¼ãƒ
            **kwargs: ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
        Returns:
            Pydanticãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        pass
```

```python
    def __init__(self, llm_provider: Optional[LLMProvider] = None):
        """
        Args:
            llm_provider: LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆçœç•¥æ™‚ã¯GeminiProviderï¼‰
        """
        # âœ… ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒæŒ‡å®šã•ã‚Œãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨
        if llm_provider is None:
            llm_provider = GeminiProvider(
                api_key=settings.gemini_api_key,
                model="gemini-2.0-flash-exp"
            )
        
        self.llm_node = LLMNode(provider=llm_provider)
        self.graph = self._build_graph()
        logger.info(f"ChatWorkflow initialized with {llm_provider.__class__.__name__}")
```

```python
class ProviderFactory:
    """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ï¼ˆè»½é‡ç‰ˆï¼‰
    
    å„ç¨®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ç”Ÿæˆã‚’ä¸€å…ƒç®¡ç†ã—ã¾ã™ã€‚
    è¨­å®šãƒ™ãƒ¼ã‚¹ã§ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç”Ÿæˆã§ãã€æ–°ã—ã„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ç™»éŒ²ã‚‚å¯èƒ½ã§ã™ã€‚
    
    Example:
        >>> # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§ç”Ÿæˆ
        >>> provider = ProviderFactory.create_llm_provider()
        >>> 
        >>> # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ç”Ÿæˆ
        >>> provider = ProviderFactory.create_llm_provider(
        ...     provider_type="gemini",
        ...     config={"model": "gemini-pro"}
        ... )
        >>> 
        >>> # æ–°ã—ã„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç™»éŒ²
        >>> ProviderFactory.register_llm_provider("openai", OpenAIProvider)
    """

    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ¬ã‚¸ã‚¹ãƒˆãƒª
    _llm_providers: Dict[str, Type[LLMProvider]] = {
        "gemini": GeminiProvider,
        "mock": MockLLMProvider,
    }
    
    _rag_providers: Dict[str, Type[RAGProvider]] = {
        "simple": SimpleRAGProvider,
    }

    @classmethod
    def create_llm_provider(
        cls,
        provider_type: str = "gemini",
        config: Optional[Dict[str, Any]] = None
    ) -> LLMProvider:
        """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç”Ÿæˆ
        
        Args:
            provider_type: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚¿ã‚¤ãƒ—ï¼ˆ"gemini", "mock"ãªã©ï¼‰
            config: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å›ºæœ‰ã®è¨­å®š
        
        Returns:
            LLMProvider: ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
        
        Raises:
            ValueError: æœªçŸ¥ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚¿ã‚¤ãƒ—ã®å ´åˆ
        
        Example:
            >>> # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆGeminiï¼‰
            >>> provider = ProviderFactory.create_llm_provider()
            >>> 
            >>> # ãƒ¢ãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
            >>> provider = ProviderFactory.create_llm_provider(
            ...     provider_type="mock",
            ...     config={"default_response": "Test response"}
            ... )
        """
        if provider_type not in cls._llm_providers:
            available = ", ".join(cls._llm_providers.keys())
            raise ValueError(
                f"Unknown LLM provider type: {provider_type}. "
                f"Available: {available}"
            )

        provider_class = cls._llm_providers[provider_type]
        config = config or {}

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’é©ç”¨
        if provider_type == "gemini":
            if "api_key" not in config:
                config["api_key"] = settings.gemini_api_key
            if "model" not in config:
                config["model"] = "gemini-2.0-flash-exp"

        logger.info(f"Creating LLM provider: {provider_type}")
        return provider_class(**config)
```

```python
# å°†æ¥çš„ãªæ”¹å–„æ¡ˆ
from dependency_injector import containers, providers

class Container(containers.DeclarativeContainer):
    config = providers.Configuration()
    
    # Providers
    llm_provider = providers.Singleton(
        GeminiProvider,
        api_key=config.gemini_api_key,
        model=config.model
    )
    
    # Workflows
    chat_workflow = providers.Factory(
        ChatWorkflow,
        llm_provider=llm_provider
    )
```

```python
    @classmethod
    def register_llm_provider(
        cls,
        name: str,
        provider_class: Type[LLMProvider]
    ):
        """æ–°ã—ã„LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç™»éŒ²ï¼ˆæ‹¡å¼µç”¨ï¼‰
        
        Args:
            name: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å
            provider_class: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚¯ãƒ©ã‚¹
        
        Example:
            >>> class CustomProvider(LLMProvider):
            ...     pass
            >>> 
            >>> ProviderFactory.register_llm_provider("custom", CustomProvider)
            >>> provider = ProviderFactory.create_llm_provider("custom")
        """
        logger.info(f"Registering LLM provider: {name}")
        cls._llm_providers[name] = provider_class
    
    @classmethod
    def register_rag_provider(
        cls,
        name: str,
        provider_class: Type[RAGProvider]
    ):
        """æ–°ã—ã„RAGãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç™»éŒ²ï¼ˆæ‹¡å¼µç”¨ï¼‰
        
        Args:
            name: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å
            provider_class: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚¯ãƒ©ã‚¹
        """
        logger.info(f"Registering RAG provider: {name}")
        cls._rag_providers[name] = provider_class
```

```python
# Step 1: æ–°ã—ã„Providerã‚’ä½œæˆ
class OpenAIProvider(LLMProvider):
    async def generate(self, prompt: str, **kwargs) -> str:
        # OpenAI APIå®Ÿè£…
        ...

# Step 2: ç™»éŒ²
ProviderFactory.register_llm_provider("openai", OpenAIProvider)

# Step 3: ä½¿ç”¨
workflow = ChatWorkflow(
    llm_provider=ProviderFactory.create_llm_provider("openai")
)
```

```python
class Settings:
    """Simple settings class for environment configuration"""

    def __init__(self):
        self.app_name: str = "LangGraph Training"
        self.debug: bool = True

        # API Keys
        self.gemini_api_key: Optional[str] = os.getenv("GEMINI_API_KEY")
        self.slack_token: Optional[str] = os.getenv("SLACK_TOKEN")

        # Jira Configuration
        self.jira_token: Optional[str] = os.getenv("JIRA_TOKEN")
        self.jira_server: Optional[str] = os.getenv("JIRA_SERVER")
        self.jira_email: Optional[str] = os.getenv("JIRA_EMAIL")

        # RAG Configuration
        self.embedding_model: str = os.getenv("EMBEDDING_MODEL", "gemini")
        self.embedding_dimension: int = int(os.getenv("EMBEDDING_DIMENSION", "768"))
        self.gemini_embedding_model: str = os.getenv("GEMINI_EMBEDDING_MODEL", "models/embedding-001")

        # Supabase Configuration
        self.supabase_url: Optional[str] = os.getenv("SUPABASE_URL")
        self.supabase_key: Optional[str] = os.getenv("SUPABASE_KEY")
        self.supabase_service_key: Optional[str] = os.getenv("SUPABASE_SERVICE_KEY")

        # Document Processing
        self.max_chunk_size: int = int(os.getenv("MAX_CHUNK_SIZE", "1000"))
        self.chunk_overlap: int = int(os.getenv("CHUNK_OVERLAP", "200"))

        # Search & Retrieval
        self.similarity_threshold: float = float(os.getenv("SIMILARITY_THRESHOLD", "0.7"))
        self.max_retrieval_results: int = int(os.getenv("MAX_RETRIEVAL_RESULTS", "5"))
```

```python
# æ”¹å–„æ¡ˆ: Pydanticãƒ™ãƒ¼ã‚¹ã®è¨­å®šç®¡ç†
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    app_name: str = "LangGraph Training"
    debug: bool = True
    
    # API Keys
    gemini_api_key: str
    slack_token: Optional[str] = None
    
    # è¨­å®šã®æ¤œè¨¼
    @validator('gemini_api_key')
    def validate_api_key(cls, v):
        if not v:
            raise ValueError('GEMINI_API_KEY is required')
        return v
    
    class Config:
        env_file = '.env'
        case_sensitive = False
```

```python
# æ”¹å–„æ¡ˆ: ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®è‡ªå‹•èª­ã¿è¾¼ã¿
import importlib
import pkgutil

def auto_register_providers(package_name: str):
    """æŒ‡å®šãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‹ã‚‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è‡ªå‹•ç™»éŒ²"""
    package = importlib.import_module(package_name)
    for _, module_name, _ in pkgutil.iter_modules(package.__path__):
        module = importlib.import_module(f"{package_name}.{module_name}")
        # LLMProviderã®ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦ç™»éŒ²
        ...
```

```python
# ============================================================================
# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
# ============================================================================

@router.post("/simple-chat")
async def run_simple_chat_legacy(input_data: ChatInput):
    """å¾Œæ–¹äº’æ›æ€§: simple-chat â†’ atomic/chat"""
    return await run_chat(input_data)


@router.post("/ppt-summary")
async def run_ppt_summary_legacy(
    file: UploadFile = File(...),
    summary_style: str = "bullet_points"
):
    """å¾Œæ–¹äº’æ›æ€§: ppt-summary â†’ composite/ppt-summary"""
    return await run_ppt_summary(file, summary_style)


@router.post("/rag")
async def run_rag_legacy(input_data: RAGQueryInput):
    """å¾Œæ–¹äº’æ›æ€§: rag â†’ atomic/rag-query"""
    return await run_rag_query(input_data)


@router.post("/chain-of-thought")
async def run_chain_of_thought_legacy(input_data: ChainOfThoughtInput):
    """å¾Œæ–¹äº’æ›æ€§: chain-of-thought â†’ composite/chain-of-thought"""
    return await run_chain_of_thought(input_data)


@router.post("/reflection")
async def run_reflection_legacy(input_data: ReflectionInput):
    """å¾Œæ–¹äº’æ›æ€§: reflection â†’ composite/reflection"""
    return await run_reflection(input_data)
```

