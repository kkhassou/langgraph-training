**User**

Dify ã«ã¨ã¯è¨€ã£ãŸã‚“ã§ã™ãŒã€
yml ã‚’ä½¿ã£ã¦ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æ§‹ç¯‰ã¯è€ƒãˆã¦ãªãã¦ã€
çµ„ã¿ç«‹ã¦ãŸå¾Œã«ã€ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§å‡ºåŠ›ã™ã‚‹æ©Ÿèƒ½ãŒ LangGraph ã«ã‚ã‚‹ã®ã§ã€ãã‚Œã‚’ä½¿ã£ã¦è¦‹ãˆã‚‹åŒ–ã—ã¦ã€ãã‚Œã‚’å…ƒã«æ›´ã«ãƒ–ãƒ©ãƒƒã‚·ãƒ¥ã‚¢ãƒƒãƒ—ã™ã‚‹ã¿ãŸã„ãªäº‹ã‚’è€ƒãˆã¦ã¾ã™ã€‚
ä¿®æ­£ã®æ–¹å‘æ€§ã¯æç¤ºã—ã¦ãã‚ŒãŸã‚‚ã®ã§æ¦‚ã­è‰¯ã„ã¨æ€ã„ã¾ã™ã€‚

---

**Cursor**

ç†è§£ã—ã¾ã—ãŸï¼**ã‚³ãƒ¼ãƒ‰ã§ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’çµ„ã¿ç«‹ã¦**ã€LangGraph ã®å¯è¦–åŒ–æ©Ÿèƒ½ã§ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹å½¢ã§ã™ã­ã€‚ãã‚Œãªã‚‰ã‚‚ã£ã¨ç¾å®Ÿçš„ã§æ®µéšçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¾ã™ã€‚

å®Œç’§ã§ã™ï¼ã™ã§ã«å¯è¦–åŒ–ã®ä»•çµ„ã¿ãŒã‚ã‚‹ã‚“ã§ã™ã­ã€‚ãã‚Œãªã‚‰ã€**ã‚³ãƒ¼ãƒ‰é§†å‹•ã‚’ç¶­æŒã—ã¤ã¤ã€æ‹¡å¼µæ€§ã‚’æœ€å¤§åŒ–ã™ã‚‹**æ®µéšçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¾ã™ã€‚

## ğŸ¯ ç¾å®Ÿçš„ãªæ”¹å–„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆã‚³ãƒ¼ãƒ‰é§†å‹•ç‰ˆï¼‰

YAML ã¯ä½¿ã‚ãšã€ã‚³ãƒ¼ãƒ‰ã§çµ„ã¿ç«‹ã¦ã¤ã¤ã€**éƒ¨å“ã®äº¤æ›å¯èƒ½æ€§**ã¨**ãƒ†ã‚¹ã‚¿ãƒ“ãƒªãƒ†ã‚£**ã‚’æœ€å¤§åŒ–ã™ã‚‹æ–¹å‘ã§ã™ã€‚

---

## ğŸ“ ææ¡ˆã™ã‚‹æ®µéšçš„ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°

### **Phase 1: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å±¤ã®æŠ½è±¡åŒ–ï¼ˆæœ€å„ªå…ˆï¼‰** â­

ç¾åœ¨ã®æœ€å¤§ã®å•é¡Œã¯ã€ã‚µãƒ¼ãƒ“ã‚¹ãŒå…·è±¡ã‚¯ãƒ©ã‚¹ã«ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã§ã™ã€‚

#### ç¾çŠ¶ã®å•é¡Œ

```python
# âŒ ç¾åœ¨: ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
class GeminiNode(BaseNode):
    async def execute(self, state: NodeState) -> NodeState:
        # GeminiServiceã«ç›´æ¥ä¾å­˜ï¼
        response_text = await GeminiService.generate(prompt=prompt, ...)
```

#### æ”¹å–„æ¡ˆ: Provider å±¤ã®å°å…¥

```python
# âœ… æ–°ã—ã„æ§‹é€ 
src/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ providers/              # æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ llm.py              # LLMProviderã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
â”‚       â”œâ”€â”€ rag.py              # RAGProviderã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
â”‚       â””â”€â”€ document.py         # DocumentProviderã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
â”‚
â”œâ”€â”€ providers/                  # å…·è±¡å®Ÿè£…
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ gemini.py          # GeminiProvider
â”‚   â”‚   â”œâ”€â”€ openai.py          # (å°†æ¥) OpenAIProvider
â”‚   â”‚   â””â”€â”€ mock.py            # (ãƒ†ã‚¹ãƒˆç”¨) MockLLMProvider
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ simple.py
â”‚   â”‚   â””â”€â”€ advanced.py
â”‚   â””â”€â”€ document/
â”‚       â””â”€â”€ ppt.py
```

#### å®Ÿè£…ä¾‹

```python
# core/providers/llm.py
from abc import ABC, abstractmethod
from typing import Optional, Type, Dict, Any
from pydantic import BaseModel

class LLMProvider(ABC):
    """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""

    @abstractmethod
    async def generate(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        pass

    @abstractmethod
    async def generate_json(
        self,
        prompt: str,
        schema: Type[BaseModel],
        temperature: float = 0.7,
        **kwargs
    ) -> BaseModel:
        """æ§‹é€ åŒ–JSONç”Ÿæˆ"""
        pass

    @abstractmethod
    async def generate_with_context(
        self,
        user_query: str,
        context: str,
        system_instruction: Optional[str] = None,
        **kwargs
    ) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ãç”Ÿæˆï¼ˆRAGç”¨ï¼‰"""
        pass


# providers/llm/gemini.py
from src.core.providers.llm import LLMProvider
import google.generativeai as genai

class GeminiProvider(LLMProvider):
    """Geminiå®Ÿè£…"""

    def __init__(
        self,
        api_key: str,
        model: str = "gemini-2.0-flash-exp"
    ):
        self.api_key = api_key
        self.model = model
        genai.configure(api_key=api_key)

    async def generate(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        generation_config = {
            "temperature": temperature,
            **kwargs
        }
        if max_tokens:
            generation_config["max_output_tokens"] = max_tokens

        model_instance = genai.GenerativeModel(
            model_name=self.model,
            generation_config=generation_config
        )

        response = model_instance.generate_content(prompt)
        return response.text.strip()

    async def generate_json(self, prompt: str, schema: Type[BaseModel], **kwargs) -> BaseModel:
        # å®Ÿè£…...
        pass

    async def generate_with_context(self, user_query: str, context: str, **kwargs) -> str:
        # å®Ÿè£…...
        pass


# providers/llm/mock.py - ãƒ†ã‚¹ãƒˆç”¨
class MockLLMProvider(LLMProvider):
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ¢ãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""

    def __init__(self, responses: Dict[str, str] = None):
        self.responses = responses or {}
        self.call_history = []

    async def generate(self, prompt: str, **kwargs) -> str:
        self.call_history.append({"method": "generate", "prompt": prompt, "kwargs": kwargs})
        return self.responses.get(prompt, "Mock response")

    async def generate_json(self, prompt: str, schema: Type[BaseModel], **kwargs) -> BaseModel:
        self.call_history.append({"method": "generate_json", "prompt": prompt})
        # ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
        return schema.parse_obj({"mock": "data"})

    async def generate_with_context(self, user_query: str, context: str, **kwargs) -> str:
        self.call_history.append({"method": "generate_with_context", "query": user_query})
        return f"Mock answer for: {user_query}"
```

#### ãƒãƒ¼ãƒ‰ã®æ”¹å–„

```python
# nodes/primitives/llm/gemini/node.py
from src.core.providers.llm import LLMProvider

class LLMNode(BaseNode):
    """LLMãƒãƒ¼ãƒ‰ï¼ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ³¨å…¥å¯èƒ½ï¼‰"""

    def __init__(self, provider: LLMProvider, name: str = "llm_node"):
        super().__init__(name=name)
        self.provider = provider  # æ³¨å…¥ã•ã‚ŒãŸæŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

    async def execute(self, state: NodeState) -> NodeState:
        try:
            prompt = state.messages[-1] if state.messages else state.data.get("prompt", "")
            temperature = state.data.get("temperature", 0.7)
            max_tokens = state.data.get("max_tokens")

            # âœ… æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’é€šã˜ã¦å‘¼ã³å‡ºã—
            response_text = await self.provider.generate(
                prompt=prompt,
                temperature=temperature,
                max_tokens=max_tokens
            )

            state.messages.append(response_text)
            state.data["llm_response"] = response_text
            state.metadata["node"] = self.name

            return state

        except Exception as e:
            logger.error(f"Error in LLM node: {e}")
            state.data["error"] = str(e)
            return state


# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
class GeminiNode(LLMNode):
    """Geminiãƒãƒ¼ãƒ‰ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰"""

    def __init__(self, api_key: str = None, model: str = "gemini-2.0-flash-exp"):
        from src.providers.llm.gemini import GeminiProvider
        from src.core.config import settings

        provider = GeminiProvider(
            api_key=api_key or settings.gemini_api_key,
            model=model
        )
        super().__init__(provider=provider, name="gemini_node")
```

---

### **Phase 2: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¸ã®ä¾å­˜æ€§æ³¨å…¥**

```python
# workflows/atomic/chat.py
from src.core.providers.llm import LLMProvider
from src.providers.llm.gemini import GeminiProvider

class ChatWorkflow:
    """ãƒãƒ£ãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ³¨å…¥å¯èƒ½ï¼‰"""

    def __init__(self, llm_provider: Optional[LLMProvider] = None):
        # âœ… ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒæŒ‡å®šã•ã‚Œãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨
        if llm_provider is None:
            from src.core.config import settings
            llm_provider = GeminiProvider(api_key=settings.gemini_api_key)

        self.llm_node = LLMNode(provider=llm_provider)
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        workflow = StateGraph(NodeState)
        workflow.add_node("gemini", self.llm_node.execute)
        workflow.add_edge(START, "gemini")
        workflow.add_edge("gemini", END)
        return workflow.compile()

    async def run(self, input_data: ChatInput) -> ChatOutput:
        # å®Ÿè£…...
        pass

    def get_mermaid_diagram(self) -> str:
        """LangGraphã®å¯è¦–åŒ–æ©Ÿèƒ½ã‚’ä½¿ç”¨"""
        return self.graph.get_graph().draw_mermaid()


# ä½¿ç”¨ä¾‹
# æœ¬ç•ªç’°å¢ƒ
chat_workflow = ChatWorkflow()  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§Gemini

# ãƒ†ã‚¹ãƒˆç’°å¢ƒ
mock_provider = MockLLMProvider(responses={"Hello": "Hi there!"})
chat_workflow = ChatWorkflow(llm_provider=mock_provider)

# ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«åˆ‡ã‚Šæ›¿ãˆï¼ˆå°†æ¥ï¼‰
openai_provider = OpenAIProvider(api_key="...")
chat_workflow = ChatWorkflow(llm_provider=openai_provider)
```

---

### **Phase 3: ã‚³ãƒ³ãƒã‚¸ãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®æ”¹å–„**

```python
# workflows/composite/document_analysis/ppt_summary.py
class PPTSummaryWorkflow:
    """PPTè¦ç´„ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼"""

    def __init__(
        self,
        llm_provider: Optional[LLMProvider] = None,
        document_provider: Optional[DocumentProvider] = None
    ):
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
        if llm_provider is None:
            from src.providers.llm.gemini import GeminiProvider
            llm_provider = GeminiProvider(api_key=settings.gemini_api_key)

        if document_provider is None:
            from src.providers.document.ppt import PPTProvider
            document_provider = PPTProvider()

        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’çµ„ã¿ç«‹ã¦
        self.extractor = DocumentExtractWorkflow(document_provider=document_provider)
        self.chat = ChatWorkflow(llm_provider=llm_provider)

    async def run(self, ppt_path: str) -> str:
        # Step 1: æŠ½å‡º
        extracted_text = await self.extractor.run(ppt_path)

        # Step 2: è¦ç´„
        prompt = f"ä»¥ä¸‹ã®ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å†…å®¹ã‚’è¦ç´„ã—ã¦ãã ã•ã„:\n\n{extracted_text}"
        summary = await self.chat.run(ChatInput(message=prompt))

        return summary.response

    def get_mermaid_diagram(self) -> str:
        """è¤‡åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å¯è¦–åŒ–"""
        # å†…éƒ¨ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’çµ„ã¿åˆã‚ã›ãŸå›³ã‚’ç”Ÿæˆ
        return f"""
graph TD
    START --> extract[Document Extract]
    extract --> chat[Chat LLM]
    chat --> END

    subgraph "Document Extract Workflow"
    {self.extractor.get_mermaid_diagram()}
    end

    subgraph "Chat Workflow"
    {self.chat.get_mermaid_diagram()}
    end
"""
```

---

### **Phase 4: ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆè»½é‡ç‰ˆï¼‰**

YAML ã¯ä½¿ã‚ãªã„ãŒã€**ã‚³ãƒ¼ãƒ‰ã‹ã‚‰æŸ”è»Ÿã«ç”Ÿæˆ**ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

```python
# core/factory.py
from typing import Dict, Any, Optional
from src.core.providers.llm import LLMProvider
from src.providers.llm.gemini import GeminiProvider
from src.providers.llm.mock import MockLLMProvider

class ProviderFactory:
    """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ï¼ˆè»½é‡ç‰ˆï¼‰"""

    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ¬ã‚¸ã‚¹ãƒˆãƒª
    _llm_providers = {
        "gemini": GeminiProvider,
        "mock": MockLLMProvider,
        # å°†æ¥: "openai": OpenAIProvider,
    }

    @classmethod
    def create_llm_provider(
        cls,
        provider_type: str = "gemini",
        config: Optional[Dict[str, Any]] = None
    ) -> LLMProvider:
        """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç”Ÿæˆ"""
        if provider_type not in cls._llm_providers:
            raise ValueError(f"Unknown provider type: {provider_type}")

        provider_class = cls._llm_providers[provider_type]
        config = config or {}

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        if provider_type == "gemini" and "api_key" not in config:
            from src.core.config import settings
            config["api_key"] = settings.gemini_api_key

        return provider_class(**config)

    @classmethod
    def register_llm_provider(cls, name: str, provider_class: Type[LLMProvider]):
        """æ–°ã—ã„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç™»éŒ²ï¼ˆæ‹¡å¼µç”¨ï¼‰"""
        cls._llm_providers[name] = provider_class


# ä½¿ç”¨ä¾‹
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
provider = ProviderFactory.create_llm_provider("gemini")

# ã‚«ã‚¹ã‚¿ãƒ è¨­å®š
provider = ProviderFactory.create_llm_provider(
    "gemini",
    config={"model": "gemini-pro", "api_key": "custom-key"}
)

# ãƒ¢ãƒƒã‚¯ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
provider = ProviderFactory.create_llm_provider(
    "mock",
    config={"responses": {"Hello": "Hi!"}}
)

# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ä½¿ç”¨
workflow = ChatWorkflow(
    llm_provider=ProviderFactory.create_llm_provider("gemini")
)
```

---

### **Phase 5: ãƒ†ã‚¹ãƒˆã®æ”¹å–„**

ä¾å­˜æ€§æ³¨å…¥ã«ã‚ˆã‚Šã€ãƒ†ã‚¹ãƒˆãŒåŠ‡çš„ã«ç°¡å˜ã«ãªã‚Šã¾ã™ã€‚

```python
# tests/test_chat_workflow.py
import pytest
from src.workflows.atomic.chat import ChatWorkflow, ChatInput
from src.providers.llm.mock import MockLLMProvider

@pytest.mark.asyncio
async def test_chat_workflow_basic():
    """åŸºæœ¬çš„ãªãƒãƒ£ãƒƒãƒˆãƒ•ãƒ­ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
    # âœ… ãƒ¢ãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’æ³¨å…¥
    mock_provider = MockLLMProvider(
        responses={
            "Hello": "Hi there! How can I help you today?"
        }
    )

    workflow = ChatWorkflow(llm_provider=mock_provider)

    # å®Ÿè¡Œ
    result = await workflow.run(ChatInput(message="Hello"))

    # æ¤œè¨¼
    assert result.success is True
    assert "Hi there" in result.response
    assert len(mock_provider.call_history) == 1
    assert mock_provider.call_history[0]["prompt"] == "Hello"


@pytest.mark.asyncio
async def test_chat_workflow_with_temperature():
    """æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ†ã‚¹ãƒˆ"""
    mock_provider = MockLLMProvider()
    workflow = ChatWorkflow(llm_provider=mock_provider)

    await workflow.run(ChatInput(message="Test", temperature=0.9))

    # ãƒ¢ãƒƒã‚¯ã®å‘¼ã³å‡ºã—å±¥æ­´ã‚’ç¢ºèª
    assert mock_provider.call_history[0]["kwargs"]["temperature"] == 0.9


@pytest.mark.asyncio
async def test_ppt_summary_workflow():
    """PPTè¦ç´„ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
    from src.workflows.composite.document_analysis.ppt_summary import PPTSummaryWorkflow
    from src.providers.document.mock import MockDocumentProvider

    # âœ… ä¸¡æ–¹ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ãƒ¢ãƒƒã‚¯åŒ–
    mock_llm = MockLLMProvider(responses={
        "ä»¥ä¸‹ã®ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å†…å®¹ã‚’è¦ç´„ã—ã¦ãã ã•ã„:\n\nSlide 1: Test": "Summary: Test presentation"
    })
    mock_doc = MockDocumentProvider(responses={
        "test.pptx": "Slide 1: Test"
    })

    workflow = PPTSummaryWorkflow(
        llm_provider=mock_llm,
        document_provider=mock_doc
    )

    result = await workflow.run("test.pptx")

    assert "Summary" in result
    assert len(mock_llm.call_history) == 1
    assert len(mock_doc.call_history) == 1
```

---

### **Phase 6: å¯è¦–åŒ–ã®å¼·åŒ–**

````python
# core/visualization.py
from typing import Dict, Any, List
from pathlib import Path

class WorkflowVisualizer:
    """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å¯è¦–åŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""

    @staticmethod
    def generate_mermaid(workflow) -> str:
        """Mermaidå›³ã‚’ç”Ÿæˆ"""
        return workflow.graph.get_graph().draw_mermaid()

    @staticmethod
    def save_diagram(workflow, output_path: Path, format: str = "mermaid"):
        """å›³ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        diagram = WorkflowVisualizer.generate_mermaid(workflow)

        if format == "mermaid":
            output_path.write_text(diagram)
        elif format == "html":
            html = WorkflowVisualizer._generate_html(diagram, workflow.name)
            output_path.write_text(html)

    @staticmethod
    def _generate_html(diagram: str, title: str) -> str:
        """HTMLãƒ©ãƒƒãƒ‘ãƒ¼ã‚’ç”Ÿæˆ"""
        return f"""<!DOCTYPE html>
<html>
<head>
    <title>{title}</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
    <h1>{title}</h1>
    <div class="mermaid">
{diagram}
    </div>
    <script>mermaid.initialize({{startOnLoad: true}});</script>
</body>
</html>"""

    @staticmethod
    def compare_workflows(workflows: Dict[str, Any]) -> str:
        """è¤‡æ•°ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ¯”è¼ƒ"""
        comparison = "# Workflow Comparison\n\n"
        for name, workflow in workflows.items():
            comparison += f"## {name}\n\n"
            comparison += f"```mermaid\n{workflow.get_mermaid_diagram()}\n```\n\n"
        return comparison


# ä½¿ç”¨ä¾‹
from src.workflows.atomic.chat import ChatWorkflow
from src.workflows.composite.document_analysis.ppt_summary import PPTSummaryWorkflow

# å˜ä¸€ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å¯è¦–åŒ–
chat = ChatWorkflow()
visualizer = WorkflowVisualizer()
visualizer.save_diagram(chat, Path("diagrams/chat.mmd"))
visualizer.save_diagram(chat, Path("diagrams/chat.html"), format="html")

# è¤‡æ•°ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®æ¯”è¼ƒ
workflows = {
    "Chat": ChatWorkflow(),
    "PPT Summary": PPTSummaryWorkflow(),
}
comparison = visualizer.compare_workflows(workflows)
Path("diagrams/comparison.md").write_text(comparison)
````

---

## ğŸ“Š æ®µéšçš„ãªå®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

### Week 1: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å±¤ã®æŠ½è±¡åŒ–

- [ ] `core/providers/llm.py` ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å®šç¾©
- [ ] `providers/llm/gemini.py` æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã®ç§»è¡Œ
- [ ] `providers/llm/mock.py` ãƒ†ã‚¹ãƒˆç”¨ãƒ¢ãƒƒã‚¯ä½œæˆ
- [ ] å¾Œæ–¹äº’æ›æ€§ã®ç¢ºä¿

### Week 2: ãƒãƒ¼ãƒ‰å±¤ã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°

- [ ] `LLMNode` ã®æ”¹å–„ï¼ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ³¨å…¥å¯¾å¿œï¼‰
- [ ] æ—¢å­˜ã®`GeminiNode`ã‚’ã‚¨ã‚¤ãƒªã‚¢ã‚¹åŒ–
- [ ] ä»–ã®ãƒãƒ¼ãƒ‰ï¼ˆRAGã€Documentï¼‰ã‚‚åŒæ§˜ã«å¯¾å¿œ

### Week 3: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å±¤ã®æ”¹å–„

- [ ] Atomic ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«ä¾å­˜æ€§æ³¨å…¥ã‚’è¿½åŠ 
- [ ] Composite ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚‚å¯¾å¿œ
- [ ] å¯è¦–åŒ–ãƒ¡ã‚½ãƒƒãƒ‰ (`get_mermaid_diagram()`) ã®çµ±ä¸€

### Week 4: ãƒ†ã‚¹ãƒˆã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [ ] å…¨ã¦ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
- [ ] å¯è¦–åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ›´æ–°
- [ ] README ã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ›´æ–°

---

## ğŸ¯ ãƒ¡ãƒªãƒƒãƒˆ

### 1. **ãƒ†ã‚¹ã‚¿ãƒ“ãƒªãƒ†ã‚£** âœ…

```python
# ãƒ¢ãƒƒã‚¯ã‚’æ³¨å…¥ã—ã¦ãƒ†ã‚¹ãƒˆ
mock = MockLLMProvider()
workflow = ChatWorkflow(llm_provider=mock)
```

### 2. **ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆ‡ã‚Šæ›¿ãˆ** âœ…

```python
# Gemini â†’ OpenAIï¼ˆå°†æ¥ï¼‰
workflow = ChatWorkflow(llm_provider=OpenAIProvider(...))
```

### 3. **ã‚³ãƒ¼ãƒ‰é§†å‹•ã‚’ç¶­æŒ** âœ…

- YAML ãªã—
- ã‚³ãƒ¼ãƒ‰ã§æŸ”è»Ÿã«çµ„ã¿ç«‹ã¦
- LangGraph ã®å¯è¦–åŒ–ã§ãƒ¬ãƒ“ãƒ¥ãƒ¼

### 4. **æ®µéšçš„ç§»è¡Œ** âœ…

- æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¯å‹•ä½œã—ç¶šã‘ã‚‹
- æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ã‹ã‚‰å¾ã€…ã«é©ç”¨
- å¾Œæ–¹äº’æ›æ€§ã‚’ä¿æŒ

### 5. **æ‹¡å¼µæ€§** âœ…

- æ–°ã—ã„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç°¡å˜ã«è¿½åŠ 
- ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã§ç®¡ç†
- ãƒ—ãƒ©ã‚°ã‚¤ãƒ³çš„ã«æ‹¡å¼µå¯èƒ½

---

ã“ã®æ–¹å‘æ€§ã¯ã„ã‹ãŒã§ã—ã‚‡ã†ã‹ï¼Ÿã‚³ãƒ¼ãƒ‰é§†å‹•ã‚’ç¶­æŒã—ã¤ã¤ã€ä¾å­˜æ€§æ³¨å…¥ã¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ‹¡å¼µæ€§ã‚’ç¢ºä¿ã§ãã¾ã™ã€‚å®Ÿè£…ã¯æ®µéšçš„ã«é€²ã‚ã‚‰ã‚Œã‚‹ã®ã§ã€æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã‚’å£Šã•ãšã«æ”¹å–„ã§ãã¾ã™ã€‚

```python
# âŒ ç¾åœ¨: ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
class GeminiNode(BaseNode):
    async def execute(self, state: NodeState) -> NodeState:
        # GeminiServiceã«ç›´æ¥ä¾å­˜ï¼
        response_text = await GeminiService.generate(prompt=prompt, ...)
```

```python
# âœ… æ–°ã—ã„æ§‹é€ 
src/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ providers/              # æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ llm.py              # LLMProviderã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
â”‚       â”œâ”€â”€ rag.py              # RAGProviderã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
â”‚       â””â”€â”€ document.py         # DocumentProviderã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
â”‚
â”œâ”€â”€ providers/                  # å…·è±¡å®Ÿè£…
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ gemini.py          # GeminiProvider
â”‚   â”‚   â”œâ”€â”€ openai.py          # (å°†æ¥) OpenAIProvider
â”‚   â”‚   â””â”€â”€ mock.py            # (ãƒ†ã‚¹ãƒˆç”¨) MockLLMProvider
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ simple.py
â”‚   â”‚   â””â”€â”€ advanced.py
â”‚   â””â”€â”€ document/
â”‚       â””â”€â”€ ppt.py
```

```python
# core/providers/llm.py
from abc import ABC, abstractmethod
from typing import Optional, Type, Dict, Any
from pydantic import BaseModel

class LLMProvider(ABC):
    """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""

    @abstractmethod
    async def generate(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        pass

    @abstractmethod
    async def generate_json(
        self,
        prompt: str,
        schema: Type[BaseModel],
        temperature: float = 0.7,
        **kwargs
    ) -> BaseModel:
        """æ§‹é€ åŒ–JSONç”Ÿæˆ"""
        pass

    @abstractmethod
    async def generate_with_context(
        self,
        user_query: str,
        context: str,
        system_instruction: Optional[str] = None,
        **kwargs
    ) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ãç”Ÿæˆï¼ˆRAGç”¨ï¼‰"""
        pass


# providers/llm/gemini.py
from src.core.providers.llm import LLMProvider
import google.generativeai as genai

class GeminiProvider(LLMProvider):
    """Geminiå®Ÿè£…"""

    def __init__(
        self,
        api_key: str,
        model: str = "gemini-2.0-flash-exp"
    ):
        self.api_key = api_key
        self.model = model
        genai.configure(api_key=api_key)

    async def generate(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        generation_config = {
            "temperature": temperature,
            **kwargs
        }
        if max_tokens:
            generation_config["max_output_tokens"] = max_tokens

        model_instance = genai.GenerativeModel(
            model_name=self.model,
            generation_config=generation_config
        )

        response = model_instance.generate_content(prompt)
        return response.text.strip()

    async def generate_json(self, prompt: str, schema: Type[BaseModel], **kwargs) -> BaseModel:
        # å®Ÿè£…...
        pass

    async def generate_with_context(self, user_query: str, context: str, **kwargs) -> str:
        # å®Ÿè£…...
        pass


# providers/llm/mock.py - ãƒ†ã‚¹ãƒˆç”¨
class MockLLMProvider(LLMProvider):
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ¢ãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""

    def __init__(self, responses: Dict[str, str] = None):
        self.responses = responses or {}
        self.call_history = []

    async def generate(self, prompt: str, **kwargs) -> str:
        self.call_history.append({"method": "generate", "prompt": prompt, "kwargs": kwargs})
        return self.responses.get(prompt, "Mock response")

    async def generate_json(self, prompt: str, schema: Type[BaseModel], **kwargs) -> BaseModel:
        self.call_history.append({"method": "generate_json", "prompt": prompt})
        # ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
        return schema.parse_obj({"mock": "data"})

    async def generate_with_context(self, user_query: str, context: str, **kwargs) -> str:
        self.call_history.append({"method": "generate_with_context", "query": user_query})
        return f"Mock answer for: {user_query}"
```

```python
# nodes/primitives/llm/gemini/node.py
from src.core.providers.llm import LLMProvider

class LLMNode(BaseNode):
    """LLMãƒãƒ¼ãƒ‰ï¼ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ³¨å…¥å¯èƒ½ï¼‰"""

    def __init__(self, provider: LLMProvider, name: str = "llm_node"):
        super().__init__(name=name)
        self.provider = provider  # æ³¨å…¥ã•ã‚ŒãŸæŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

    async def execute(self, state: NodeState) -> NodeState:
        try:
            prompt = state.messages[-1] if state.messages else state.data.get("prompt", "")
            temperature = state.data.get("temperature", 0.7)
            max_tokens = state.data.get("max_tokens")

            # âœ… æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’é€šã˜ã¦å‘¼ã³å‡ºã—
            response_text = await self.provider.generate(
                prompt=prompt,
                temperature=temperature,
                max_tokens=max_tokens
            )

            state.messages.append(response_text)
            state.data["llm_response"] = response_text
            state.metadata["node"] = self.name

            return state

        except Exception as e:
            logger.error(f"Error in LLM node: {e}")
            state.data["error"] = str(e)
            return state


# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
class GeminiNode(LLMNode):
    """Geminiãƒãƒ¼ãƒ‰ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰"""

    def __init__(self, api_key: str = None, model: str = "gemini-2.0-flash-exp"):
        from src.providers.llm.gemini import GeminiProvider
        from src.core.config import settings

        provider = GeminiProvider(
            api_key=api_key or settings.gemini_api_key,
            model=model
        )
        super().__init__(provider=provider, name="gemini_node")
```

```python
# workflows/atomic/chat.py
from src.core.providers.llm import LLMProvider
from src.providers.llm.gemini import GeminiProvider

class ChatWorkflow:
    """ãƒãƒ£ãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ³¨å…¥å¯èƒ½ï¼‰"""

    def __init__(self, llm_provider: Optional[LLMProvider] = None):
        # âœ… ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒæŒ‡å®šã•ã‚Œãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨
        if llm_provider is None:
            from src.core.config import settings
            llm_provider = GeminiProvider(api_key=settings.gemini_api_key)

        self.llm_node = LLMNode(provider=llm_provider)
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        workflow = StateGraph(NodeState)
        workflow.add_node("gemini", self.llm_node.execute)
        workflow.add_edge(START, "gemini")
        workflow.add_edge("gemini", END)
        return workflow.compile()

    async def run(self, input_data: ChatInput) -> ChatOutput:
        # å®Ÿè£…...
        pass

    def get_mermaid_diagram(self) -> str:
        """LangGraphã®å¯è¦–åŒ–æ©Ÿèƒ½ã‚’ä½¿ç”¨"""
        return self.graph.get_graph().draw_mermaid()


# ä½¿ç”¨ä¾‹
# æœ¬ç•ªç’°å¢ƒ
chat_workflow = ChatWorkflow()  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§Gemini

# ãƒ†ã‚¹ãƒˆç’°å¢ƒ
mock_provider = MockLLMProvider(responses={"Hello": "Hi there!"})
chat_workflow = ChatWorkflow(llm_provider=mock_provider)

# ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«åˆ‡ã‚Šæ›¿ãˆï¼ˆå°†æ¥ï¼‰
openai_provider = OpenAIProvider(api_key="...")
chat_workflow = ChatWorkflow(llm_provider=openai_provider)
```

```python
# workflows/composite/document_analysis/ppt_summary.py
class PPTSummaryWorkflow:
    """PPTè¦ç´„ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼"""

    def __init__(
        self,
        llm_provider: Optional[LLMProvider] = None,
        document_provider: Optional[DocumentProvider] = None
    ):
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
        if llm_provider is None:
            from src.providers.llm.gemini import GeminiProvider
            llm_provider = GeminiProvider(api_key=settings.gemini_api_key)

        if document_provider is None:
            from src.providers.document.ppt import PPTProvider
            document_provider = PPTProvider()

        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’çµ„ã¿ç«‹ã¦
        self.extractor = DocumentExtractWorkflow(document_provider=document_provider)
        self.chat = ChatWorkflow(llm_provider=llm_provider)

    async def run(self, ppt_path: str) -> str:
        # Step 1: æŠ½å‡º
        extracted_text = await self.extractor.run(ppt_path)

        # Step 2: è¦ç´„
        prompt = f"ä»¥ä¸‹ã®ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å†…å®¹ã‚’è¦ç´„ã—ã¦ãã ã•ã„:\n\n{extracted_text}"
        summary = await self.chat.run(ChatInput(message=prompt))

        return summary.response

    def get_mermaid_diagram(self) -> str:
        """è¤‡åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å¯è¦–åŒ–"""
        # å†…éƒ¨ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’çµ„ã¿åˆã‚ã›ãŸå›³ã‚’ç”Ÿæˆ
        return f"""
graph TD
    START --> extract[Document Extract]
    extract --> chat[Chat LLM]
    chat --> END

    subgraph "Document Extract Workflow"
    {self.extractor.get_mermaid_diagram()}
    end

    subgraph "Chat Workflow"
    {self.chat.get_mermaid_diagram()}
    end
"""
```

```python
# core/factory.py
from typing import Dict, Any, Optional
from src.core.providers.llm import LLMProvider
from src.providers.llm.gemini import GeminiProvider
from src.providers.llm.mock import MockLLMProvider

class ProviderFactory:
    """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ï¼ˆè»½é‡ç‰ˆï¼‰"""

    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ¬ã‚¸ã‚¹ãƒˆãƒª
    _llm_providers = {
        "gemini": GeminiProvider,
        "mock": MockLLMProvider,
        # å°†æ¥: "openai": OpenAIProvider,
    }

    @classmethod
    def create_llm_provider(
        cls,
        provider_type: str = "gemini",
        config: Optional[Dict[str, Any]] = None
    ) -> LLMProvider:
        """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç”Ÿæˆ"""
        if provider_type not in cls._llm_providers:
            raise ValueError(f"Unknown provider type: {provider_type}")

        provider_class = cls._llm_providers[provider_type]
        config = config or {}

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        if provider_type == "gemini" and "api_key" not in config:
            from src.core.config import settings
            config["api_key"] = settings.gemini_api_key

        return provider_class(**config)

    @classmethod
    def register_llm_provider(cls, name: str, provider_class: Type[LLMProvider]):
        """æ–°ã—ã„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç™»éŒ²ï¼ˆæ‹¡å¼µç”¨ï¼‰"""
        cls._llm_providers[name] = provider_class


# ä½¿ç”¨ä¾‹
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
provider = ProviderFactory.create_llm_provider("gemini")

# ã‚«ã‚¹ã‚¿ãƒ è¨­å®š
provider = ProviderFactory.create_llm_provider(
    "gemini",
    config={"model": "gemini-pro", "api_key": "custom-key"}
)

# ãƒ¢ãƒƒã‚¯ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
provider = ProviderFactory.create_llm_provider(
    "mock",
    config={"responses": {"Hello": "Hi!"}}
)

# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ä½¿ç”¨
workflow = ChatWorkflow(
    llm_provider=ProviderFactory.create_llm_provider("gemini")
)
```

```python
# tests/test_chat_workflow.py
import pytest
from src.workflows.atomic.chat import ChatWorkflow, ChatInput
from src.providers.llm.mock import MockLLMProvider

@pytest.mark.asyncio
async def test_chat_workflow_basic():
    """åŸºæœ¬çš„ãªãƒãƒ£ãƒƒãƒˆãƒ•ãƒ­ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
    # âœ… ãƒ¢ãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’æ³¨å…¥
    mock_provider = MockLLMProvider(
        responses={
            "Hello": "Hi there! How can I help you today?"
        }
    )

    workflow = ChatWorkflow(llm_provider=mock_provider)

    # å®Ÿè¡Œ
    result = await workflow.run(ChatInput(message="Hello"))

    # æ¤œè¨¼
    assert result.success is True
    assert "Hi there" in result.response
    assert len(mock_provider.call_history) == 1
    assert mock_provider.call_history[0]["prompt"] == "Hello"


@pytest.mark.asyncio
async def test_chat_workflow_with_temperature():
    """æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ†ã‚¹ãƒˆ"""
    mock_provider = MockLLMProvider()
    workflow = ChatWorkflow(llm_provider=mock_provider)

    await workflow.run(ChatInput(message="Test", temperature=0.9))

    # ãƒ¢ãƒƒã‚¯ã®å‘¼ã³å‡ºã—å±¥æ­´ã‚’ç¢ºèª
    assert mock_provider.call_history[0]["kwargs"]["temperature"] == 0.9


@pytest.mark.asyncio
async def test_ppt_summary_workflow():
    """PPTè¦ç´„ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
    from src.workflows.composite.document_analysis.ppt_summary import PPTSummaryWorkflow
    from src.providers.document.mock import MockDocumentProvider

    # âœ… ä¸¡æ–¹ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ãƒ¢ãƒƒã‚¯åŒ–
    mock_llm = MockLLMProvider(responses={
        "ä»¥ä¸‹ã®ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å†…å®¹ã‚’è¦ç´„ã—ã¦ãã ã•ã„:\n\nSlide 1: Test": "Summary: Test presentation"
    })
    mock_doc = MockDocumentProvider(responses={
        "test.pptx": "Slide 1: Test"
    })

    workflow = PPTSummaryWorkflow(
        llm_provider=mock_llm,
        document_provider=mock_doc
    )

    result = await workflow.run("test.pptx")

    assert "Summary" in result
    assert len(mock_llm.call_history) == 1
    assert len(mock_doc.call_history) == 1
```

````python
# core/visualization.py
from typing import Dict, Any, List
from pathlib import Path

class WorkflowVisualizer:
    """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å¯è¦–åŒ–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""

    @staticmethod
    def generate_mermaid(workflow) -> str:
        """Mermaidå›³ã‚’ç”Ÿæˆ"""
        return workflow.graph.get_graph().draw_mermaid()

    @staticmethod
    def save_diagram(workflow, output_path: Path, format: str = "mermaid"):
        """å›³ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        diagram = WorkflowVisualizer.generate_mermaid(workflow)

        if format == "mermaid":
            output_path.write_text(diagram)
        elif format == "html":
            html = WorkflowVisualizer._generate_html(diagram, workflow.name)
            output_path.write_text(html)

    @staticmethod
    def _generate_html(diagram: str, title: str) -> str:
        """HTMLãƒ©ãƒƒãƒ‘ãƒ¼ã‚’ç”Ÿæˆ"""
        return f"""<!DOCTYPE html>
<html>
<head>
    <title>{title}</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
    <h1>{title}</h1>
    <div class="mermaid">
{diagram}
    </div>
    <script>mermaid.initialize({{startOnLoad: true}});</script>
</body>
</html>"""

    @staticmethod
    def compare_workflows(workflows: Dict[str, Any]) -> str:
        """è¤‡æ•°ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ¯”è¼ƒ"""
        comparison = "# Workflow Comparison\n\n"
        for name, workflow in workflows.items():
            comparison += f"## {name}\n\n"
            comparison += f"```mermaid\n{workflow.get_mermaid_diagram()}\n```\n\n"
        return comparison


# ä½¿ç”¨ä¾‹
from src.workflows.atomic.chat import ChatWorkflow
from src.workflows.composite.document_analysis.ppt_summary import PPTSummaryWorkflow

# å˜ä¸€ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å¯è¦–åŒ–
chat = ChatWorkflow()
visualizer = WorkflowVisualizer()
visualizer.save_diagram(chat, Path("diagrams/chat.mmd"))
visualizer.save_diagram(chat, Path("diagrams/chat.html"), format="html")

# è¤‡æ•°ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®æ¯”è¼ƒ
workflows = {
    "Chat": ChatWorkflow(),
    "PPT Summary": PPTSummaryWorkflow(),
}
comparison = visualizer.compare_workflows(workflows)
Path("diagrams/comparison.md").write_text(comparison)
````

```python
# ãƒ¢ãƒƒã‚¯ã‚’æ³¨å…¥ã—ã¦ãƒ†ã‚¹ãƒˆ
mock = MockLLMProvider()
workflow = ChatWorkflow(llm_provider=mock)
```

```python
# Gemini â†’ OpenAIï¼ˆå°†æ¥ï¼‰
workflow = ChatWorkflow(llm_provider=OpenAIProvider(...))
```

---

**User**

ã“ã®ææ¡ˆã ã¨ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£çš„ã«ã¯ä½•ã«ãªã‚‹ã®ï¼Ÿ

---

**Cursor**

ã“ã®ææ¡ˆã¯ã€è¤‡æ•°ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’çµ„ã¿åˆã‚ã›ãŸã‚‚ã®ã§ã™ã€‚åå‰ã‚’ã¤ã‘ã‚‹ãªã‚‰ï¼š

## ğŸ›ï¸ **ãƒ—ãƒ©ã‚¬ãƒ–ãƒ«ãƒ»ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**

ï¼ˆPluggable Layered Architectureï¼‰

ã‚‚ã—ãã¯ã€ã‚ˆã‚Šæ­£å¼ã«ã¯ï¼š

## ğŸ¯ **ãƒãƒ¼ãƒˆ&ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ + ä¾å­˜æ€§æ³¨å…¥ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**

ï¼ˆPorts & Adapters with Dependency Injectionï¼‰

---

## ğŸ“ æ§‹æˆè¦ç´ ã¨è©²å½“ãƒ‘ã‚¿ãƒ¼ãƒ³

### 1. **ãƒãƒ¼ãƒˆ&ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆãƒ˜ã‚­ã‚µã‚´ãƒŠãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼‰**

åˆ¥åï¼šHexagonal Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Application Core               â”‚
â”‚  (Workflows, Nodes, Business Logic) â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Ports (Interfaces)       â”‚     â”‚
â”‚  â”‚  - LLMProvider           â”‚â”€â”€â”€â”€â”€â”¼â”€â†’ Adapters (å®Ÿè£…)
â”‚  â”‚  - RAGProvider           â”‚     â”‚   - GeminiProvider
â”‚  â”‚  - DocumentProvider      â”‚     â”‚   - OpenAIProvider
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   - MockProvider
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ç‰¹å¾´ï¼š**

- **ãƒãƒ¼ãƒˆï¼ˆPortï¼‰** = ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆ`LLMProvider`, `RAGProvider`ï¼‰
- **ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ï¼ˆAdapterï¼‰** = å®Ÿè£…ï¼ˆ`GeminiProvider`, `OpenAIProvider`ï¼‰
- ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯ã¯å¤–éƒ¨ä¾å­˜ã‹ã‚‰ç‹¬ç«‹

---

### 2. **ä¾å­˜æ€§æ³¨å…¥ï¼ˆDI: Dependency Injectionï¼‰**

```python
class ChatWorkflow:
    def __init__(self, llm_provider: LLMProvider):  # â† æ³¨å…¥
        self.provider = llm_provider
```

**ãƒ¡ãƒªãƒƒãƒˆï¼š**

- ç–çµåˆ
- ãƒ†ã‚¹ã‚¿ãƒ“ãƒªãƒ†ã‚£
- å®Ÿè£…ã®äº¤æ›å¯èƒ½æ€§

---

### 3. **ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆStrategy Patternï¼‰**

```python
# å®Ÿè¡Œæ™‚ã«ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’åˆ‡ã‚Šæ›¿ãˆ
workflow_with_gemini = ChatWorkflow(llm_provider=GeminiProvider())
workflow_with_openai = ChatWorkflow(llm_provider=OpenAIProvider())
workflow_with_mock = ChatWorkflow(llm_provider=MockProvider())
```

**ç‰¹å¾´ï¼š**

- åŒã˜ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ç•°ãªã‚‹å®Ÿè£…ã‚’åˆ‡ã‚Šæ›¿ãˆ
- ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ = ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ãƒ¼

---

### 4. **ãƒ¬ã‚¸ã‚¹ãƒˆãƒªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆRegistry Patternï¼‰**

```python
class ProviderFactory:
    _llm_providers = {
        "gemini": GeminiProvider,
        "openai": OpenAIProvider,
        "mock": MockProvider,
    }

    @classmethod
    def register_llm_provider(cls, name: str, provider_class):
        cls._llm_providers[name] = provider_class  # â† å‹•çš„ç™»éŒ²
```

**ãƒ¡ãƒªãƒƒãƒˆï¼š**

- ãƒ—ãƒ©ã‚°ã‚¤ãƒ³æ©Ÿæ§‹
- å®Ÿè¡Œæ™‚ã«åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç®¡ç†

---

### 5. **ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆFactory Patternï¼‰**

```python
provider = ProviderFactory.create_llm_provider("gemini", config={...})
```

**å½¹å‰²ï¼š**

- ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç”Ÿæˆã®è²¬ä»»ã‚’åˆ†é›¢
- è¨­å®šã‹ã‚‰é©åˆ‡ãªå®Ÿè£…ã‚’ç”Ÿæˆ

---

### 6. **ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆLayered Architectureï¼‰**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Layer (FastAPI)                â”‚  â† ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å±¤
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Workflows (Atomic/Composite/Orch)  â”‚  â† ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¤
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Nodes (å‡¦ç†å˜ä½)                    â”‚  â† ãƒ‰ãƒ¡ã‚¤ãƒ³å±¤
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Providers (æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹)    â”‚  â† ã‚¤ãƒ³ãƒ•ãƒ©æŠ½è±¡å±¤
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Concrete Providers (å®Ÿè£…)          â”‚  â† ã‚¤ãƒ³ãƒ•ãƒ©å®Ÿè£…å±¤
â”‚  - GeminiProvider                   â”‚
â”‚  - OpenAIProvider                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¨ å…¨ä½“åƒï¼š**ãƒ—ãƒ©ã‚¬ãƒ–ãƒ«ãƒ»ãƒãƒ¼ãƒˆ&ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ»ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**

æ­£å¼ãªå›³ã§è¡¨ã™ã¨ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Presentation Layer                      â”‚
â”‚                    (FastAPI Endpoints)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Application Layer                        â”‚
â”‚              (Workflows: Atomic, Composite)                 â”‚
â”‚                                                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚    â”‚ChatWorkflow â”‚      â”‚PPTSummary   â”‚                   â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Domain Layer                           â”‚
â”‚                   (Nodes: Processing Units)                 â”‚
â”‚                                                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚    â”‚  LLMNode    â”‚      â”‚  RAGNode    â”‚                   â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Infrastructure Abstraction Layer               â”‚
â”‚                   (Ports: Interfaces)                       â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  LLMProvider     â”‚  â”‚  RAGProvider     â”‚               â”‚
â”‚  â”‚  (Interface)     â”‚  â”‚  (Interface)     â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Infrastructure Implementation Layer            â”‚
â”‚                   (Adapters: Implementations)               â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚Gemini        â”‚  â”‚OpenAI        â”‚  â”‚Mock          â”‚    â”‚
â”‚  â”‚Provider      â”‚  â”‚Provider      â”‚  â”‚Provider      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚        Factory + Registry (Plugin System)        â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ·ï¸ ä»–ã®å‘¼ã³æ–¹

ã“ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€æ–‡è„ˆã«ã‚ˆã£ã¦ä»¥ä¸‹ã®ã‚ˆã†ã«å‘¼ã°ã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ï¼š

### 1. **ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£é¢¨**

ï¼ˆClean Architecture Inspiredï¼‰

- ä¾å­˜æ€§ã®æ–¹å‘ãŒå¤–å´ â†’ å†…å´ï¼ˆã‚¤ãƒ³ãƒ•ãƒ© â†’ ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼‰
- ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ãŒå¤–éƒ¨æŠ€è¡“ã‹ã‚‰ç‹¬ç«‹

### 2. **ã‚ªãƒ‹ã‚ªãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£çš„**

ï¼ˆOnion Architecture Likeï¼‰

- ä¸­å¿ƒã«ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆNodes, Workflowsï¼‰
- å¤–å´ã«ã‚¤ãƒ³ãƒ•ãƒ©ï¼ˆProvidersï¼‰

### 3. **ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**

ï¼ˆPlugin Architectureï¼‰

- ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ã‚ˆã‚‹å‹•çš„ãªæ‹¡å¼µ
- ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ = ãƒ—ãƒ©ã‚°ã‚¤ãƒ³

### 4. **IoCï¼ˆåˆ¶å¾¡ã®åè»¢ï¼‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**

ï¼ˆInversion of Control Architectureï¼‰

- ä¾å­˜æ€§æ³¨å…¥ã«ã‚ˆã‚Šåˆ¶å¾¡ã‚’åè»¢
- ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã¯ãªãã€ã‚¢ãƒ—ãƒªãŒä¸»å°æ¨©ã‚’æŒã¤

---

## ğŸ“Š ä¸»è¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¾ã¨ã‚è¡¨

| ãƒ‘ã‚¿ãƒ¼ãƒ³                 | ææ¡ˆã§ã®ä½¿ç”¨ç®‡æ‰€                                          | ç›®çš„                     |
| ------------------------ | --------------------------------------------------------- | ------------------------ |
| **Ports & Adapters**     | `LLMProvider`ï¼ˆãƒãƒ¼ãƒˆï¼‰<br>`GeminiProvider`ï¼ˆã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ï¼‰ | ã‚³ã‚¢ã¨ã‚¤ãƒ³ãƒ•ãƒ©ã®åˆ†é›¢     |
| **Dependency Injection** | `ChatWorkflow(llm_provider=...)`                          | ç–çµåˆãƒ»ãƒ†ã‚¹ã‚¿ãƒ“ãƒªãƒ†ã‚£   |
| **Strategy Pattern**     | ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆ‡ã‚Šæ›¿ãˆ                                    | ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®äº¤æ›å¯èƒ½æ€§ |
| **Registry Pattern**     | `ProviderFactory._llm_providers`                          | ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ç®¡ç†           |
| **Factory Pattern**      | `ProviderFactory.create_llm_provider()`                   | ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç”Ÿæˆã®é›†ä¸­åŒ– |
| **Layered Architecture** | API â†’ Workflows â†’ Nodes â†’ Providers                       | è²¬ä»»ã®åˆ†é›¢               |

---

## ğŸ¯ çµè«–

**æ­£å¼åç§°ï¼š**

> **ä¾å­˜æ€§æ³¨å…¥ã‚’å‚™ãˆãŸãƒ—ãƒ©ã‚¬ãƒ–ãƒ«ãƒ»ãƒãƒ¼ãƒˆ&ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒ»ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**  
> ï¼ˆPluggable Ports & Adapters Architecture with Dependency Injectionï¼‰

**çŸ­ç¸®ç‰ˆï¼š**

> **ãƒ—ãƒ©ã‚¬ãƒ–ãƒ«ãƒ»ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£** ã¾ãŸã¯ **ãƒ˜ã‚­ã‚µã‚´ãƒŠãƒ«ãƒ»ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**

**å®Ÿç”¨çš„ãªå‘¼ã³æ–¹ï¼š**

> **ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ™ãƒ¼ã‚¹ãƒ»ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**ï¼ˆProvider-based Architectureï¼‰

---

ã“ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€ç‰¹ã« AI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚„ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¨ãƒ³ã‚¸ãƒ³ã§æœ‰åŠ¹ã§ã™ã€‚LangChain ã‚„ LlamaIndex ãªã©ã‚‚ä¼¼ãŸã‚ˆã†ãªæ§‹é€ ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ï¼ˆæŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ + å®Ÿè£…ã®åˆ‡ã‚Šæ›¿ãˆï¼‰ã€‚

```plaintext
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Application Core               â”‚
â”‚  (Workflows, Nodes, Business Logic) â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Ports (Interfaces)       â”‚     â”‚
â”‚  â”‚  - LLMProvider           â”‚â”€â”€â”€â”€â”€â”¼â”€â†’ Adapters (å®Ÿè£…)
â”‚  â”‚  - RAGProvider           â”‚     â”‚   - GeminiProvider
â”‚  â”‚  - DocumentProvider      â”‚     â”‚   - OpenAIProvider
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   - MockProvider
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
class ChatWorkflow:
    def __init__(self, llm_provider: LLMProvider):  # â† æ³¨å…¥
        self.provider = llm_provider
```

```python
# å®Ÿè¡Œæ™‚ã«ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’åˆ‡ã‚Šæ›¿ãˆ
workflow_with_gemini = ChatWorkflow(llm_provider=GeminiProvider())
workflow_with_openai = ChatWorkflow(llm_provider=OpenAIProvider())
workflow_with_mock = ChatWorkflow(llm_provider=MockProvider())
```

```python
class ProviderFactory:
    _llm_providers = {
        "gemini": GeminiProvider,
        "openai": OpenAIProvider,
        "mock": MockProvider,
    }

    @classmethod
    def register_llm_provider(cls, name: str, provider_class):
        cls._llm_providers[name] = provider_class  # â† å‹•çš„ç™»éŒ²
```

```python
provider = ProviderFactory.create_llm_provider("gemini", config={...})
```

```plaintext
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Layer (FastAPI)                â”‚  â† ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å±¤
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Workflows (Atomic/Composite/Orch)  â”‚  â† ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¤
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Nodes (å‡¦ç†å˜ä½)                    â”‚  â† ãƒ‰ãƒ¡ã‚¤ãƒ³å±¤
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Providers (æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹)    â”‚  â† ã‚¤ãƒ³ãƒ•ãƒ©æŠ½è±¡å±¤
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Concrete Providers (å®Ÿè£…)          â”‚  â† ã‚¤ãƒ³ãƒ•ãƒ©å®Ÿè£…å±¤
â”‚  - GeminiProvider                   â”‚
â”‚  - OpenAIProvider                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```plaintext
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Presentation Layer                      â”‚
â”‚                    (FastAPI Endpoints)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Application Layer                        â”‚
â”‚              (Workflows: Atomic, Composite)                 â”‚
â”‚                                                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚    â”‚ChatWorkflow â”‚      â”‚PPTSummary   â”‚                   â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Domain Layer                           â”‚
â”‚                   (Nodes: Processing Units)                 â”‚
â”‚                                                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚    â”‚  LLMNode    â”‚      â”‚  RAGNode    â”‚                   â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Infrastructure Abstraction Layer               â”‚
â”‚                   (Ports: Interfaces)                       â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  LLMProvider     â”‚  â”‚  RAGProvider     â”‚               â”‚
â”‚  â”‚  (Interface)     â”‚  â”‚  (Interface)     â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Infrastructure Implementation Layer            â”‚
â”‚                   (Adapters: Implementations)               â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚Gemini        â”‚  â”‚OpenAI        â”‚  â”‚Mock          â”‚    â”‚
â”‚  â”‚Provider      â”‚  â”‚Provider      â”‚  â”‚Provider      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚        Factory + Registry (Plugin System)        â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
