# ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æ”¹å–„å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ

**å®Ÿæ–½æ—¥**: 2025å¹´11æœˆ22æ—¥  
**ç›®çš„**: æ§‹é€ åŒ–ã•ã‚ŒãŸä¾‹å¤–ã‚¯ãƒ©ã‚¹ã®å®šç¾©ã¨ã€ã‚ˆã‚Šå…·ä½“çš„ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æä¾›  
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: âœ… å®Œäº†

---

## ğŸ“‹ å®Ÿæ–½å†…å®¹

### 1. ã‚«ã‚¹ã‚¿ãƒ ä¾‹å¤–ã‚¯ãƒ©ã‚¹ã®å®šç¾©ï¼ˆå®Œäº†ï¼‰

**æ‹¡å¼µã—ãŸãƒ•ã‚¡ã‚¤ãƒ«**: `src/core/exceptions.py`

#### åŸºåº•ä¾‹å¤–ã‚¯ãƒ©ã‚¹ã®æ”¹å–„

```python
class LangGraphBaseException(Exception):
    """åŸºåº•ä¾‹å¤–ã‚¯ãƒ©ã‚¹
    
    å…¨ã¦ã®ã‚«ã‚¹ã‚¿ãƒ ä¾‹å¤–ã®åŸºåº•ã‚¯ãƒ©ã‚¹ã€‚
    è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿æŒã§ãã¾ã™ã€‚
    
    Attributes:
        message: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        details: ã‚¨ãƒ©ãƒ¼ã®è©³ç´°æƒ…å ±ï¼ˆè¾æ›¸ï¼‰
        original_error: å…ƒã®ä¾‹å¤–ï¼ˆã‚ã‚Œã°ï¼‰
    """
    
    def __init__(
        self,
        message: str,
        details: Optional[Dict[str, Any]] = None,
        original_error: Optional[Exception] = None
    ):
        self.message = message
        self.details = details or {}
        self.original_error = original_error
        super().__init__(self.format_message())
    
    def format_message(self) -> str:
        """ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        msg = self.message
        if self.details:
            detail_str = ", ".join(f"{k}={v}" for k, v in self.details.items())
            msg = f"{msg} ({detail_str})"
        if self.original_error:
            msg = f"{msg} [åŸå› : {type(self.original_error).__name__}: {str(self.original_error)}]"
        return msg
    
    def to_dict(self) -> Dict[str, Any]:
        """ä¾‹å¤–æƒ…å ±ã‚’è¾æ›¸å½¢å¼ã§è¿”ã™"""
        return {
            "error_type": self.__class__.__name__,
            "message": self.message,
            "details": self.details,
            "original_error": str(self.original_error) if self.original_error else None
        }
```

**ãƒ¡ãƒªãƒƒãƒˆ**:
- âœ… æ§‹é€ åŒ–ã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼æƒ…å ±
- âœ… å…ƒã®ä¾‹å¤–ã‚’ä¿æŒï¼ˆãƒ‡ãƒãƒƒã‚°å®¹æ˜“ï¼‰
- âœ… JSONåŒ–å¯èƒ½ï¼ˆAPIå¿œç­”ã«é©ã—ã¦ã„ã‚‹ï¼‰

#### å®šç¾©ã—ãŸä¾‹å¤–ã‚¯ãƒ©ã‚¹éšå±¤

```
LangGraphBaseException (åŸºåº•)
â”‚
â”œâ”€â”€ ProviderError
â”‚   â”œâ”€â”€ LLMProviderError
â”‚   â”‚   â”œâ”€â”€ LLMGenerationError
â”‚   â”‚   â”œâ”€â”€ LLMJSONParseError
â”‚   â”‚   â”œâ”€â”€ LLMRateLimitError
â”‚   â”‚   â””â”€â”€ LLMAuthenticationError
â”‚   â””â”€â”€ RAGProviderError
â”‚
â”œâ”€â”€ NodeError
â”‚   â”œâ”€â”€ NodeExecutionError
â”‚   â”œâ”€â”€ NodeInputValidationError
â”‚   â””â”€â”€ NodeOutputValidationError
â”‚
â”œâ”€â”€ WorkflowError
â”‚   â”œâ”€â”€ WorkflowExecutionError
â”‚   â””â”€â”€ WorkflowBuildError
â”‚
â”œâ”€â”€ InfrastructureError
â”‚   â”œâ”€â”€ VectorStoreError
â”‚   â”‚   â”œâ”€â”€ VectorStoreConnectionError
â”‚   â”‚   â””â”€â”€ VectorStoreQueryError
â”‚   â”œâ”€â”€ EmbeddingError
â”‚   â”‚   â””â”€â”€ EmbeddingDimensionError
â”‚   â””â”€â”€ SearchError
â”‚
â”œâ”€â”€ MCPError
â”‚   â”œâ”€â”€ MCPConnectionError
â”‚   â”œâ”€â”€ MCPToolError
â”‚   â””â”€â”€ MCPAuthenticationError
â”‚
â”œâ”€â”€ ConfigurationError
â”‚   â”œâ”€â”€ MissingConfigError
â”‚   â””â”€â”€ InvalidConfigError
â”‚
â””â”€â”€ FactoryError
    â”œâ”€â”€ UnknownProviderError
    â””â”€â”€ ProviderRegistrationError
```

**åˆè¨ˆ**: 30+ ã®å…·ä½“çš„ãªä¾‹å¤–ã‚¯ãƒ©ã‚¹

---

### 2. Provider ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ”¹å–„ï¼ˆå®Œäº†ï¼‰

#### GeminiProvider ã®ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

**ä¿®æ­£ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«**: `src/providers/llm/gemini.py`

```python
async def generate(self, prompt: str, **kwargs) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
    
    Raises:
        LLMAuthenticationError: APIèªè¨¼ã«å¤±æ•—ã—ãŸå ´åˆ
        LLMRateLimitError: ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ãŸå ´åˆ
        LLMGenerationError: ãã®ä»–ã®ç”Ÿæˆã‚¨ãƒ©ãƒ¼
    """
    try:
        # ... ç”Ÿæˆå‡¦ç† ...
        
        if not response.text:
            raise LLMGenerationError(
                "Empty response from Gemini API",
                details={
                    "model": self.model,
                    "prompt_length": len(prompt),
                    "temperature": temperature
                }
            )
        
        return response.text.strip()
        
    except ValueError as e:
        error_msg = str(e).lower()
        if "api" in error_msg and ("key" in error_msg or "auth" in error_msg):
            raise LLMAuthenticationError(
                "Gemini API authentication failed",
                details={"model": self.model},
                original_error=e
            )
        raise LLMGenerationError(...)
    
    except Exception as e:
        error_msg = str(e).lower()
        if "quota" in error_msg or "rate" in error_msg:
            raise LLMRateLimitError(
                "Gemini API rate limit exceeded",
                details={"model": self.model},
                original_error=e
            )
        raise LLMGenerationError(...)
```

**æ”¹å–„ç‚¹**:
- âœ… ã‚¨ãƒ©ãƒ¼ã®ç¨®é¡ã‚’è‡ªå‹•åˆ¤åˆ¥ï¼ˆèªè¨¼ã€ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãªã©ï¼‰
- âœ… è©³ç´°ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’æä¾›
- âœ… å…ƒã®ä¾‹å¤–ã‚’ä¿æŒ

#### ProviderFactory ã®ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

**ä¿®æ­£ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«**: `src/core/factory.py`

```python
@classmethod
def create_llm_provider(cls, provider_type: str = "gemini", **kwargs) -> LLMProvider:
    """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç”Ÿæˆ
    
    Raises:
        UnknownProviderError: æœªçŸ¥ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚¿ã‚¤ãƒ—ã®å ´åˆ
        MissingConfigError: å¿…é ˆè¨­å®šãŒæ¬ è½ã—ã¦ã„ã‚‹å ´åˆ
        ProviderRegistrationError: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ç”Ÿæˆã«å¤±æ•—ã—ãŸå ´åˆ
    """
    if provider_type not in cls._llm_providers:
        raise UnknownProviderError(
            f"Unknown LLM provider type: {provider_type}",
            details={
                "provider_type": provider_type,
                "available_providers": list(cls._llm_providers.keys())
            }
        )
    
    # ... è¨­å®šãƒã‚§ãƒƒã‚¯ ...
    
    if not settings.gemini_api_key:
        raise MissingConfigError(
            "GEMINI_API_KEY is not configured",
            details={
                "provider_type": provider_type,
                "missing_config": "GEMINI_API_KEY",
                "hint": "Set GEMINI_API_KEY in .env file"
            }
        )
```

**æ”¹å–„ç‚¹**:
- âœ… åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ãƒªã‚¹ãƒˆè¡¨ç¤º
- âœ… è¨­å®šãƒŸã‚¹ã‚’æ˜ç¢ºã«æŒ‡æ‘˜
- âœ… ä¿®æ­£æ–¹æ³•ã®ãƒ’ãƒ³ãƒˆã‚’æä¾›

---

### 3. Node ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ”¹å–„ï¼ˆå®Œäº†ï¼‰

#### LLMNode ã®ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

**ä¿®æ­£ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«**: `src/nodes/primitives/llm/gemini/node.py`

```python
async def execute(self, state: NodeState) -> NodeState:
    """LLMç”Ÿæˆã‚’å®Ÿè¡Œ
    
    Raises:
        NodeInputValidationError: å…¥åŠ›ãŒä¸æ­£ãªå ´åˆ
        NodeExecutionError: ãƒãƒ¼ãƒ‰ã®å®Ÿè¡Œã«å¤±æ•—ã—ãŸå ´åˆ
    """
    try:
        prompt = state.messages[-1] if state.messages else state.data.get("prompt")
        
        if not prompt or not isinstance(prompt, str):
            raise NodeInputValidationError(
                "Invalid prompt: must be a non-empty string",
                details={
                    "node": self.name,
                    "prompt_type": type(prompt).__name__,
                    "messages_count": len(state.messages)
                }
            )
        
        # ... LLMå‘¼ã³å‡ºã— ...
        
    except NodeInputValidationError:
        raise
    except LLMProviderError as e:
        # Providerå±¤ã‹ã‚‰ã®ä¾‹å¤–ã‚’é©åˆ‡ã«å¤‰æ›
        raise NodeExecutionError(
            f"LLM provider error in node {self.name}",
            details={
                "node": self.name,
                "provider": type(self.provider).__name__,
                "error_details": e.details if hasattr(e, 'details') else {}
            },
            original_error=e
        )
```

**æ”¹å–„ç‚¹**:
- âœ… å…¥åŠ›æ¤œè¨¼ã‚’æ˜ç¤ºçš„ã«å®Ÿæ–½
- âœ… Providerå±¤ã®ä¾‹å¤–ã‚’é©åˆ‡ã«å¤‰æ›
- âœ… ãƒãƒ¼ãƒ‰å›ºæœ‰ã®æƒ…å ±ã‚’è¿½åŠ 

#### TodoAdvisorNode ã®ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

**ä¿®æ­£ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«**: `src/nodes/composites/todo/advisor/node.py`

```python
async def execute(self, input_data: Dict[str, Any]) -> NodeResult:
    """Generate advice for a TODO item
    
    Raises:
        NodeInputValidationError: å…¥åŠ›ãŒä¸æ­£ãªå ´åˆ
        NodeExecutionError: å®Ÿè¡Œã«å¤±æ•—ã—ãŸå ´åˆ
    """
    try:
        todo = input_data.get("todo", {})
        
        # å…¥åŠ›æ¤œè¨¼
        if not todo:
            raise NodeInputValidationError(
                "TODO item is required",
                details={
                    "node": self.name,
                    "index": index,
                    "total": total
                }
            )
        
        if not isinstance(todo, dict):
            raise NodeInputValidationError(
                "TODO item must be a dictionary",
                details={
                    "node": self.name,
                    "todo_type": type(todo).__name__
                }
            )
        
        # ... ã‚¢ãƒ‰ãƒã‚¤ã‚¹ç”Ÿæˆ ...
```

**æ”¹å–„ç‚¹**:
- âœ… ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯å›ºæœ‰ã®æ¤œè¨¼
- âœ… å‹ãƒã‚§ãƒƒã‚¯ã®æ˜ç¤ºåŒ–
- âœ… è©³ç´°ãªã‚¨ãƒ©ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

---

### 4. Workflow ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ”¹å–„ï¼ˆå®Œäº†ï¼‰

#### ChatWorkflow ã®ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

**ä¿®æ­£ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«**: `src/workflows/atomic/chat.py`

```python
async def run(self, input_data: ChatInput) -> ChatOutput:
    """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ
    
    Raises:
        WorkflowExecutionError: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å®Ÿè¡Œã«å¤±æ•—ã—ãŸå ´åˆ
    """
    try:
        # å…¥åŠ›æ¤œè¨¼
        if not input_data.message or not input_data.message.strip():
            raise WorkflowExecutionError(
                "Message cannot be empty",
                details={
                    "workflow": "ChatWorkflow",
                    "input_type": type(input_data).__name__
                }
            )
        
        # ... ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ ...
        
        # çµæœæ¤œè¨¼
        if "error" in result_state.data:
            raise WorkflowExecutionError(
                "Node execution failed in chat workflow",
                details={
                    "workflow": "ChatWorkflow",
                    "error": result_state.data["error"],
                    "error_node": result_state.metadata.get("error_node", "unknown")
                }
            )
        
    except WorkflowExecutionError:
        raise
    except NodeExecutionError as e:
        # Nodeå±¤ã®ä¾‹å¤–ã‚’é©åˆ‡ã«å¤‰æ›
        raise WorkflowExecutionError(
            "Node execution failed in chat workflow",
            details={
                "workflow": "ChatWorkflow",
                "message_length": len(input_data.message),
                "error_details": e.details if hasattr(e, 'details') else {}
            },
            original_error=e
        )
```

**æ”¹å–„ç‚¹**:
- âœ… ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å›ºæœ‰ã®æ¤œè¨¼
- âœ… Nodeå±¤ã®ä¾‹å¤–ã‚’é©åˆ‡ã«å¤‰æ›
- âœ… ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å…¨ä½“ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æä¾›

---

## ğŸ“Š æ”¹å–„å‰å¾Œã®æ¯”è¼ƒ

### Beforeï¼ˆæ”¹å–„å‰ï¼‰

```python
# âŒ ä¸€èˆ¬çš„ãª Exception catch
try:
    response = model.generate(prompt)
except Exception as e:
    logger.error(f"Error: {e}")
    return {"error": str(e)}
```

**å•é¡Œç‚¹**:
- âŒ ã‚¨ãƒ©ãƒ¼ã®ç¨®é¡ãŒä¸æ˜
- âŒ ãƒ‡ãƒãƒƒã‚°æƒ…å ±ãŒä¸è¶³
- âŒ é©åˆ‡ãªå‡¦ç†ãŒã§ããªã„
- âŒ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æœ‰ç”¨ãªæƒ…å ±ã‚’æä¾›ã§ããªã„

### Afterï¼ˆæ”¹å–„å¾Œï¼‰

```python
# âœ… å…·ä½“çš„ãªä¾‹å¤–ã‚’ä½¿ç”¨
try:
    response = await provider.generate(prompt)
except LLMAuthenticationError as e:
    # èªè¨¼ã‚¨ãƒ©ãƒ¼: è¨­å®šã‚’ç¢ºèªã—ã¦ã‚‚ã‚‰ã†
    raise NodeExecutionError(
        "LLM authentication failed",
        details={
            "node": self.name,
            "hint": "Check GEMINI_API_KEY in .env",
            "error_details": e.details
        },
        original_error=e
    )
except LLMRateLimitError as e:
    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™: ãƒªãƒˆãƒ©ã‚¤ã‚’ææ¡ˆ
    raise NodeExecutionError(
        "LLM rate limit exceeded",
        details={
            "node": self.name,
            "hint": "Wait and retry later",
            "error_details": e.details
        },
        original_error=e
    )
except LLMGenerationError as e:
    # ç”Ÿæˆã‚¨ãƒ©ãƒ¼: è©³ç´°æƒ…å ±ã‚’æä¾›
    raise NodeExecutionError(
        "LLM generation failed",
        details={
            "node": self.name,
            "prompt_length": len(prompt),
            "error_details": e.details
        },
        original_error=e
    )
```

**ãƒ¡ãƒªãƒƒãƒˆ**:
- âœ… ã‚¨ãƒ©ãƒ¼ã®ç¨®é¡ãŒæ˜ç¢º
- âœ… è©³ç´°ãªè¨ºæ–­æƒ…å ±
- âœ… ã‚¨ãƒ©ãƒ¼ã”ã¨ã«é©åˆ‡ãªå‡¦ç†ãŒå¯èƒ½
- âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æœ‰ç”¨ãªãƒ’ãƒ³ãƒˆã‚’æä¾›

---

## ğŸ§ª ãƒ†ã‚¹ãƒˆçµæœ

### 1. Import ãƒ†ã‚¹ãƒˆ âœ…

```
âœ… exceptions: OK
âœ… GeminiProvider: OK
âœ… ProviderFactory: OK
âœ… LLMNode: OK
âœ… ChatWorkflow: OK
```

### 2. ä¾‹å¤–éšå±¤ãƒ†ã‚¹ãƒˆ âœ…

```
âœ… Exception hierarchy: Correct
âœ… Exception details: OK
âœ… Exception to_dict: OK
```

### 3. Providerä¾‹å¤–ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ âœ…

```
âœ… UnknownProviderError: Correct
âœ… Error details included: OK
âœ… Available providers listed: OK
```

### 4. Linter âœ…

```
âœ… No linter errors found
```

---

## ğŸ’¡ ä½¿ç”¨ä¾‹

### ä¾‹1: å…·ä½“çš„ãªã‚¨ãƒ©ãƒ¼å‡¦ç†

```python
try:
    provider = ProviderFactory.create_llm_provider("openai")
except UnknownProviderError as e:
    print(f"Error: {e.message}")
    print(f"Available: {e.details['available_providers']}")
    # Output:
    # Error: Unknown LLM provider type: openai
    # Available: ['gemini', 'mock']
```

### ä¾‹2: ã‚¨ãƒ©ãƒ¼æƒ…å ±ã®å–å¾—

```python
try:
    result = await provider.generate(prompt)
except LLMGenerationError as e:
    error_info = e.to_dict()
    # {
    #     "error_type": "LLMGenerationError",
    #     "message": "Failed to generate text",
    #     "details": {
    #         "model": "gemini-2.0-flash-exp",
    #         "temperature": 0.7,
    #         "prompt_length": 150
    #     },
    #     "original_error": "ValueError: API key invalid"
    # }
```

### ä¾‹3: ãƒ¬ã‚¤ãƒ¤ãƒ¼é–“ã®ä¾‹å¤–å¤‰æ›

```python
# Providerå±¤
try:
    response = await genai.generate(prompt)
except ValueError as e:
    if "auth" in str(e).lower():
        raise LLMAuthenticationError(
            "API authentication failed",
            details={"model": self.model},
            original_error=e
        )

# Nodeå±¤
try:
    response = await self.provider.generate(prompt)
except LLMProviderError as e:
    raise NodeExecutionError(
        f"Provider error in {self.name}",
        details={"node": self.name, "error_details": e.details},
        original_error=e
    )

# Workflowå±¤
try:
    result = await node.execute(state)
except NodeExecutionError as e:
    raise WorkflowExecutionError(
        f"Node failed in {self.workflow_name}",
        details={"workflow": self.workflow_name, "error_details": e.details},
        original_error=e
    )
```

---

## ğŸ“ˆ æ”¹å–„åŠ¹æœ

### ã‚³ãƒ¼ãƒ‰å“è³ªã®å‘ä¸Š

| é …ç›® | æ”¹å–„å‰ | æ”¹å–„å¾Œ |
|------|--------|--------|
| **ä¾‹å¤–ã‚¯ãƒ©ã‚¹æ•°** | 5å€‹ | 30+å€‹ |
| **ä¾‹å¤–ã®å…·ä½“æ€§** | ä½ã„ | é«˜ã„ |
| **ã‚¨ãƒ©ãƒ¼æƒ…å ±** | æœ€å°é™ | è©³ç´° |
| **ãƒ‡ãƒãƒƒã‚°å®¹æ˜“æ€§** | å›°é›£ | å®¹æ˜“ |
| **ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“** | ä¸æ˜ç­ | æ˜ç¢º |

### ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ”¹å–„

**Before**:
```
Error: Unknown provider type: openai
```

**After**:
```
Unknown LLM provider type: openai (provider_type=openai, available_providers=['gemini', 'mock'])
```

### ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®å……å®Ÿ

**Before**:
```
Error in LLM node: Failed to generate
```

**After**:
```
LLM provider error in node llm_node (node=llm_node, provider=GeminiProvider, error_details={'model': 'gemini-2.0-flash-exp', 'temperature': 0.7}) [åŸå› : LLMGenerationError: Empty response from Gemini API]
```

---

## ğŸ¯ é”æˆã—ãŸç›®æ¨™

### 1. âœ… ã‚«ã‚¹ã‚¿ãƒ ä¾‹å¤–ã‚¯ãƒ©ã‚¹ã®å®šç¾©

- 30+ ã®å…·ä½“çš„ãªä¾‹å¤–ã‚¯ãƒ©ã‚¹
- ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«é©åˆ‡ãªéšå±¤
- æ§‹é€ åŒ–ã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼æƒ…å ±

### 2. âœ… ã‚ˆã‚Šå…·ä½“çš„ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

- è©³ç´°ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
- å…ƒã®ä¾‹å¤–ã®ä¿æŒ
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®ãƒ’ãƒ³ãƒˆæä¾›

### 3. âœ… ã‚¨ãƒ©ãƒ¼å‡¦ç†ã®ä¸€è²«æ€§

- å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§çµ±ä¸€ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³
- ä¾‹å¤–ã®é©åˆ‡ãªå¤‰æ›
- ãƒ­ã‚°ã¨ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®åˆ†é›¢

### 4. âœ… ãƒ‡ãƒãƒƒã‚°ã®å®¹æ˜“æ€§

- è©³ç´°ãªã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹
- ã‚¨ãƒ©ãƒ¼æƒ…å ±ã®JSONåŒ–
- ãƒ¬ã‚¤ãƒ¤ãƒ¼é–“ã®ã‚¨ãƒ©ãƒ¼ä¼æ’­ã®æ˜ç¢ºåŒ–

---

## ğŸ”® ä»Šå¾Œã®æ‹¡å¼µ

### 1. ã‚¨ãƒ©ãƒ¼ãƒªã‚«ãƒãƒªãƒ¼æ©Ÿèƒ½

```python
class RetryableError(LangGraphBaseException):
    """ãƒªãƒˆãƒ©ã‚¤å¯èƒ½ãªã‚¨ãƒ©ãƒ¼"""
    
    def __init__(self, message: str, retry_after: int = 5, **kwargs):
        super().__init__(message, **kwargs)
        self.retry_after = retry_after
```

### 2. ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®æ§‹é€ åŒ–

```python
# æ§‹é€ åŒ–ãƒ­ã‚°å‡ºåŠ›
error_log = {
    "timestamp": datetime.now().isoformat(),
    "error": exc.to_dict(),
    "context": {
        "user_id": user_id,
        "request_id": request_id
    }
}
logger.error(json.dumps(error_log))
```

### 3. ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åé›†

```python
# ã‚¨ãƒ©ãƒ¼çµ±è¨ˆ
error_metrics = {
    "LLMRateLimitError": 15,
    "LLMAuthenticationError": 2,
    "NodeExecutionError": 5
}
```

---

## ğŸ“š ä¿®æ­£çµ±è¨ˆ

### ä¿®æ­£ãƒ•ã‚¡ã‚¤ãƒ«æ•°

- `src/core/exceptions.py`: ä¾‹å¤–ã‚¯ãƒ©ã‚¹å®šç¾©æ‹¡å¼µï¼ˆç´„200è¡Œè¿½åŠ ï¼‰
- `src/providers/llm/gemini.py`: ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æ”¹å–„
- `src/core/factory.py`: ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æ”¹å–„
- `src/nodes/primitives/llm/gemini/node.py`: ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æ”¹å–„
- `src/nodes/composites/todo/advisor/node.py`: ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æ”¹å–„
- `src/workflows/atomic/chat.py`: ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æ”¹å–„

**åˆè¨ˆ**: 6ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£ã€ç´„300è¡Œã®ã‚³ãƒ¼ãƒ‰è¿½åŠ 

### ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸

- âœ… Import ãƒ†ã‚¹ãƒˆ: å…¨ã¦æˆåŠŸ
- âœ… ä¾‹å¤–éšå±¤ãƒ†ã‚¹ãƒˆ: å…¨ã¦æˆåŠŸ
- âœ… ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ: å…¨ã¦æˆåŠŸ
- âœ… Linter: ã‚¨ãƒ©ãƒ¼0ä»¶

---

## âœ… çµè«–

ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®æ”¹å–„ã«ã‚ˆã‚Šã€ä»¥ä¸‹ã‚’é”æˆã—ã¾ã—ãŸï¼š

1. **æ§‹é€ åŒ–ã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼æƒ…å ±** - è¨ºæ–­ã«å¿…è¦ãªæƒ…å ±ã‚’å…¨ã¦ä¿æŒ
2. **ãƒ¬ã‚¤ãƒ¤ãƒ¼é–“ã®ä¸€è²«æ€§** - å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§çµ±ä¸€ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³
3. **ãƒ‡ãƒãƒƒã‚°ã®å®¹æ˜“æ€§** - ã‚¨ãƒ©ãƒ¼ã®åŸå› ã‚’ç´ æ—©ãç‰¹å®šå¯èƒ½
4. **ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã®å‘ä¸Š** - æ˜ç¢ºã§å®Ÿç”¨çš„ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
5. **ä¿å®ˆæ€§ã®å‘ä¸Š** - ã‚¨ãƒ©ãƒ¼å‡¦ç†ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’ç¢ºç«‹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¯ã€**ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚°ãƒ¬ãƒ¼ãƒ‰ã®å“è³ª**ã«é”ã—ã¾ã—ãŸã€‚

---

*å®Œäº†æ—¥: 2025å¹´11æœˆ22æ—¥*  
*ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: âœ… å…¨æ”¹å–„å®Œäº†*

