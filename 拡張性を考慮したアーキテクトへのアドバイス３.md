**User**

ã§ã¯ã€ã“ã¡ã‚‰ã®æ–¹å‘æ€§ã§ã€æ®µéšçš„ã«ä¿®æ­£ã‚’ã—ã¦ãã ã•ã„ã€‚
ã¾ãšã¯ã€è¨ˆç”»ã‚’ç«‹ã¦ã¦

---

**Cursor**

å®Œç’§ã§ã™ï¼æ®µéšçš„ãªä¿®æ­£è¨ˆç”»ã‚’ç«‹ã¦ã¾ã—ã‚‡ã†ã€‚ã¾ãšç¾åœ¨ã®ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®çŠ¶æ³ã‚’è©³ã—ãç¢ºèªã—ã¾ã™ã€‚

å®Œç’§ã§ã™ï¼ç¾çŠ¶ã‚’æŠŠæ¡ã—ã¾ã—ãŸã€‚ã§ã¯ã€**æ®µéšçš„ãªå®Ÿè£…è¨ˆç”»**ã‚’ç«‹ã¦ã¾ã™ã€‚

---

# ğŸ¯ ãƒ—ãƒ©ã‚¬ãƒ–ãƒ«ãƒ»ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¸ã®æ®µéšçš„ç§»è¡Œè¨ˆç”»

## ğŸ“‹ ç¾çŠ¶åˆ†æ

### âœ… ã™ã§ã«å­˜åœ¨ã™ã‚‹ã‚‚ã®

- `services/llm/base.py` - `BaseLLMService`æŠ½è±¡ã‚¯ãƒ©ã‚¹ï¼ˆè‰¯ã„åŸºç›¤ï¼ï¼‰
- `infrastructure/` - embeddings, vector_storesï¼ˆã™ã§ã«è‰¯ã„æ§‹é€ ï¼‰
- Workflow å±¤ã® 3 å±¤æ§‹é€ ï¼ˆatomic/composite/orchestrationsï¼‰

### âŒ å•é¡Œç‚¹

1. `GeminiService`ãŒã‚¯ãƒ©ã‚¹ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆDI ä¸å¯ï¼‰
2. `RAGService`ãŒå†…éƒ¨ã§`GeminiService`ã‚’ç›´æ¥å‘¼ã³å‡ºã—ï¼ˆãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
3. `DocumentService`ãŒé™çš„ãƒ¡ã‚½ãƒƒãƒ‰
4. ãƒãƒ¼ãƒ‰ãŒã‚µãƒ¼ãƒ“ã‚¹ã‚’ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
5. ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒãƒãƒ¼ãƒ‰ã‚’ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°

---

## ğŸ“… æ®µéšçš„å®Ÿè£…è¨ˆç”»ï¼ˆ4 ãƒ•ã‚§ãƒ¼ã‚ºï¼‰

### **Phase 1: Provider å±¤ã®åŸºç›¤æ§‹ç¯‰** ğŸŸ¢

**ç›®æ¨™**: ã‚µãƒ¼ãƒ“ã‚¹ã‚’ Provider ã«å¤‰æ›ã—ã€DI å¯èƒ½ã«ã™ã‚‹  
**æœŸé–“**: 2-3 æ—¥  
**å½±éŸ¿ç¯„å›²**: æœ€å°ï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¯å‹•ä½œã—ç¶šã‘ã‚‹ï¼‰

#### ã‚¿ã‚¹ã‚¯ä¸€è¦§

##### 1.1 ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®ä½œæˆ

```bash
src/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ providers/              # æ–°è¦ä½œæˆ
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ llm.py              # LLMProvideræŠ½è±¡ã‚¯ãƒ©ã‚¹
â”‚       â”œâ”€â”€ rag.py              # RAGProvideræŠ½è±¡ã‚¯ãƒ©ã‚¹
â”‚       â””â”€â”€ document.py         # DocumentProvideræŠ½è±¡ã‚¯ãƒ©ã‚¹
â”‚
â””â”€â”€ providers/                  # æ–°è¦ä½œæˆ
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ llm/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ gemini.py          # GeminiProviderå®Ÿè£…
    â”‚   â””â”€â”€ mock.py            # MockLLMProviderï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
    â”œâ”€â”€ rag/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ simple.py          # SimpleRAGProvider
    â”‚   â””â”€â”€ mock.py            # MockRAGProvider
    â””â”€â”€ document/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ ppt.py             # PPTProvider
        â””â”€â”€ mock.py            # MockDocumentProvider
```

##### 1.2 æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®å®šç¾©

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/core/providers/llm.py`

```python
from abc import ABC, abstractmethod
from typing import Optional, Type, Dict, Any
from pydantic import BaseModel

class LLMProvider(ABC):
    """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""

    @abstractmethod
    async def generate(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        pass

    @abstractmethod
    async def generate_json(
        self,
        prompt: str,
        schema: Type[BaseModel],
        temperature: float = 0.7,
        **kwargs
    ) -> BaseModel:
        """æ§‹é€ åŒ–JSONç”Ÿæˆ"""
        pass

    @abstractmethod
    async def generate_with_context(
        self,
        user_query: str,
        context: str,
        system_instruction: Optional[str] = None,
        temperature: float = 0.7,
        **kwargs
    ) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ãç”Ÿæˆï¼ˆRAGç”¨ï¼‰"""
        pass
```

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/core/providers/rag.py`

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from pydantic import BaseModel

class RAGResult(BaseModel):
    """RAGå®Ÿè¡Œçµæœ"""
    answer: str
    retrieved_documents: List[Dict[str, Any]]
    query_embedding: Optional[List[float]] = None
    context_used: str

class RAGProvider(ABC):
    """RAGãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""

    @abstractmethod
    async def query(
        self,
        query: str,
        collection_name: str = "default_collection",
        top_k: int = 5,
        include_embedding: bool = False,
        temperature: float = 0.7
    ) -> RAGResult:
        """RAGã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ"""
        pass

    @abstractmethod
    async def ingest_documents(
        self,
        documents: List[Dict[str, Any]],
        collection_name: str = "default_collection"
    ) -> Dict[str, Any]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç™»éŒ²"""
        pass
```

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/core/providers/document.py`

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any
from pydantic import BaseModel

class SlideContent(BaseModel):
    """ã‚¹ãƒ©ã‚¤ãƒ‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„"""
    slide_number: int
    title: str
    content: List[str]
    notes: str

class DocumentProvider(ABC):
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""

    @abstractmethod
    async def extract_from_ppt(self, file_path: str) -> List[SlideContent]:
        """PowerPointã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        pass

    @abstractmethod
    def format_slides_as_text(self, slides: List[SlideContent]) -> str:
        """ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        pass
```

##### 1.3 å…·è±¡å®Ÿè£…ã®ä½œæˆ

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/providers/llm/gemini.py`

````python
from typing import Optional, Type, Dict, Any
from pydantic import BaseModel
import google.generativeai as genai
import logging

from src.core.providers.llm import LLMProvider

logger = logging.getLogger(__name__)

class GeminiProvider(LLMProvider):
    """Gemini LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å®Ÿè£…"""

    def __init__(
        self,
        api_key: str,
        model: str = "gemini-2.0-flash-exp"
    ):
        self.api_key = api_key
        self.model = model
        genai.configure(api_key=api_key)
        logger.info(f"GeminiProvider initialized with model: {model}")

    async def generate(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        generation_config = {
            "temperature": temperature,
            **kwargs
        }
        if max_tokens:
            generation_config["max_output_tokens"] = max_tokens

        model_instance = genai.GenerativeModel(
            model_name=self.model,
            generation_config=generation_config
        )

        logger.info(f"Generating text with Gemini (temp: {temperature})")
        response = model_instance.generate_content(prompt)
        return response.text.strip()

    async def generate_json(
        self,
        prompt: str,
        schema: Type[BaseModel],
        temperature: float = 0.7,
        **kwargs
    ) -> BaseModel:
        # JSONã‚¹ã‚­ãƒ¼ãƒã‚’è¿½åŠ 
        schema_json = schema.schema_json(indent=2)
        json_prompt = f"""{prompt}

ä»¥ä¸‹ã®JSONå½¢å¼ã§ã®ã¿è¿”ç­”ã—ã¦ãã ã•ã„ã€‚ä»–ã®èª¬æ˜ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ï¼š
{schema_json}
"""

        text_response = await self.generate(
            prompt=json_prompt,
            temperature=temperature,
            **kwargs
        )

        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰Šé™¤
        cleaned_response = self._clean_json_response(text_response)

        try:
            return schema.parse_raw(cleaned_response)
        except Exception as e:
            logger.error(f"Failed to parse JSON response: {e}")
            raise ValueError(f"Failed to parse LLM response as JSON: {str(e)}")

    async def generate_with_context(
        self,
        user_query: str,
        context: str,
        system_instruction: Optional[str] = None,
        temperature: float = 0.7,
        **kwargs
    ) -> str:
        default_instruction = "ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’å‚è€ƒã«ã—ã¦ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚"
        instruction = system_instruction or default_instruction

        prompt = f"""{instruction}

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±:
{context}

è³ªå•: {user_query}

å›ç­”:"""

        return await self.generate(
            prompt=prompt,
            temperature=temperature,
            **kwargs
        )

    @staticmethod
    def _clean_json_response(response: str) -> str:
        """JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’å‰Šé™¤"""
        cleaned = response.strip()
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        elif cleaned.startswith("```"):
            cleaned = cleaned[3:]
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
        return cleaned.strip()
````

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/providers/llm/mock.py`ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰

```python
from typing import Optional, Type, Dict, Any, List
from pydantic import BaseModel
import logging

from src.core.providers.llm import LLMProvider

logger = logging.getLogger(__name__)

class MockLLMProvider(LLMProvider):
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ¢ãƒƒã‚¯LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""

    def __init__(self, responses: Optional[Dict[str, str]] = None):
        self.responses = responses or {}
        self.call_history: List[Dict[str, Any]] = []
        logger.info("MockLLMProvider initialized")

    async def generate(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        self.call_history.append({
            "method": "generate",
            "prompt": prompt,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "kwargs": kwargs
        })

        # å®Œå…¨ä¸€è‡´ã¾ãŸã¯éƒ¨åˆ†ä¸€è‡´ã§å¿œç­”ã‚’è¿”ã™
        if prompt in self.responses:
            return self.responses[prompt]

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¢ãƒƒã‚¯å¿œç­”
        return f"Mock response for: {prompt[:50]}..."

    async def generate_json(
        self,
        prompt: str,
        schema: Type[BaseModel],
        temperature: float = 0.7,
        **kwargs
    ) -> BaseModel:
        self.call_history.append({
            "method": "generate_json",
            "prompt": prompt,
            "schema": schema.__name__,
            "temperature": temperature,
            "kwargs": kwargs
        })

        # ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        return schema.parse_obj({"mock": "data"})

    async def generate_with_context(
        self,
        user_query: str,
        context: str,
        system_instruction: Optional[str] = None,
        temperature: float = 0.7,
        **kwargs
    ) -> str:
        self.call_history.append({
            "method": "generate_with_context",
            "user_query": user_query,
            "context": context[:100],
            "system_instruction": system_instruction,
            "temperature": temperature,
            "kwargs": kwargs
        })

        if user_query in self.responses:
            return self.responses[user_query]

        return f"Mock answer for: {user_query}"
```

##### 1.4 æ—¢å­˜ã‚µãƒ¼ãƒ“ã‚¹ã¨ã®äº’æ›æ€§ãƒ¬ã‚¤ãƒ¤ãƒ¼

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/services/llm/gemini_service.py`ï¼ˆæ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ï¼‰

```python
"""Gemini LLM Service - å¾Œæ–¹äº’æ›æ€§ãƒ¬ã‚¤ãƒ¤ãƒ¼"""

from typing import Optional, Type, Dict, Any
from pydantic import BaseModel
import logging

from src.core.config import settings
from src.providers.llm.gemini import GeminiProvider
from .base import BaseLLMService

logger = logging.getLogger(__name__)

# âœ… ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆå¾Œæ–¹äº’æ›æ€§ç”¨ï¼‰
_default_provider: Optional[GeminiProvider] = None

def _get_default_provider() -> GeminiProvider:
    """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å–å¾—"""
    global _default_provider
    if _default_provider is None:
        if not settings.gemini_api_key:
            raise ValueError("GEMINI_API_KEY is not configured")
        _default_provider = GeminiProvider(
            api_key=settings.gemini_api_key,
            model="gemini-2.0-flash-exp"
        )
    return _default_provider

class GeminiService(BaseLLMService):
    """Gemini API ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰

    âš ï¸ éæ¨å¥¨: æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ã§ã¯ GeminiProvider ã‚’ç›´æ¥ä½¿ç”¨ã—ã¦ãã ã•ã„
    """

    @classmethod
    async def generate(cls, prompt: str, **kwargs) -> str:
        """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼"""
        provider = _get_default_provider()
        return await provider.generate(prompt, **kwargs)

    @classmethod
    async def generate_json(cls, prompt: str, schema: Type[BaseModel], **kwargs) -> BaseModel:
        """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼"""
        provider = _get_default_provider()
        return await provider.generate_json(prompt, schema, **kwargs)

    @classmethod
    async def generate_with_context(cls, user_query: str, context: str, **kwargs) -> str:
        """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼"""
        provider = _get_default_provider()
        return await provider.generate_with_context(user_query, context, **kwargs)

# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹é–¢æ•°
async def generate_text(prompt: str, **kwargs) -> str:
    return await GeminiService.generate(prompt, **kwargs)

async def generate_json_response(prompt: str, schema: Type[BaseModel], **kwargs) -> BaseModel:
    return await GeminiService.generate_json(prompt, schema, **kwargs)

async def generate_rag_response(user_query: str, context: str, **kwargs) -> str:
    return await GeminiService.generate_with_context(user_query, context, **kwargs)
```

---

### **Phase 2: Node å±¤ã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°** ğŸŸ¡

**ç›®æ¨™**: ãƒãƒ¼ãƒ‰ã« DI ã‚’å°å…¥  
**æœŸé–“**: 2-3 æ—¥  
**å½±éŸ¿ç¯„å›²**: ä¸­ï¼ˆæ—¢å­˜ãƒãƒ¼ãƒ‰ã¯ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã§å‹•ä½œã—ç¶šã‘ã‚‹ï¼‰

#### ã‚¿ã‚¹ã‚¯ä¸€è¦§

##### 2.1 LLMNode ã®æ”¹å–„

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/nodes/primitives/llm/gemini/node.py`ï¼ˆæ›´æ–°ï¼‰

```python
from typing import Optional
import logging

from src.nodes.base import BaseNode, NodeState
from src.core.providers.llm import LLMProvider
from src.providers.llm.gemini import GeminiProvider
from src.core.config import settings

logger = logging.getLogger(__name__)

class LLMNode(BaseNode):
    """LLMãƒãƒ¼ãƒ‰ï¼ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ³¨å…¥å¯èƒ½ï¼‰"""

    def __init__(
        self,
        provider: LLMProvider,
        name: str = "llm_node",
        description: str = "Generate responses using LLM"
    ):
        super().__init__(name=name, description=description)
        self.provider = provider

    async def execute(self, state: NodeState) -> NodeState:
        """LLMç”Ÿæˆã‚’å®Ÿè¡Œ"""
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
            if state.messages:
                prompt = state.messages[-1]
            else:
                prompt = state.data.get("prompt", "")

            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
            temperature = state.data.get("temperature", 0.7)
            max_tokens = state.data.get("max_tokens")

            # âœ… ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’é€šã˜ã¦ç”Ÿæˆ
            logger.info(f"Generating with {self.provider.__class__.__name__}")
            response_text = await self.provider.generate(
                prompt=prompt,
                temperature=temperature,
                max_tokens=max_tokens
            )

            # çŠ¶æ…‹ã‚’æ›´æ–°
            state.messages.append(response_text)
            state.data["llm_response"] = response_text
            state.metadata["node"] = self.name
            state.metadata["provider"] = self.provider.__class__.__name__

            return state

        except Exception as e:
            logger.error(f"Error in LLM node: {e}")
            state.data["error"] = str(e)
            state.metadata["error_node"] = self.name
            return state


# âœ… å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
class GeminiNode(LLMNode):
    """Geminiãƒãƒ¼ãƒ‰ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰

    âš ï¸ éæ¨å¥¨: æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ã§ã¯ LLMNode(provider=GeminiProvider(...)) ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "gemini-2.0-flash-exp"
    ):
        provider = GeminiProvider(
            api_key=api_key or settings.gemini_api_key,
            model=model
        )
        super().__init__(
            provider=provider,
            name="gemini_node",
            description="Generate responses using Google Gemini LLM"
        )
```

##### 2.2 RAGNode ã®æ”¹å–„

åŒæ§˜ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ `RAGNode` ã‚‚æ›´æ–°ã—ã¾ã™ã€‚

---

### **Phase 3: Workflow å±¤ã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°** ğŸŸ¡

**ç›®æ¨™**: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã« DI ã‚’å°å…¥  
**æœŸé–“**: 2-3 æ—¥  
**å½±éŸ¿ç¯„å›²**: ä¸­ï¼ˆAPI ã¯äº’æ›æ€§ç¶­æŒï¼‰

#### ã‚¿ã‚¹ã‚¯ä¸€è¦§

##### 3.1 Atomic ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®æ”¹å–„

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/workflows/atomic/chat.py`ï¼ˆæ›´æ–°ï¼‰

```python
from typing import Optional
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, START, END
import logging

from src.nodes.base import NodeState
from src.nodes.primitives.llm.gemini.node import LLMNode
from src.core.providers.llm import LLMProvider
from src.providers.llm.gemini import GeminiProvider
from src.core.config import settings

logger = logging.getLogger(__name__)

class ChatInput(BaseModel):
    message: str = Field(..., description="User message")
    temperature: float = Field(default=0.7, description="LLM temperature")
    max_tokens: int = Field(default=1000, description="Maximum tokens")

class ChatOutput(BaseModel):
    response: str = Field(..., description="LLM response")
    success: bool = Field(default=True)
    error_message: str = Field(default="")

class ChatWorkflow:
    """ãƒãƒ£ãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ³¨å…¥å¯èƒ½ï¼‰"""

    def __init__(self, llm_provider: Optional[LLMProvider] = None):
        # âœ… ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒæŒ‡å®šã•ã‚Œãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨
        if llm_provider is None:
            llm_provider = GeminiProvider(api_key=settings.gemini_api_key)

        self.llm_node = LLMNode(provider=llm_provider)
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        workflow = StateGraph(NodeState)
        workflow.add_node("gemini", self.llm_node.execute)
        workflow.add_edge(START, "gemini")
        workflow.add_edge("gemini", END)
        return workflow.compile()

    async def run(self, input_data: ChatInput) -> ChatOutput:
        try:
            state = NodeState()
            state.messages = [input_data.message]
            state.data["temperature"] = input_data.temperature
            state.data["max_tokens"] = input_data.max_tokens

            logger.info(f"Executing chat workflow")
            result_state = await self.graph.ainvoke(state)

            if "error" in result_state.data:
                return ChatOutput(
                    response="",
                    success=False,
                    error_message=result_state.data["error"]
                )

            return ChatOutput(
                response=result_state.data.get("llm_response", ""),
                success=True
            )

        except Exception as e:
            logger.error(f"Error in chat workflow: {e}")
            return ChatOutput(
                response="",
                success=False,
                error_message=str(e)
            )

    def get_mermaid_diagram(self) -> str:
        """LangGraphã®å¯è¦–åŒ–"""
        return self.graph.get_graph().draw_mermaid()
```

---

### **Phase 4: Factory & ãƒ†ã‚¹ãƒˆæ•´å‚™** ğŸŸ¢

**ç›®æ¨™**: ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ãƒ†ã‚¹ãƒˆã®æ•´å‚™  
**æœŸé–“**: 2-3 æ—¥  
**å½±éŸ¿ç¯„å›²**: æœ€å°ï¼ˆæ–°æ©Ÿèƒ½ã®è¿½åŠ ï¼‰

#### ã‚¿ã‚¹ã‚¯ä¸€è¦§

##### 4.1 è»½é‡ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ã®ä½œæˆ

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/core/factory.py`ï¼ˆæ–°è¦ä½œæˆï¼‰

```python
from typing import Dict, Any, Optional, Type
from src.core.providers.llm import LLMProvider
from src.providers.llm.gemini import GeminiProvider
from src.providers.llm.mock import MockLLMProvider
from src.core.config import settings

class ProviderFactory:
    """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ï¼ˆè»½é‡ç‰ˆï¼‰"""

    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ¬ã‚¸ã‚¹ãƒˆãƒª
    _llm_providers: Dict[str, Type[LLMProvider]] = {
        "gemini": GeminiProvider,
        "mock": MockLLMProvider,
    }

    @classmethod
    def create_llm_provider(
        cls,
        provider_type: str = "gemini",
        config: Optional[Dict[str, Any]] = None
    ) -> LLMProvider:
        """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç”Ÿæˆ"""
        if provider_type not in cls._llm_providers:
            available = ", ".join(cls._llm_providers.keys())
            raise ValueError(
                f"Unknown provider type: {provider_type}. "
                f"Available: {available}"
            )

        provider_class = cls._llm_providers[provider_type]
        config = config or {}

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’é©ç”¨
        if provider_type == "gemini" and "api_key" not in config:
            config["api_key"] = settings.gemini_api_key

        return provider_class(**config)

    @classmethod
    def register_llm_provider(
        cls,
        name: str,
        provider_class: Type[LLMProvider]
    ):
        """æ–°ã—ã„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç™»éŒ²ï¼ˆæ‹¡å¼µç”¨ï¼‰"""
        cls._llm_providers[name] = provider_class

    @classmethod
    def list_llm_providers(cls) -> list[str]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ãƒªã‚¹ãƒˆ"""
        return list(cls._llm_providers.keys())
```

##### 4.2 ãƒ†ã‚¹ãƒˆã®ä½œæˆ

**ãƒ•ã‚¡ã‚¤ãƒ«**: `tests/test_chat_workflow.py`ï¼ˆæ–°è¦ä½œæˆï¼‰

```python
import pytest
from src.workflows.atomic.chat import ChatWorkflow, ChatInput
from src.providers.llm.mock import MockLLMProvider

@pytest.mark.asyncio
async def test_chat_workflow_basic():
    """åŸºæœ¬çš„ãªãƒãƒ£ãƒƒãƒˆãƒ•ãƒ­ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
    # âœ… ãƒ¢ãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’æ³¨å…¥
    mock_provider = MockLLMProvider(
        responses={
            "Hello": "Hi there! How can I help you today?"
        }
    )

    workflow = ChatWorkflow(llm_provider=mock_provider)
    result = await workflow.run(ChatInput(message="Hello"))

    # æ¤œè¨¼
    assert result.success is True
    assert "Hi there" in result.response
    assert len(mock_provider.call_history) == 1
    assert mock_provider.call_history[0]["prompt"] == "Hello"

@pytest.mark.asyncio
async def test_chat_workflow_with_parameters():
    """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»˜ããƒ†ã‚¹ãƒˆ"""
    mock_provider = MockLLMProvider()
    workflow = ChatWorkflow(llm_provider=mock_provider)

    await workflow.run(
        ChatInput(message="Test", temperature=0.9, max_tokens=500)
    )

    # ãƒ¢ãƒƒã‚¯ã®å‘¼ã³å‡ºã—å±¥æ­´ã‚’ç¢ºèª
    call = mock_provider.call_history[0]
    assert call["temperature"] == 0.9
    assert call["max_tokens"] == 500
```

---

## ğŸ“Š å®Ÿè£…ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

| ãƒ•ã‚§ãƒ¼ã‚º    | æœŸé–“        | æˆæœç‰©                           | ãƒ–ãƒ­ãƒƒã‚«ãƒ¼   |
| ----------- | ----------- | -------------------------------- | ------------ |
| **Phase 1** | 2-3 æ—¥      | Provider å±¤ã®åŸºç›¤                | ãªã—ï¼ˆç‹¬ç«‹ï¼‰ |
| **Phase 2** | 2-3 æ—¥      | Node å±¤ã® DI åŒ–                  | Phase 1 å®Œäº† |
| **Phase 3** | 2-3 æ—¥      | Workflow å±¤ã® DI åŒ–              | Phase 2 å®Œäº† |
| **Phase 4** | 2-3 æ—¥      | Factory & ãƒ†ã‚¹ãƒˆ                 | Phase 3 å®Œäº† |
| **åˆè¨ˆ**    | **8-12 æ—¥** | å®Œå…¨ãªãƒ—ãƒ©ã‚¬ãƒ–ãƒ«ãƒ»ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ | -            |

---

## âœ… å„ãƒ•ã‚§ãƒ¼ã‚ºã®å®Œäº†åŸºæº–

### Phase 1 å®Œäº†

- [ ] æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ 3 ã¤ä½œæˆï¼ˆLLM, RAG, Documentï¼‰
- [ ] GeminiProvider å®Ÿè£…
- [ ] MockProvider å®Ÿè£…ï¼ˆå„ç¨®ï¼‰
- [ ] æ—¢å­˜ GeminiService ãŒå‹•ä½œã—ç¶šã‘ã‚‹
- [ ] æ—¢å­˜ã®ãƒ†ã‚¹ãƒˆãŒå…¨ã¦ãƒ‘ã‚¹

### Phase 2 å®Œäº†

- [ ] LLMNode, RAGNode, DocumentNode ãŒ DI å¯¾å¿œ
- [ ] æ—¢å­˜ã®ãƒãƒ¼ãƒ‰ãŒã‚¨ã‚¤ãƒªã‚¢ã‚¹ã§å‹•ä½œ
- [ ] ãƒãƒ¼ãƒ‰ã®å˜ä½“ãƒ†ã‚¹ãƒˆä½œæˆ

### Phase 3 å®Œäº†

- [ ] å…¨ Atomic ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ³¨å…¥å¯¾å¿œ
- [ ] å…¨ Composite ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒ DI å¯¾å¿œ
- [ ] æ—¢å­˜ API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒå‹•ä½œ
- [ ] ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ãƒ†ã‚¹ãƒˆä½œæˆ

### Phase 4 å®Œäº†

- [ ] ProviderFactory å®Ÿè£…
- [ ] å…¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ 80%ä»¥ä¸Š
- [ ] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°
- [ ] å¯è¦–åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆæ›´æ–°

---

## ğŸš¨ ãƒªã‚¹ã‚¯ç®¡ç†

### ãƒªã‚¹ã‚¯ 1: æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã®ç ´å£Š

**å¯¾ç­–**:

- å¾Œæ–¹äº’æ›æ€§ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å¿…ãšä½œæˆ
- æ—¢å­˜ãƒ†ã‚¹ãƒˆã‚’å…¨ã¦å®Ÿè¡Œã—ã¦ã‹ã‚‰ãƒãƒ¼ã‚¸
- æ®µéšçš„ã«ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

### ãƒªã‚¹ã‚¯ 2: ãƒ†ã‚¹ãƒˆã®ä¸è¶³

**å¯¾ç­–**:

- Phase 1 ã‹ã‚‰ãƒ†ã‚¹ãƒˆã‚’æ›¸ã
- MockProvider ã‚’æ—©æœŸã«ä½œæˆ
- ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’æ¸¬å®š

### ãƒªã‚¹ã‚¯ 3: è¤‡é›‘æ€§ã®å¢—åŠ 

**å¯¾ç­–**:

- ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ã‹ã‚‰å§‹ã‚ã‚‹
- å¿…è¦æœ€å°é™ã®æŠ½è±¡åŒ–
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å……å®Ÿ

---

## ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ç¾åœ¨**Ask Mode**ãªã®ã§ã€å®Ÿè£…ã«ã¯ Agent Mode ã«åˆ‡ã‚Šæ›¿ãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

**æ¨å¥¨æ‰‹é †**:

1. **Agent Mode ã«åˆ‡ã‚Šæ›¿ãˆ**
2. **Phase 1 ã‹ã‚‰é–‹å§‹**: `core/providers/` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
3. **æ®µéšçš„ã« PR**: å„ãƒ•ã‚§ãƒ¼ã‚ºã”ã¨ã«ã‚³ãƒŸãƒƒãƒˆ
4. **ãƒ¬ãƒ“ãƒ¥ãƒ¼**: å„ãƒ•ã‚§ãƒ¼ã‚ºå®Œäº†å¾Œã«å¯è¦–åŒ–ã—ã¦ç¢ºèª

ã“ã®è¨ˆç”»ã§é€²ã‚ã¾ã™ã‹ï¼Ÿãã‚Œã¨ã‚‚èª¿æ•´ãŒå¿…è¦ã§ã™ã‹ï¼Ÿ

```shellscript
src/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ providers/              # æ–°è¦ä½œæˆ
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ llm.py              # LLMProvideræŠ½è±¡ã‚¯ãƒ©ã‚¹
â”‚       â”œâ”€â”€ rag.py              # RAGProvideræŠ½è±¡ã‚¯ãƒ©ã‚¹
â”‚       â””â”€â”€ document.py         # DocumentProvideræŠ½è±¡ã‚¯ãƒ©ã‚¹
â”‚
â””â”€â”€ providers/                  # æ–°è¦ä½œæˆ
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ llm/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ gemini.py          # GeminiProviderå®Ÿè£…
    â”‚   â””â”€â”€ mock.py            # MockLLMProviderï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
    â”œâ”€â”€ rag/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ simple.py          # SimpleRAGProvider
    â”‚   â””â”€â”€ mock.py            # MockRAGProvider
    â””â”€â”€ document/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ ppt.py             # PPTProvider
        â””â”€â”€ mock.py            # MockDocumentProvider
```

```python
from abc import ABC, abstractmethod
from typing import Optional, Type, Dict, Any
from pydantic import BaseModel

class LLMProvider(ABC):
    """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""

    @abstractmethod
    async def generate(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        pass

    @abstractmethod
    async def generate_json(
        self,
        prompt: str,
        schema: Type[BaseModel],
        temperature: float = 0.7,
        **kwargs
    ) -> BaseModel:
        """æ§‹é€ åŒ–JSONç”Ÿæˆ"""
        pass

    @abstractmethod
    async def generate_with_context(
        self,
        user_query: str,
        context: str,
        system_instruction: Optional[str] = None,
        temperature: float = 0.7,
        **kwargs
    ) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ãç”Ÿæˆï¼ˆRAGç”¨ï¼‰"""
        pass
```

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from pydantic import BaseModel

class RAGResult(BaseModel):
    """RAGå®Ÿè¡Œçµæœ"""
    answer: str
    retrieved_documents: List[Dict[str, Any]]
    query_embedding: Optional[List[float]] = None
    context_used: str

class RAGProvider(ABC):
    """RAGãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""

    @abstractmethod
    async def query(
        self,
        query: str,
        collection_name: str = "default_collection",
        top_k: int = 5,
        include_embedding: bool = False,
        temperature: float = 0.7
    ) -> RAGResult:
        """RAGã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ"""
        pass

    @abstractmethod
    async def ingest_documents(
        self,
        documents: List[Dict[str, Any]],
        collection_name: str = "default_collection"
    ) -> Dict[str, Any]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç™»éŒ²"""
        pass
```

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any
from pydantic import BaseModel

class SlideContent(BaseModel):
    """ã‚¹ãƒ©ã‚¤ãƒ‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„"""
    slide_number: int
    title: str
    content: List[str]
    notes: str

class DocumentProvider(ABC):
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""

    @abstractmethod
    async def extract_from_ppt(self, file_path: str) -> List[SlideContent]:
        """PowerPointã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        pass

    @abstractmethod
    def format_slides_as_text(self, slides: List[SlideContent]) -> str:
        """ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        pass
```

````python
from typing import Optional, Type, Dict, Any
from pydantic import BaseModel
import google.generativeai as genai
import logging

from src.core.providers.llm import LLMProvider

logger = logging.getLogger(__name__)

class GeminiProvider(LLMProvider):
    """Gemini LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å®Ÿè£…"""

    def __init__(
        self,
        api_key: str,
        model: str = "gemini-2.0-flash-exp"
    ):
        self.api_key = api_key
        self.model = model
        genai.configure(api_key=api_key)
        logger.info(f"GeminiProvider initialized with model: {model}")

    async def generate(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        generation_config = {
            "temperature": temperature,
            **kwargs
        }
        if max_tokens:
            generation_config["max_output_tokens"] = max_tokens

        model_instance = genai.GenerativeModel(
            model_name=self.model,
            generation_config=generation_config
        )

        logger.info(f"Generating text with Gemini (temp: {temperature})")
        response = model_instance.generate_content(prompt)
        return response.text.strip()

    async def generate_json(
        self,
        prompt: str,
        schema: Type[BaseModel],
        temperature: float = 0.7,
        **kwargs
    ) -> BaseModel:
        # JSONã‚¹ã‚­ãƒ¼ãƒã‚’è¿½åŠ 
        schema_json = schema.schema_json(indent=2)
        json_prompt = f"""{prompt}

ä»¥ä¸‹ã®JSONå½¢å¼ã§ã®ã¿è¿”ç­”ã—ã¦ãã ã•ã„ã€‚ä»–ã®èª¬æ˜ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ï¼š
{schema_json}
"""

        text_response = await self.generate(
            prompt=json_prompt,
            temperature=temperature,
            **kwargs
        )

        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰Šé™¤
        cleaned_response = self._clean_json_response(text_response)

        try:
            return schema.parse_raw(cleaned_response)
        except Exception as e:
            logger.error(f"Failed to parse JSON response: {e}")
            raise ValueError(f"Failed to parse LLM response as JSON: {str(e)}")

    async def generate_with_context(
        self,
        user_query: str,
        context: str,
        system_instruction: Optional[str] = None,
        temperature: float = 0.7,
        **kwargs
    ) -> str:
        default_instruction = "ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’å‚è€ƒã«ã—ã¦ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚"
        instruction = system_instruction or default_instruction

        prompt = f"""{instruction}

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±:
{context}

è³ªå•: {user_query}

å›ç­”:"""

        return await self.generate(
            prompt=prompt,
            temperature=temperature,
            **kwargs
        )

    @staticmethod
    def _clean_json_response(response: str) -> str:
        """JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’å‰Šé™¤"""
        cleaned = response.strip()
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        elif cleaned.startswith("```"):
            cleaned = cleaned[3:]
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
        return cleaned.strip()
````

```python
from typing import Optional, Type, Dict, Any, List
from pydantic import BaseModel
import logging

from src.core.providers.llm import LLMProvider

logger = logging.getLogger(__name__)

class MockLLMProvider(LLMProvider):
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ¢ãƒƒã‚¯LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""

    def __init__(self, responses: Optional[Dict[str, str]] = None):
        self.responses = responses or {}
        self.call_history: List[Dict[str, Any]] = []
        logger.info("MockLLMProvider initialized")

    async def generate(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        self.call_history.append({
            "method": "generate",
            "prompt": prompt,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "kwargs": kwargs
        })

        # å®Œå…¨ä¸€è‡´ã¾ãŸã¯éƒ¨åˆ†ä¸€è‡´ã§å¿œç­”ã‚’è¿”ã™
        if prompt in self.responses:
            return self.responses[prompt]

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¢ãƒƒã‚¯å¿œç­”
        return f"Mock response for: {prompt[:50]}..."

    async def generate_json(
        self,
        prompt: str,
        schema: Type[BaseModel],
        temperature: float = 0.7,
        **kwargs
    ) -> BaseModel:
        self.call_history.append({
            "method": "generate_json",
            "prompt": prompt,
            "schema": schema.__name__,
            "temperature": temperature,
            "kwargs": kwargs
        })

        # ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        return schema.parse_obj({"mock": "data"})

    async def generate_with_context(
        self,
        user_query: str,
        context: str,
        system_instruction: Optional[str] = None,
        temperature: float = 0.7,
        **kwargs
    ) -> str:
        self.call_history.append({
            "method": "generate_with_context",
            "user_query": user_query,
            "context": context[:100],
            "system_instruction": system_instruction,
            "temperature": temperature,
            "kwargs": kwargs
        })

        if user_query in self.responses:
            return self.responses[user_query]

        return f"Mock answer for: {user_query}"
```

```python
"""Gemini LLM Service - å¾Œæ–¹äº’æ›æ€§ãƒ¬ã‚¤ãƒ¤ãƒ¼"""

from typing import Optional, Type, Dict, Any
from pydantic import BaseModel
import logging

from src.core.config import settings
from src.providers.llm.gemini import GeminiProvider
from .base import BaseLLMService

logger = logging.getLogger(__name__)

# âœ… ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆå¾Œæ–¹äº’æ›æ€§ç”¨ï¼‰
_default_provider: Optional[GeminiProvider] = None

def _get_default_provider() -> GeminiProvider:
    """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å–å¾—"""
    global _default_provider
    if _default_provider is None:
        if not settings.gemini_api_key:
            raise ValueError("GEMINI_API_KEY is not configured")
        _default_provider = GeminiProvider(
            api_key=settings.gemini_api_key,
            model="gemini-2.0-flash-exp"
        )
    return _default_provider

class GeminiService(BaseLLMService):
    """Gemini API ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰

    âš ï¸ éæ¨å¥¨: æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ã§ã¯ GeminiProvider ã‚’ç›´æ¥ä½¿ç”¨ã—ã¦ãã ã•ã„
    """

    @classmethod
    async def generate(cls, prompt: str, **kwargs) -> str:
        """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼"""
        provider = _get_default_provider()
        return await provider.generate(prompt, **kwargs)

    @classmethod
    async def generate_json(cls, prompt: str, schema: Type[BaseModel], **kwargs) -> BaseModel:
        """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼"""
        provider = _get_default_provider()
        return await provider.generate_json(prompt, schema, **kwargs)

    @classmethod
    async def generate_with_context(cls, user_query: str, context: str, **kwargs) -> str:
        """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼"""
        provider = _get_default_provider()
        return await provider.generate_with_context(user_query, context, **kwargs)

# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹é–¢æ•°
async def generate_text(prompt: str, **kwargs) -> str:
    return await GeminiService.generate(prompt, **kwargs)

async def generate_json_response(prompt: str, schema: Type[BaseModel], **kwargs) -> BaseModel:
    return await GeminiService.generate_json(prompt, schema, **kwargs)

async def generate_rag_response(user_query: str, context: str, **kwargs) -> str:
    return await GeminiService.generate_with_context(user_query, context, **kwargs)
```

```python
from typing import Optional
import logging

from src.nodes.base import BaseNode, NodeState
from src.core.providers.llm import LLMProvider
from src.providers.llm.gemini import GeminiProvider
from src.core.config import settings

logger = logging.getLogger(__name__)

class LLMNode(BaseNode):
    """LLMãƒãƒ¼ãƒ‰ï¼ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ³¨å…¥å¯èƒ½ï¼‰"""

    def __init__(
        self,
        provider: LLMProvider,
        name: str = "llm_node",
        description: str = "Generate responses using LLM"
    ):
        super().__init__(name=name, description=description)
        self.provider = provider

    async def execute(self, state: NodeState) -> NodeState:
        """LLMç”Ÿæˆã‚’å®Ÿè¡Œ"""
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
            if state.messages:
                prompt = state.messages[-1]
            else:
                prompt = state.data.get("prompt", "")

            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
            temperature = state.data.get("temperature", 0.7)
            max_tokens = state.data.get("max_tokens")

            # âœ… ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’é€šã˜ã¦ç”Ÿæˆ
            logger.info(f"Generating with {self.provider.__class__.__name__}")
            response_text = await self.provider.generate(
                prompt=prompt,
                temperature=temperature,
                max_tokens=max_tokens
            )

            # çŠ¶æ…‹ã‚’æ›´æ–°
            state.messages.append(response_text)
            state.data["llm_response"] = response_text
            state.metadata["node"] = self.name
            state.metadata["provider"] = self.provider.__class__.__name__

            return state

        except Exception as e:
            logger.error(f"Error in LLM node: {e}")
            state.data["error"] = str(e)
            state.metadata["error_node"] = self.name
            return state


# âœ… å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
class GeminiNode(LLMNode):
    """Geminiãƒãƒ¼ãƒ‰ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰

    âš ï¸ éæ¨å¥¨: æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ã§ã¯ LLMNode(provider=GeminiProvider(...)) ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "gemini-2.0-flash-exp"
    ):
        provider = GeminiProvider(
            api_key=api_key or settings.gemini_api_key,
            model=model
        )
        super().__init__(
            provider=provider,
            name="gemini_node",
            description="Generate responses using Google Gemini LLM"
        )
```

```python
from typing import Optional
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, START, END
import logging

from src.nodes.base import NodeState
from src.nodes.primitives.llm.gemini.node import LLMNode
from src.core.providers.llm import LLMProvider
from src.providers.llm.gemini import GeminiProvider
from src.core.config import settings

logger = logging.getLogger(__name__)

class ChatInput(BaseModel):
    message: str = Field(..., description="User message")
    temperature: float = Field(default=0.7, description="LLM temperature")
    max_tokens: int = Field(default=1000, description="Maximum tokens")

class ChatOutput(BaseModel):
    response: str = Field(..., description="LLM response")
    success: bool = Field(default=True)
    error_message: str = Field(default="")

class ChatWorkflow:
    """ãƒãƒ£ãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ³¨å…¥å¯èƒ½ï¼‰"""

    def __init__(self, llm_provider: Optional[LLMProvider] = None):
        # âœ… ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒæŒ‡å®šã•ã‚Œãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨
        if llm_provider is None:
            llm_provider = GeminiProvider(api_key=settings.gemini_api_key)

        self.llm_node = LLMNode(provider=llm_provider)
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        workflow = StateGraph(NodeState)
        workflow.add_node("gemini", self.llm_node.execute)
        workflow.add_edge(START, "gemini")
        workflow.add_edge("gemini", END)
        return workflow.compile()

    async def run(self, input_data: ChatInput) -> ChatOutput:
        try:
            state = NodeState()
            state.messages = [input_data.message]
            state.data["temperature"] = input_data.temperature
            state.data["max_tokens"] = input_data.max_tokens

            logger.info(f"Executing chat workflow")
            result_state = await self.graph.ainvoke(state)

            if "error" in result_state.data:
                return ChatOutput(
                    response="",
                    success=False,
                    error_message=result_state.data["error"]
                )

            return ChatOutput(
                response=result_state.data.get("llm_response", ""),
                success=True
            )

        except Exception as e:
            logger.error(f"Error in chat workflow: {e}")
            return ChatOutput(
                response="",
                success=False,
                error_message=str(e)
            )

    def get_mermaid_diagram(self) -> str:
        """LangGraphã®å¯è¦–åŒ–"""
        return self.graph.get_graph().draw_mermaid()
```

```python
from typing import Dict, Any, Optional, Type
from src.core.providers.llm import LLMProvider
from src.providers.llm.gemini import GeminiProvider
from src.providers.llm.mock import MockLLMProvider
from src.core.config import settings

class ProviderFactory:
    """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ï¼ˆè»½é‡ç‰ˆï¼‰"""

    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ¬ã‚¸ã‚¹ãƒˆãƒª
    _llm_providers: Dict[str, Type[LLMProvider]] = {
        "gemini": GeminiProvider,
        "mock": MockLLMProvider,
    }

    @classmethod
    def create_llm_provider(
        cls,
        provider_type: str = "gemini",
        config: Optional[Dict[str, Any]] = None
    ) -> LLMProvider:
        """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç”Ÿæˆ"""
        if provider_type not in cls._llm_providers:
            available = ", ".join(cls._llm_providers.keys())
            raise ValueError(
                f"Unknown provider type: {provider_type}. "
                f"Available: {available}"
            )

        provider_class = cls._llm_providers[provider_type]
        config = config or {}

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’é©ç”¨
        if provider_type == "gemini" and "api_key" not in config:
            config["api_key"] = settings.gemini_api_key

        return provider_class(**config)

    @classmethod
    def register_llm_provider(
        cls,
        name: str,
        provider_class: Type[LLMProvider]
    ):
        """æ–°ã—ã„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç™»éŒ²ï¼ˆæ‹¡å¼µç”¨ï¼‰"""
        cls._llm_providers[name] = provider_class

    @classmethod
    def list_llm_providers(cls) -> list[str]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ãƒªã‚¹ãƒˆ"""
        return list(cls._llm_providers.keys())
```

```python
import pytest
from src.workflows.atomic.chat import ChatWorkflow, ChatInput
from src.providers.llm.mock import MockLLMProvider

@pytest.mark.asyncio
async def test_chat_workflow_basic():
    """åŸºæœ¬çš„ãªãƒãƒ£ãƒƒãƒˆãƒ•ãƒ­ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
    # âœ… ãƒ¢ãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’æ³¨å…¥
    mock_provider = MockLLMProvider(
        responses={
            "Hello": "Hi there! How can I help you today?"
        }
    )

    workflow = ChatWorkflow(llm_provider=mock_provider)
    result = await workflow.run(ChatInput(message="Hello"))

    # æ¤œè¨¼
    assert result.success is True
    assert "Hi there" in result.response
    assert len(mock_provider.call_history) == 1
    assert mock_provider.call_history[0]["prompt"] == "Hello"

@pytest.mark.asyncio
async def test_chat_workflow_with_parameters():
    """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»˜ããƒ†ã‚¹ãƒˆ"""
    mock_provider = MockLLMProvider()
    workflow = ChatWorkflow(llm_provider=mock_provider)

    await workflow.run(
        ChatInput(message="Test", temperature=0.9, max_tokens=500)
    )

    # ãƒ¢ãƒƒã‚¯ã®å‘¼ã³å‡ºã—å±¥æ­´ã‚’ç¢ºèª
    call = mock_provider.call_history[0]
    assert call["temperature"] == 0.9
    assert call["max_tokens"] == 500
```
