# æ‹¡å¼µæ€§ã‚’è€ƒæ…®ã—ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ”¹å–„
_Exported on 2025/11/22 at 20:18:24 JST from Cursor (1.6.45)_

---

**User**

ä»Šå¾Œã®æ‹¡å¼µæ€§ã‚’è¦‹æ®ãˆã¦ã€åŽ³ã—ããƒ€ãƒ¡å‡ºã—ã‚’ã—ã¦ãã ã•ã„ã€‚
Difyã®ã‚ˆã†ã«ã€æ—¢å­˜ã®éƒ¨å“ã‚’çµ„ã¿åˆã‚ã›ã¦ã€è‰²ã€…ãªè¤‡é›‘ã§ä¾¡å€¤ã‚ã‚‹ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’çµ„ã¿ç«‹ã¦ã‚„ã™ã„ã‚‚ã®ã«ã—ãŸã„ã€‚
ç‰¹ã«ã€ä¾å­˜æ€§ã®æ³¨å…¥ã¿ãŸã„ã®ãŒè¶³ã‚Šãªã„ã®ã¨ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ•´å‚™ã—ã¦æ‹¡å¼µã—ã‚„ã™ã„ã‚‚ã®ã«ã—ãŸã„

---

**Cursor**

ç¾åœ¨ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è©³ã—ãåˆ†æžã—ã¦ã€åŽ³ã—ãæ‰¹è©•ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚ã¾ãšã€é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã•ã›ã¦ãã ã•ã„ã€‚

ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã‚’åˆ†æžã—ã¾ã—ãŸã€‚ã§ã¯ã€Difyã®ã‚ˆã†ãªæ‹¡å¼µæ€§ã®é«˜ã„ã‚·ã‚¹ãƒ†ãƒ ã‚’ç›®æŒ‡ã™ã«ã‚ãŸã‚Šã€**åŽ³ã—ããƒ€ãƒ¡å‡ºã—**ã‚’ã—ã¾ã™ã€‚

## ðŸš¨ é‡å¤§ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å•é¡Œç‚¹

### 1. **ä¾å­˜æ€§æ³¨å…¥ãŒçš†ç„¡ - æœ€å¤§ã®å•é¡Œ**

ç¾åœ¨ã®ã‚³ãƒ¼ãƒ‰ã¯**ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚ŒãŸä¾å­˜é–¢ä¿‚**ã ã‚‰ã‘ã§ã™ï¼š

```42:42:src/nodes/llm/gemini.py
        self.rag_service = RAGService()
```

```42:43:src/workflows/atomic/chat.py
        self.llm_node = GeminiNode()
        self.graph = self._build_graph()
```

**å•é¡Œç‚¹ï¼š**
- ãƒ†ã‚¹ãƒˆãŒå›°é›£ï¼ˆãƒ¢ãƒƒã‚¯ã‚’å·®ã—è¾¼ã‚ãªã„ï¼‰
- å®Ÿè£…ã®åˆ‡ã‚Šæ›¿ãˆãŒä¸å¯èƒ½ï¼ˆGemini â†’ OpenAI ã®å¤‰æ›´ãŒå¤§è¦æ¨¡ä¿®æ­£ã«ãªã‚‹ï¼‰
- ãƒŽãƒ¼ãƒ‰ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã®è¨­å®šãŒå¤–éƒ¨ã‹ã‚‰åˆ¶å¾¡ã§ããªã„
- ãƒžãƒ«ãƒãƒ†ãƒŠãƒ³ãƒˆå¯¾å¿œãŒä¸å¯èƒ½ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã«ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ãˆãªã„ï¼‰

**Difyã®ã‚ˆã†ãªæ‹¡å¼µæ€§ã®ãŸã‚ã«ã¯ï¼š**
```python
# âŒ ç¾åœ¨ï¼ˆæœ€æ‚ªï¼‰
class ChatWorkflow:
    def __init__(self):
        self.llm_node = GeminiNode()  # ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼

# âœ… ã‚ã‚‹ã¹ãå§¿
class ChatWorkflow:
    def __init__(self, llm_provider: LLMProvider):
        self.llm_provider = llm_provider  # æ³¨å…¥å¯èƒ½
        
# ã¾ãŸã¯
class ChatWorkflow:
    def __init__(self, node_factory: NodeFactory):
        self.llm_node = node_factory.create_llm_node()
```

---

### 2. **ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆæŠ½è±¡åŒ–ï¼‰ã®æ¬ å¦‚**

ã™ã¹ã¦ãŒå…·è±¡ã‚¯ãƒ©ã‚¹ã«ä¾å­˜ã—ã¦ã„ã¾ã™ï¼š

```15:19:src/services/llm/gemini_service.py
class GeminiService(BaseLLMService):
    """Gemini API ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚µãƒ¼ãƒ“ã‚¹"""
    
    _configured = False
    
    @classmethod
    def _ensure_configured(cls):
```

**å•é¡Œç‚¹ï¼š**
- `GeminiService`ã¯å…·è±¡ã‚¯ãƒ©ã‚¹ã€‚ä»–ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¸ã®åˆ‡ã‚Šæ›¿ãˆãŒä¸å¯èƒ½
- ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãŒæ˜Žç¢ºã§ãªã„ï¼ˆ`BaseLLMService`ã¯ã‚ã‚‹ãŒã€ä½¿ã‚ã‚Œã¦ã„ãªã„ï¼‰
- ãƒŽãƒ¼ãƒ‰ãŒç‰¹å®šã®ã‚µãƒ¼ãƒ“ã‚¹å®Ÿè£…ã«å¼·ãçµåˆã—ã¦ã„ã‚‹

**ã‚ã‚‹ã¹ãå§¿ï¼š**
```python
# ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å®šç¾©
class LLMProvider(ABC):
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> str:
        pass
    
    @abstractmethod
    async def generate_json(self, prompt: str, schema: Type[BaseModel]) -> BaseModel:
        pass

# å®Ÿè£…
class GeminiProvider(LLMProvider):
    async def generate(self, prompt: str, **kwargs) -> str:
        # Gemini specific implementation
        pass

class OpenAIProvider(LLMProvider):
    async def generate(self, prompt: str, **kwargs) -> str:
        # OpenAI specific implementation
        pass

# ãƒŽãƒ¼ãƒ‰ã¯æŠ½è±¡ã«ä¾å­˜
class LLMNode(BaseNode):
    def __init__(self, provider: LLMProvider):
        self.provider = provider
```

---

### 3. **ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ã¨ã‚¯ãƒ©ã‚¹ãƒ¡ã‚½ãƒƒãƒ‰ã®ä¹±ç”¨**

```18:28:src/services/llm/gemini_service.py
    _configured = False
    
    @classmethod
    def _ensure_configured(cls):
        """APIè¨­å®šã‚’ç¢ºå®Ÿã«è¡Œã†"""
        if not cls._configured:
            if not settings.gemini_api_key:
                raise ValueError("GEMINI_API_KEY is not configured")
            genai.configure(api_key=settings.gemini_api_key)
            cls._configured = True
            logger.info("Gemini API configured successfully")
```

**å•é¡Œç‚¹ï¼š**
- ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ï¼ˆ`_configured`ï¼‰ã«ã‚ˆã‚Šã€ä¸¦è¡Œå‡¦ç†ã§å•é¡ŒãŒèµ·ãã‚‹å¯èƒ½æ€§
- ã‚¯ãƒ©ã‚¹ãƒ¡ã‚½ãƒƒãƒ‰ã¯ä¾å­˜æ€§æ³¨å…¥ã¨ç›¸æ€§ãŒæ‚ªã„
- è¤‡æ•°ã®è¨­å®šã‚’æŒã¤ã“ã¨ãŒä¸å¯èƒ½ï¼ˆä¾‹ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼Aã¯Gemini Proã€ãƒ¦ãƒ¼ã‚¶ãƒ¼Bã¯Gemini Flashï¼‰

**ã‚ã‚‹ã¹ãå§¿ï¼š**
```python
class GeminiProvider(LLMProvider):
    def __init__(self, api_key: str, model: str = "gemini-2.0-flash-exp"):
        self.api_key = api_key
        self.model = model
        genai.configure(api_key=api_key)
    
    async def generate(self, prompt: str, **kwargs) -> str:
        model_instance = genai.GenerativeModel(self.model)
        response = model_instance.generate_content(prompt)
        return response.text.strip()
```

---

### 4. **ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä¸åœ¨**

ç¾åœ¨ã€ãƒŽãƒ¼ãƒ‰ã‚„ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ç”Ÿæˆã¯ç›´æŽ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚

**å•é¡Œç‚¹ï¼š**
- å‹•çš„ãªãƒŽãƒ¼ãƒ‰ã®çµ„ã¿åˆã‚ã›ãŒä¸å¯èƒ½
- Difyã®ã‚ˆã†ãªãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¨ãƒ‡ã‚£ã‚¿ã‚’æ§‹ç¯‰ã§ããªã„
- è¨­å®šã‹ã‚‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ç”Ÿæˆã§ããªã„

**å¿…è¦ãªã‚‚ã®ï¼š**
```python
# ãƒŽãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼
class NodeFactory:
    def __init__(self, llm_registry: LLMRegistry, rag_registry: RAGRegistry):
        self.llm_registry = llm_registry
        self.rag_registry = rag_registry
    
    def create_node(self, node_type: str, config: Dict[str, Any]) -> BaseNode:
        if node_type == "llm":
            provider = self.llm_registry.get(config["provider"])
            return LLMNode(provider=provider, **config)
        elif node_type == "rag":
            search = self.rag_registry.get(config["search_type"])
            return RAGNode(search=search, **config)
        # ...

# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼
class WorkflowFactory:
    def __init__(self, node_factory: NodeFactory):
        self.node_factory = node_factory
    
    def create_from_config(self, config: WorkflowConfig) -> BaseWorkflow:
        nodes = [
            self.node_factory.create_node(n["type"], n["config"])
            for n in config.nodes
        ]
        return WorkflowBuilder().add_nodes(nodes).build()
```

---

### 5. **ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®šç¾©ã®ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**

ç¾åœ¨ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¯ã€ã‚³ãƒ¼ãƒ‰ã§å›ºå®šçš„ã«å®šç¾©ã•ã‚Œã¦ã„ã¾ã™ï¼š

```45:56:src/workflows/atomic/chat.py
    def _build_graph(self) -> StateGraph:
        """LangGraphã‚’æ§‹ç¯‰"""
        workflow = StateGraph(NodeState)

        # ãƒŽãƒ¼ãƒ‰ã‚’è¿½åŠ 
        workflow.add_node("gemini", self.llm_node.execute)

        # ãƒ•ãƒ­ãƒ¼ã‚’å®šç¾©
        workflow.add_edge(START, "gemini")
        workflow.add_edge("gemini", END)

        return workflow.compile()
```

**å•é¡Œç‚¹ï¼š**
- ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®æ§‹é€ ã‚’å¤‰æ›´ã™ã‚‹ã«ã¯ã‚³ãƒ¼ãƒ‰ã®ä¿®æ­£ãŒå¿…è¦
- å‹•çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç”ŸæˆãŒä¸å¯èƒ½
- Difyã®ã‚ˆã†ãªãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒ•ãƒ­ãƒ¼ã‚¨ãƒ‡ã‚£ã‚¿ã‚’å®Ÿè£…ã§ããªã„

**ã‚ã‚‹ã¹ãå§¿ï¼š**
```python
# YAML/JSONã‹ã‚‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®šç¾©
workflow_config = {
    "name": "chat_workflow",
    "nodes": [
        {
            "id": "llm_1",
            "type": "llm",
            "provider": "gemini",
            "config": {"temperature": 0.7}
        }
    ],
    "edges": [
        {"from": "START", "to": "llm_1"},
        {"from": "llm_1", "to": "END"}
    ]
}

# è¨­å®šã‹ã‚‰å‹•çš„ã«ç”Ÿæˆ
workflow = WorkflowFactory().create_from_config(workflow_config)
```

---

### 6. **è¨­å®šç®¡ç†ã®è²§å¼±ã•**

ç’°å¢ƒå¤‰æ•°ã«ä¾å­˜ã—ã™ãŽã¦ãŠã‚Šã€æŸ”è»Ÿæ€§ãŒã‚ã‚Šã¾ã›ã‚“ã€‚

**å•é¡Œç‚¹ï¼š**
- å®Ÿè¡Œæ™‚ã®è¨­å®šå¤‰æ›´ãŒå›°é›£
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã”ã¨ã€ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã”ã¨ã®è¨­å®šãŒã§ããªã„
- å¤–éƒ¨è¨­å®šã‚¹ãƒˆã‚¢ï¼ˆDBã€Vaultï¼‰ã¨ã®çµ±åˆãŒå›°é›£

**ã‚ã‚‹ã¹ãå§¿ï¼š**
```python
# è¨­å®šã®éšŽå±¤åŒ–
class Configuration:
    def __init__(self):
        self.providers = ConfigurationProvider([
            EnvironmentConfigProvider(),
            DatabaseConfigProvider(),
            SecretVaultProvider(),
        ])
    
    def get(self, key: str, default: Any = None) -> Any:
        for provider in self.providers:
            value = provider.get(key)
            if value is not None:
                return value
        return default
```

---

### 7. **ãƒ¬ã‚¸ã‚¹ãƒˆãƒªãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¬ å¦‚**

ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚„ãƒŽãƒ¼ãƒ‰ã®ç™»éŒ²ãƒ»ç®¡ç†ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ãŒã‚ã‚Šã¾ã›ã‚“ã€‚

**å•é¡Œç‚¹ï¼š**
- æ–°ã—ã„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®è¿½åŠ ãŒå¤§è¦æ¨¡ãªä¿®æ­£ã«ãªã‚‹
- ãƒ—ãƒ©ã‚°ã‚¤ãƒ³æ©Ÿæ§‹ã‚’å®Ÿè£…ã§ããªã„
- å®Ÿè¡Œæ™‚ã«ã©ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒåˆ©ç”¨å¯èƒ½ã‹ã‚ã‹ã‚‰ãªã„

**ã‚ã‚‹ã¹ãå§¿ï¼š**
```python
# LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ¬ã‚¸ã‚¹ãƒˆãƒª
class LLMRegistry:
    def __init__(self):
        self._providers: Dict[str, Type[LLMProvider]] = {}
    
    def register(self, name: str, provider_class: Type[LLMProvider]):
        self._providers[name] = provider_class
    
    def create(self, name: str, config: Dict[str, Any]) -> LLMProvider:
        if name not in self._providers:
            raise ValueError(f"Provider {name} not found")
        return self._providers[name](**config)
    
    def list_available(self) -> List[str]:
        return list(self._providers.keys())

# ä½¿ç”¨ä¾‹
llm_registry = LLMRegistry()
llm_registry.register("gemini", GeminiProvider)
llm_registry.register("openai", OpenAIProvider)
llm_registry.register("anthropic", AnthropicProvider)

# å‹•çš„ã«ç”Ÿæˆ
provider = llm_registry.create("gemini", {"api_key": "..."})
```

---

### 8. **ã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ä¸åœ¨**

ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å®Ÿè¡ŒçŠ¶æ…‹ã‚„é€²æ—ã‚’å¤–éƒ¨ã‹ã‚‰ç›£è¦–ã§ãã¾ã›ã‚“ã€‚

**å•é¡Œç‚¹ï¼š**
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®é€²æ—è¡¨ç¤ºãŒã§ããªã„
- ãƒ­ã‚®ãƒ³ã‚°ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãŒè²§å¼±
- ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ãƒ‡ãƒãƒƒã‚°ãŒå›°é›£

**ã‚ã‚‹ã¹ãå§¿ï¼š**
```python
class WorkflowEventBus:
    def __init__(self):
        self._listeners: Dict[str, List[Callable]] = defaultdict(list)
    
    def subscribe(self, event_type: str, callback: Callable):
        self._listeners[event_type].append(callback)
    
    async def publish(self, event: WorkflowEvent):
        for callback in self._listeners[event.type]:
            await callback(event)

# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œæ™‚
class Workflow:
    def __init__(self, event_bus: WorkflowEventBus):
        self.event_bus = event_bus
    
    async def run(self, input_data):
        await self.event_bus.publish(WorkflowStartEvent(...))
        # å®Ÿè¡Œ...
        await self.event_bus.publish(NodeExecutedEvent(...))
        # ...
        await self.event_bus.publish(WorkflowCompletedEvent(...))
```

---

### 9. **Stateç®¡ç†ã®éžåŠ¹çŽ‡æ€§**

`NodeState`ãŒè¾žæ›¸ãƒ™ãƒ¼ã‚¹ã§ã€åž‹å®‰å…¨æ€§ãŒã‚ã‚Šã¾ã›ã‚“ã€‚

```6:10:src/nodes/base.py
class NodeState(BaseModel):
    """Base state model for node communication"""
    messages: list = Field(default_factory=list)
    data: Dict[str, Any] = Field(default_factory=dict)
    metadata: Dict[str, Any] = Field(default_factory=dict)
```

**å•é¡Œç‚¹ï¼š**
- `data`ãŒ`Dict[str, Any]`ã§åž‹ãƒã‚§ãƒƒã‚¯ãŒã§ããªã„
- ãƒŽãƒ¼ãƒ‰é–“ã§ãƒ‡ãƒ¼ã‚¿ã®å—ã‘æ¸¡ã—ãŒæš—é»™çš„
- ã‚¹ã‚­ãƒ¼ãƒžãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãŒãªã„

**ã‚ã‚‹ã¹ãå§¿ï¼š**
```python
# åž‹ä»˜ãState
class ChatState(BaseModel):
    messages: List[Message]
    current_response: Optional[str] = None
    temperature: float = 0.7
    
class RAGState(BaseModel):
    query: str
    documents: List[Document]
    answer: Optional[str] = None

# ãƒŽãƒ¼ãƒ‰ã¯æ˜Žç¤ºçš„ãªStateã‚’å—ã‘å–ã‚‹
class TypedNode(BaseNode[TInput, TOutput]):
    async def execute(self, input_state: TInput) -> TOutput:
        pass
```

---

### 10. **ãƒ†ã‚¹ã‚¿ãƒ“ãƒªãƒ†ã‚£ã®æ¬ å¦‚**

ç¾åœ¨ã®ã‚³ãƒ¼ãƒ‰ã¯**å˜ä½“ãƒ†ã‚¹ãƒˆãŒæ¥µã‚ã¦å›°é›£**ã§ã™ã€‚

**å•é¡Œç‚¹ï¼š**
- ãƒ¢ãƒƒã‚¯ã‚„å½ç‰©ã‚’æ³¨å…¥ã§ããªã„
- å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆGemini APIï¼‰ã¸ã®ä¾å­˜ã‚’åˆ‡ã‚Šé›¢ã›ãªã„
- ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ãƒ†ã‚¹ãƒˆã«å®Ÿéš›ã®APIã‚­ãƒ¼ãŒå¿…è¦

**ã‚ã‚‹ã¹ãå§¿ï¼š**
```python
# ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¢ãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
class MockLLMProvider(LLMProvider):
    def __init__(self, responses: List[str]):
        self.responses = responses
        self.call_count = 0
    
    async def generate(self, prompt: str, **kwargs) -> str:
        response = self.responses[self.call_count]
        self.call_count += 1
        return response

# ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰
async def test_chat_workflow():
    mock_provider = MockLLMProvider(["Hello, how can I help?"])
    node = LLMNode(provider=mock_provider)
    workflow = ChatWorkflow(llm_node=node)
    
    result = await workflow.run("Hi")
    assert result == "Hello, how can I help?"
    assert mock_provider.call_count == 1
```

---

## ðŸ“ ææ¡ˆã™ã‚‹æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### ã‚³ã‚¢åŽŸå‰‡
1. **ä¾å­˜æ€§ã®é€†è»¢** - å…·è±¡ã§ã¯ãªãæŠ½è±¡ã«ä¾å­˜
2. **ä¾å­˜æ€§æ³¨å…¥** - ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã‚„ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ã‹ã‚‰ä¾å­˜ã‚’æ³¨å…¥
3. **ãƒ¬ã‚¸ã‚¹ãƒˆãƒªãƒ‘ã‚¿ãƒ¼ãƒ³** - ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å‹•çš„ç™»éŒ²
4. **ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³** - ãƒŽãƒ¼ãƒ‰ã¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å‹•çš„ç”Ÿæˆ
5. **è¨­å®šé§†å‹•** - ã‚³ãƒ¼ãƒ‰ã§ã¯ãªãè¨­å®šã§ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®šç¾©

### ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®ææ¡ˆ

```
src/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ interfaces/           # æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
â”‚   â”‚   â”œâ”€â”€ llm_provider.py
â”‚   â”‚   â”œâ”€â”€ rag_provider.py
â”‚   â”‚   â”œâ”€â”€ node.py
â”‚   â”‚   â””â”€â”€ workflow.py
â”‚   â”œâ”€â”€ registry/             # ãƒ¬ã‚¸ã‚¹ãƒˆãƒª
â”‚   â”‚   â”œâ”€â”€ llm_registry.py
â”‚   â”‚   â”œâ”€â”€ node_registry.py
â”‚   â”‚   â””â”€â”€ workflow_registry.py
â”‚   â”œâ”€â”€ factory/              # ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼
â”‚   â”‚   â”œâ”€â”€ node_factory.py
â”‚   â”‚   â””â”€â”€ workflow_factory.py
â”‚   â”œâ”€â”€ container.py          # DIã‚³ãƒ³ãƒ†ãƒŠ
â”‚   â””â”€â”€ events.py             # ã‚¤ãƒ™ãƒ³ãƒˆãƒã‚¹
â”‚
â”œâ”€â”€ providers/                # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å®Ÿè£…
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ gemini.py
â”‚   â”‚   â”œâ”€â”€ openai.py
â”‚   â”‚   â””â”€â”€ anthropic.py
â”‚   â””â”€â”€ rag/
â”‚       â”œâ”€â”€ simple_rag.py
â”‚       â””â”€â”€ advanced_rag.py
â”‚
â”œâ”€â”€ nodes/                    # ãƒŽãƒ¼ãƒ‰å®Ÿè£…
â”‚   â”œâ”€â”€ primitives/
â”‚   â””â”€â”€ composites/
â”‚
â”œâ”€â”€ workflows/                # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®šç¾©
â”‚   â”œâ”€â”€ definitions/          # YAML/JSONå®šç¾©
â”‚   â””â”€â”€ executors/            # å®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³
â”‚
â””â”€â”€ api/                      # APIå±¤
```

### å®Ÿè£…ä¾‹

```python
# core/interfaces/llm_provider.py
class LLMProvider(ABC):
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> str:
        pass

# providers/llm/gemini.py
class GeminiProvider(LLMProvider):
    def __init__(self, api_key: str, model: str = "gemini-2.0-flash-exp"):
        self.api_key = api_key
        self.model = model
        genai.configure(api_key=api_key)
    
    async def generate(self, prompt: str, **kwargs) -> str:
        # å®Ÿè£…
        pass

# core/registry/llm_registry.py
class LLMRegistry:
    _instance = None
    _providers: Dict[str, Type[LLMProvider]] = {}
    
    @classmethod
    def register(cls, name: str, provider_class: Type[LLMProvider]):
        cls._providers[name] = provider_class
    
    @classmethod
    def create(cls, name: str, config: Dict[str, Any]) -> LLMProvider:
        return cls._providers[name](**config)

# core/container.py - DIã‚³ãƒ³ãƒ†ãƒŠ
class Container:
    def __init__(self, config: Configuration):
        self.config = config
        self.llm_registry = LLMRegistry()
        self.node_factory = NodeFactory(self.llm_registry)
        self.workflow_factory = WorkflowFactory(self.node_factory)
    
    def get_workflow(self, name: str) -> BaseWorkflow:
        config = self.config.get_workflow_config(name)
        return self.workflow_factory.create_from_config(config)

# ä½¿ç”¨ä¾‹
container = Container(config)
workflow = container.get_workflow("chat")
result = await workflow.run("Hello")
```

---

## ðŸŽ¯ ç§»è¡Œãƒ­ãƒ¼ãƒ‰ãƒžãƒƒãƒ—

### Phase 1: åŸºç›¤æ•´å‚™ï¼ˆ2-3é€±é–“ï¼‰
1. ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å®šç¾©
2. ãƒ¬ã‚¸ã‚¹ãƒˆãƒªå®Ÿè£…
3. DIã‚³ãƒ³ãƒ†ãƒŠæ§‹ç¯‰

### Phase 2: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç§»è¡Œï¼ˆ1-2é€±é–“ï¼‰
1. æ—¢å­˜ã‚µãƒ¼ãƒ“ã‚¹ã‚’ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¤‰æ›
2. ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ç™»éŒ²

### Phase 3: ãƒŽãƒ¼ãƒ‰/ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç§»è¡Œï¼ˆ2-3é€±é–“ï¼‰
1. ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³å°Žå…¥
2. è¨­å®šé§†å‹•ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

### Phase 4: é«˜åº¦ãªæ©Ÿèƒ½ï¼ˆ1-2é€±é–“ï¼‰
1. ã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
2. ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¨ãƒ‡ã‚£ã‚¿å¯¾å¿œ

---

ç¾çŠ¶ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã¯ã€**Difyã®ã‚ˆã†ãªæ‹¡å¼µæ€§ã¯çµ¶å¯¾ã«å®Ÿç¾ã§ãã¾ã›ã‚“**ã€‚æ ¹æœ¬çš„ãªãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ãŒå¿…è¦ã§ã™ã€‚ç‰¹ã«ä¾å­˜æ€§æ³¨å…¥ã¨ãƒ¬ã‚¸ã‚¹ãƒˆãƒªãƒ‘ã‚¿ãƒ¼ãƒ³ã®å°Žå…¥ã¯**æœ€å„ªå…ˆ**ã§ã™ã€‚

```python
        self.rag_service = RAGService()
```

```python
        self.llm_node = GeminiNode()
        self.graph = self._build_graph()
```

```python
# âŒ ç¾åœ¨ï¼ˆæœ€æ‚ªï¼‰
class ChatWorkflow:
    def __init__(self):
        self.llm_node = GeminiNode()  # ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼

# âœ… ã‚ã‚‹ã¹ãå§¿
class ChatWorkflow:
    def __init__(self, llm_provider: LLMProvider):
        self.llm_provider = llm_provider  # æ³¨å…¥å¯èƒ½
        
# ã¾ãŸã¯
class ChatWorkflow:
    def __init__(self, node_factory: NodeFactory):
        self.llm_node = node_factory.create_llm_node()
```

```python
class GeminiService(BaseLLMService):
    """Gemini API ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚µãƒ¼ãƒ“ã‚¹"""
    
    _configured = False
    
    @classmethod
    def _ensure_configured(cls):
```

```python
# ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å®šç¾©
class LLMProvider(ABC):
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> str:
        pass
    
    @abstractmethod
    async def generate_json(self, prompt: str, schema: Type[BaseModel]) -> BaseModel:
        pass

# å®Ÿè£…
class GeminiProvider(LLMProvider):
    async def generate(self, prompt: str, **kwargs) -> str:
        # Gemini specific implementation
        pass

class OpenAIProvider(LLMProvider):
    async def generate(self, prompt: str, **kwargs) -> str:
        # OpenAI specific implementation
        pass

# ãƒŽãƒ¼ãƒ‰ã¯æŠ½è±¡ã«ä¾å­˜
class LLMNode(BaseNode):
    def __init__(self, provider: LLMProvider):
        self.provider = provider
```

```python
    _configured = False
    
    @classmethod
    def _ensure_configured(cls):
        """APIè¨­å®šã‚’ç¢ºå®Ÿã«è¡Œã†"""
        if not cls._configured:
            if not settings.gemini_api_key:
                raise ValueError("GEMINI_API_KEY is not configured")
            genai.configure(api_key=settings.gemini_api_key)
            cls._configured = True
            logger.info("Gemini API configured successfully")
```

```python
class GeminiProvider(LLMProvider):
    def __init__(self, api_key: str, model: str = "gemini-2.0-flash-exp"):
        self.api_key = api_key
        self.model = model
        genai.configure(api_key=api_key)
    
    async def generate(self, prompt: str, **kwargs) -> str:
        model_instance = genai.GenerativeModel(self.model)
        response = model_instance.generate_content(prompt)
        return response.text.strip()
```

```python
# ãƒŽãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼
class NodeFactory:
    def __init__(self, llm_registry: LLMRegistry, rag_registry: RAGRegistry):
        self.llm_registry = llm_registry
        self.rag_registry = rag_registry
    
    def create_node(self, node_type: str, config: Dict[str, Any]) -> BaseNode:
        if node_type == "llm":
            provider = self.llm_registry.get(config["provider"])
            return LLMNode(provider=provider, **config)
        elif node_type == "rag":
            search = self.rag_registry.get(config["search_type"])
            return RAGNode(search=search, **config)
        # ...

# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼
class WorkflowFactory:
    def __init__(self, node_factory: NodeFactory):
        self.node_factory = node_factory
    
    def create_from_config(self, config: WorkflowConfig) -> BaseWorkflow:
        nodes = [
            self.node_factory.create_node(n["type"], n["config"])
            for n in config.nodes
        ]
        return WorkflowBuilder().add_nodes(nodes).build()
```

```python
    def _build_graph(self) -> StateGraph:
        """LangGraphã‚’æ§‹ç¯‰"""
        workflow = StateGraph(NodeState)

        # ãƒŽãƒ¼ãƒ‰ã‚’è¿½åŠ 
        workflow.add_node("gemini", self.llm_node.execute)

        # ãƒ•ãƒ­ãƒ¼ã‚’å®šç¾©
        workflow.add_edge(START, "gemini")
        workflow.add_edge("gemini", END)

        return workflow.compile()
```

```python
# YAML/JSONã‹ã‚‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®šç¾©
workflow_config = {
    "name": "chat_workflow",
    "nodes": [
        {
            "id": "llm_1",
            "type": "llm",
            "provider": "gemini",
            "config": {"temperature": 0.7}
        }
    ],
    "edges": [
        {"from": "START", "to": "llm_1"},
        {"from": "llm_1", "to": "END"}
    ]
}

# è¨­å®šã‹ã‚‰å‹•çš„ã«ç”Ÿæˆ
workflow = WorkflowFactory().create_from_config(workflow_config)
```

```python
# è¨­å®šã®éšŽå±¤åŒ–
class Configuration:
    def __init__(self):
        self.providers = ConfigurationProvider([
            EnvironmentConfigProvider(),
            DatabaseConfigProvider(),
            SecretVaultProvider(),
        ])
    
    def get(self, key: str, default: Any = None) -> Any:
        for provider in self.providers:
            value = provider.get(key)
            if value is not None:
                return value
        return default
```

```python
# LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ¬ã‚¸ã‚¹ãƒˆãƒª
class LLMRegistry:
    def __init__(self):
        self._providers: Dict[str, Type[LLMProvider]] = {}
    
    def register(self, name: str, provider_class: Type[LLMProvider]):
        self._providers[name] = provider_class
    
    def create(self, name: str, config: Dict[str, Any]) -> LLMProvider:
        if name not in self._providers:
            raise ValueError(f"Provider {name} not found")
        return self._providers[name](**config)
    
    def list_available(self) -> List[str]:
        return list(self._providers.keys())

# ä½¿ç”¨ä¾‹
llm_registry = LLMRegistry()
llm_registry.register("gemini", GeminiProvider)
llm_registry.register("openai", OpenAIProvider)
llm_registry.register("anthropic", AnthropicProvider)

# å‹•çš„ã«ç”Ÿæˆ
provider = llm_registry.create("gemini", {"api_key": "..."})
```

```python
class WorkflowEventBus:
    def __init__(self):
        self._listeners: Dict[str, List[Callable]] = defaultdict(list)
    
    def subscribe(self, event_type: str, callback: Callable):
        self._listeners[event_type].append(callback)
    
    async def publish(self, event: WorkflowEvent):
        for callback in self._listeners[event.type]:
            await callback(event)

# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œæ™‚
class Workflow:
    def __init__(self, event_bus: WorkflowEventBus):
        self.event_bus = event_bus
    
    async def run(self, input_data):
        await self.event_bus.publish(WorkflowStartEvent(...))
        # å®Ÿè¡Œ...
        await self.event_bus.publish(NodeExecutedEvent(...))
        # ...
        await self.event_bus.publish(WorkflowCompletedEvent(...))
```

```python
class NodeState(BaseModel):
    """Base state model for node communication"""
    messages: list = Field(default_factory=list)
    data: Dict[str, Any] = Field(default_factory=dict)
    metadata: Dict[str, Any] = Field(default_factory=dict)
```

```python
# åž‹ä»˜ãState
class ChatState(BaseModel):
    messages: List[Message]
    current_response: Optional[str] = None
    temperature: float = 0.7
    
class RAGState(BaseModel):
    query: str
    documents: List[Document]
    answer: Optional[str] = None

# ãƒŽãƒ¼ãƒ‰ã¯æ˜Žç¤ºçš„ãªStateã‚’å—ã‘å–ã‚‹
class TypedNode(BaseNode[TInput, TOutput]):
    async def execute(self, input_state: TInput) -> TOutput:
        pass
```

```python
# ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¢ãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
class MockLLMProvider(LLMProvider):
    def __init__(self, responses: List[str]):
        self.responses = responses
        self.call_count = 0
    
    async def generate(self, prompt: str, **kwargs) -> str:
        response = self.responses[self.call_count]
        self.call_count += 1
        return response

# ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰
async def test_chat_workflow():
    mock_provider = MockLLMProvider(["Hello, how can I help?"])
    node = LLMNode(provider=mock_provider)
    workflow = ChatWorkflow(llm_node=node)
    
    result = await workflow.run("Hi")
    assert result == "Hello, how can I help?"
    assert mock_provider.call_count == 1
```

```plaintext
src/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ interfaces/           # æŠ½è±¡ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
â”‚   â”‚   â”œâ”€â”€ llm_provider.py
â”‚   â”‚   â”œâ”€â”€ rag_provider.py
â”‚   â”‚   â”œâ”€â”€ node.py
â”‚   â”‚   â””â”€â”€ workflow.py
â”‚   â”œâ”€â”€ registry/             # ãƒ¬ã‚¸ã‚¹ãƒˆãƒª
â”‚   â”‚   â”œâ”€â”€ llm_registry.py
â”‚   â”‚   â”œâ”€â”€ node_registry.py
â”‚   â”‚   â””â”€â”€ workflow_registry.py
â”‚   â”œâ”€â”€ factory/              # ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼
â”‚   â”‚   â”œâ”€â”€ node_factory.py
â”‚   â”‚   â””â”€â”€ workflow_factory.py
â”‚   â”œâ”€â”€ container.py          # DIã‚³ãƒ³ãƒ†ãƒŠ
â”‚   â””â”€â”€ events.py             # ã‚¤ãƒ™ãƒ³ãƒˆãƒã‚¹
â”‚
â”œâ”€â”€ providers/                # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å®Ÿè£…
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ gemini.py
â”‚   â”‚   â”œâ”€â”€ openai.py
â”‚   â”‚   â””â”€â”€ anthropic.py
â”‚   â””â”€â”€ rag/
â”‚       â”œâ”€â”€ simple_rag.py
â”‚       â””â”€â”€ advanced_rag.py
â”‚
â”œâ”€â”€ nodes/                    # ãƒŽãƒ¼ãƒ‰å®Ÿè£…
â”‚   â”œâ”€â”€ primitives/
â”‚   â””â”€â”€ composites/
â”‚
â”œâ”€â”€ workflows/                # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®šç¾©
â”‚   â”œâ”€â”€ definitions/          # YAML/JSONå®šç¾©
â”‚   â””â”€â”€ executors/            # å®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³
â”‚
â””â”€â”€ api/                      # APIå±¤
```

```python
# core/interfaces/llm_provider.py
class LLMProvider(ABC):
    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> str:
        pass

# providers/llm/gemini.py
class GeminiProvider(LLMProvider):
    def __init__(self, api_key: str, model: str = "gemini-2.0-flash-exp"):
        self.api_key = api_key
        self.model = model
        genai.configure(api_key=api_key)
    
    async def generate(self, prompt: str, **kwargs) -> str:
        # å®Ÿè£…
        pass

# core/registry/llm_registry.py
class LLMRegistry:
    _instance = None
    _providers: Dict[str, Type[LLMProvider]] = {}
    
    @classmethod
    def register(cls, name: str, provider_class: Type[LLMProvider]):
        cls._providers[name] = provider_class
    
    @classmethod
    def create(cls, name: str, config: Dict[str, Any]) -> LLMProvider:
        return cls._providers[name](**config)

# core/container.py - DIã‚³ãƒ³ãƒ†ãƒŠ
class Container:
    def __init__(self, config: Configuration):
        self.config = config
        self.llm_registry = LLMRegistry()
        self.node_factory = NodeFactory(self.llm_registry)
        self.workflow_factory = WorkflowFactory(self.node_factory)
    
    def get_workflow(self, name: str) -> BaseWorkflow:
        config = self.config.get_workflow_config(name)
        return self.workflow_factory.create_from_config(config)

# ä½¿ç”¨ä¾‹
container = Container(config)
workflow = container.get_workflow("chat")
result = await workflow.run("Hello")
```

